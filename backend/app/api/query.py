"""
æ™ºèƒ½é—®ç­”API
"""

import logging
import time
from fastapi import APIRouter, WebSocket, WebSocketDisconnect, Depends, HTTPException, Query as QueryParam
from sqlalchemy.orm import Session
from datetime import datetime
from typing import List, Optional
from pydantic import BaseModel

from app.db.init_db import get_db_session
from app.models.schemas import (
    QueryRequest, QueryResponse, Citation,
    TokenStats, Confidence, ModelType, StreamMessage, QueryStage
)
from app.services.rag_engine import get_rag_engine
from app.core.error_handlers import NovelNotFoundError
from app.models.database import Novel, Query
from app.core.trace_logger import get_trace_logger

router = APIRouter(prefix="/api/query", tags=["æ™ºèƒ½é—®ç­”"])
logger = logging.getLogger(__name__)
trace_logger = get_trace_logger()


# ==================== æ•°æ®æ¨¡å‹ ====================

class QueryFeedbackRequest(BaseModel):
    """æŸ¥è¯¢åé¦ˆè¯·æ±‚"""
    feedback: str  # "positive" | "negative"
    note: Optional[str] = None  # ç”¨æˆ·å¤‡æ³¨


class QueryHistoryItem(BaseModel):
    """æŸ¥è¯¢å†å²é¡¹"""
    id: int
    novel_id: int
    query: str
    answer: str  # ç®€çŸ­æ‘˜è¦ï¼ˆå‰100å­—ï¼‰
    model: str
    total_tokens: int
    confidence: str
    created_at: datetime
    feedback: Optional[str] = None
    
    class Config:
        from_attributes = True


class QueryHistoryResponse(BaseModel):
    """æŸ¥è¯¢å†å²å“åº”"""
    items: List[QueryHistoryItem]
    total: int
    page: int
    page_size: int


@router.post("", response_model=QueryResponse, summary="éæµå¼æŸ¥è¯¢")
async def query_novel(
    request: QueryRequest,
    db: Session = Depends(get_db_session)
):
    """
    éæµå¼æ™ºèƒ½é—®ç­”
    
    - åŸºç¡€RAGæ£€ç´¢
    - å®Œæ•´ç­”æ¡ˆè¿”å›
    - åŒ…å«å¼•ç”¨å’Œç»Ÿè®¡ä¿¡æ¯
    """
    start_time = time.time()
    
    # ç”Ÿæˆä¸´æ—¶æŸ¥è¯¢IDç”¨äºæ—¥å¿—è¿½è¸ªï¼ˆåœ¨ä¿å­˜åˆ°æ•°æ®åº“å‰ä½¿ç”¨æ—¶é—´æˆ³ï¼‰
    temp_query_id = int(time.time() * 1000000)  # å¾®ç§’çº§æ—¶é—´æˆ³
    
    # è®°å½•æŸ¥è¯¢å¼€å§‹
    trace_logger.trace_section(
        query_id=temp_query_id,
        section_name="éæµå¼æŸ¥è¯¢å¼€å§‹",
        emoji="ğŸš€"
    )
    
    try:
        # éªŒè¯å°è¯´æ˜¯å¦å­˜åœ¨
        novel = db.query(Novel).filter(Novel.id == request.novel_id).first()
        if not novel:
            raise NovelNotFoundError(request.novel_id)
        
        trace_logger.trace_step(
            query_id=temp_query_id,
            step_name="æŸ¥è¯¢åˆå§‹åŒ–",
            emoji="ğŸ“‹",
            input_data={
                "å°è¯´ID": request.novel_id,
                "å°è¯´åç§°": novel.title,
                "æŸ¥è¯¢å†…å®¹": request.query,
                "æ¨¡å‹": request.model.value,
                "å¯ç”¨æŸ¥è¯¢æ”¹å†™": request.enable_query_rewrite
            },
            output_data="åˆå§‹åŒ–å®Œæˆ",
            status="success"
        )
        
        # Tokenè®¡æ•°å™¨
        from app.utils.token_counter import get_token_counter
        token_counter = get_token_counter()
        
        # ç»Ÿè®¡Embedding tokens
        embedding_tokens = token_counter.count_tokens(request.query)
        
        # æ‰§è¡ŒRAGæŸ¥è¯¢
        rag_engine = get_rag_engine()
        answer, citations, stats, rewritten_query = rag_engine.query(
            db=db,
            novel_id=request.novel_id,
            query=request.query,
            model=request.model.value,
            enable_query_rewrite=request.enable_query_rewrite,
            query_id=temp_query_id,
            recency_bias_weight=request.recency_bias_weight
        )
        
        # ç»Ÿè®¡Promptå’ŒCompletion tokens
        # æ³¨æ„ï¼šè¿™é‡Œä½¿ç”¨ä¼°ç®—ï¼Œå› ä¸ºéæµå¼æ¥å£ä¸è¿”å›å®é™…çš„usageä¿¡æ¯
        # å¯ä»¥é€šè¿‡é‡æ–°æ„å»ºpromptæ¥è®¡ç®—ï¼Œæˆ–è€…ä¼°ç®—
        query_embedding = rag_engine.query_embedding(request.query)
        vector_results = rag_engine.vector_search(request.novel_id, query_embedding)
        reranked_chunks = rag_engine.rerank(
            request.query, 
            vector_results, 
            None,
            novel_id=request.novel_id,
            db=db,
            recency_bias_weight=request.recency_bias_weight
        )
        
        # æ„å»ºpromptç”¨äºè®¡ç®—tokens
        prompt = rag_engine.build_prompt(db, request.novel_id, request.query, reranked_chunks)
        prompt_tokens = token_counter.count_tokens(prompt)
        completion_tokens = token_counter.count_tokens(answer)
        
        total_tokens = embedding_tokens + prompt_tokens + completion_tokens
        
        response_time = time.time() - start_time
        
        # è®¡ç®—ç½®ä¿¡åº¦
        from app.services.confidence_calculator import get_confidence_calculator
        confidence_calculator = get_confidence_calculator()
        confidence_level = confidence_calculator.calculate_confidence(
            answer=answer,
            citations=[{'score': c.score} for c in citations],
            reranked_chunks=reranked_chunks,
            retrieved_count=stats.get('retrieved_chunks', 0)
        )
        
        # è·å–ç½®ä¿¡åº¦è¯¦æƒ…ï¼ˆç”¨äºæ—¥å¿—ï¼‰
        confidence_details = confidence_calculator.get_confidence_details(
            answer=answer,
            citations=[{'score': c.score} for c in citations],
            reranked_chunks=reranked_chunks,
            retrieved_count=stats.get('retrieved_chunks', 0)
        )
        logger.info(f"ğŸ“Š ç½®ä¿¡åº¦è®¡ç®—: {confidence_level.value} "
                   f"(å¾—åˆ†: {confidence_details['confidence_percentage']:.1f}%)")
        logger.debug(f"   - å¼•ç”¨è´¨é‡: {confidence_details['citation_score']:.2f}")
        logger.debug(f"   - ç­”æ¡ˆè´¨é‡: {confidence_details['answer_quality_score']:.2f}")
        logger.debug(f"   - æ£€ç´¢æ•ˆæœ: {confidence_details['retrieval_score']:.2f}")
        logger.debug(f"   - è¯­è¨€ç¡®å®šæ€§: {confidence_details['certainty_score']:.2f}")
        
        # ä¿å­˜æŸ¥è¯¢å†å²
        query_record = Query(
            novel_id=request.novel_id,
            query_text=request.query,
            answer_text=answer,
            model_used=request.model.value,
            response_time=response_time,
            total_tokens=total_tokens,
            confidence=confidence_level.value
        )
        db.add(query_record)
        db.commit()
        db.refresh(query_record)
        
        # è®°å½•Tokenä½¿ç”¨ç»Ÿè®¡
        try:
            from app.services.token_stats_service import get_token_stats_service
            token_stats_service = get_token_stats_service()
            
            # Embedding-3ä½¿ç”¨è®°å½•
            token_stats_service.record_token_usage(
                db=db,
                operation_type='query',
                operation_id=query_record.id,
                model_name='embedding-3',
                input_tokens=embedding_tokens,
                output_tokens=0
            )
            
            # LLMæ¨¡å‹ä½¿ç”¨è®°å½•
            token_stats_service.record_token_usage(
                db=db,
                operation_type='query',
                operation_id=query_record.id,
                model_name=request.model.value,
                input_tokens=prompt_tokens,
                output_tokens=completion_tokens
            )
        except Exception as e:
            logger.warning(f"âš ï¸ Tokenç»Ÿè®¡è®°å½•å¤±è´¥ï¼ˆä¸å½±å“ä¸»æµç¨‹ï¼‰: {e}")
        
        # æ„å»ºè¯¦ç»†çš„TokenStatså¯¹è±¡ï¼ˆåŒ…å«é˜¶æ®µç»Ÿè®¡ï¼‰
        by_stage = [
            {
                'stage': 'retrieving',
                'model': 'embedding-3',
                'inputTokens': embedding_tokens,
                'outputTokens': 0,
                'totalTokens': embedding_tokens
            },
            {
                'stage': 'generating',
                'model': request.model.value,
                'inputTokens': prompt_tokens,
                'outputTokens': completion_tokens,
                'totalTokens': prompt_tokens + completion_tokens
            }
        ]
        
        token_stats_obj = TokenStats(
            total_tokens=total_tokens,
            input_tokens=embedding_tokens + prompt_tokens,
            output_tokens=completion_tokens,
            embedding_tokens=embedding_tokens,
            prompt_tokens=prompt_tokens,
            completion_tokens=completion_tokens,
            by_model={
                'embedding-3': {
                    'inputTokens': embedding_tokens
                },
                request.model.value: {
                    'promptTokens': prompt_tokens,
                    'completionTokens': completion_tokens,
                    'totalTokens': prompt_tokens + completion_tokens
                }
            },
            by_stage=by_stage
        )
        
        # è®°å½•æŸ¥è¯¢å®Œæˆ
        trace_logger.trace_section(
            query_id=temp_query_id,
            section_name="éæµå¼æŸ¥è¯¢å®Œæˆ",
            emoji="âœ…"
        )
        trace_logger.trace_step(
            query_id=temp_query_id,
            step_name="æŸ¥è¯¢ç»“æœ",
            emoji="ğŸ“Š",
            input_data="æŸ¥è¯¢å¤„ç†å®Œæˆ",
            output_data={
                "ç­”æ¡ˆé•¿åº¦": len(answer),
                "å¼•ç”¨æ•°é‡": len(citations),
                "æ€»Tokenæ•°": total_tokens,
                "å“åº”æ—¶é—´": f"{response_time:.2f}ç§’",
                "ç½®ä¿¡åº¦": confidence_level.value,
                "ç½®ä¿¡åº¦å¾—åˆ†": f"{confidence_details['confidence_percentage']:.1f}%",
                "æŸ¥è¯¢ID": query_record.id
            },
            status="success"
        )
        
        # æ„å»ºå“åº”
        return QueryResponse(
            query_id=query_record.id,
            answer=answer,
            citations=citations,
            token_stats=token_stats_obj,
            response_time=response_time,
            confidence=confidence_level,
            model=request.model.value,
            timestamp=datetime.now().isoformat(),
            rewritten_query=rewritten_query
        )
        
    except NovelNotFoundError:
        raise
    except Exception as e:
        logger.error(f"âŒ æŸ¥è¯¢å¤±è´¥: {e}")
        trace_logger.trace_step(
            query_id=temp_query_id,
            step_name="æŸ¥è¯¢å¤±è´¥",
            emoji="âŒ",
            input_data=request.query,
            output_data=f"é”™è¯¯: {str(e)}",
            status="failed"
        )
        raise HTTPException(status_code=500, detail=f"æŸ¥è¯¢å¤±è´¥: {str(e)}")


@router.websocket("/stream")
async def query_stream(websocket: WebSocket):
    """
    æµå¼æ™ºèƒ½é—®ç­” WebSocket
    
    æ¥æ”¶: {"novel_id": 1, "query": "xxx", "model": "glm-4"}
    å‘é€: {"stage": "xxx", "content": "xxx", "progress": 0.5}
    """
    await websocket.accept()
    logger.info("ğŸ”Œ WebSocketè¿æ¥å·²å»ºç«‹")
    
    # ç”Ÿæˆä¸´æ—¶æŸ¥è¯¢IDç”¨äºæ—¥å¿—è¿½è¸ª
    temp_query_id = int(time.time() * 1000000)  # å¾®ç§’çº§æ—¶é—´æˆ³
    start_time = time.time()  # è®°å½•å¼€å§‹æ—¶é—´
    
    try:
        # æ¥æ”¶æŸ¥è¯¢è¯·æ±‚
        data = await websocket.receive_json()
        novel_id = data.get('novel_id')
        query = data.get('query')
        model = data.get('model', 'glm-4')
        config = data.get('config', {})
        
        # è®°å½•æµå¼æŸ¥è¯¢å¼€å§‹
        trace_logger.trace_section(
            query_id=temp_query_id,
            section_name="æµå¼æŸ¥è¯¢å¼€å§‹",
            emoji="ğŸš€"
        )
        
        # æå–é…ç½®å‚æ•°ï¼Œä½¿ç”¨é»˜è®¤å€¼
        top_k_retrieval = config.get('top_k_retrieval', 30)
        top_k_rerank = config.get('top_k_rerank', 10)
        max_context_chunks = config.get('max_context_chunks', 10)
        enable_query_rewrite = config.get('enable_query_rewrite', True)
        recency_bias_weight = config.get('recency_bias_weight', 0.15)
        
        # éªŒè¯å‚æ•°èŒƒå›´
        top_k_retrieval = max(10, min(100, top_k_retrieval))
        top_k_rerank = max(5, min(30, top_k_rerank))
        max_context_chunks = max(5, min(20, max_context_chunks))
        recency_bias_weight = max(0.0, min(0.5, recency_bias_weight))
        
        logger.info(f"ğŸ“Š æŸ¥è¯¢é…ç½®: top_k_retrieval={top_k_retrieval}, top_k_rerank={top_k_rerank}, max_context_chunks={max_context_chunks}")
        
        if not novel_id or not query:
            await websocket.send_json({
                'error': 'ç¼ºå°‘å¿…è¦å‚æ•°: novel_id æˆ– query'
            })
            await websocket.close()
            return
        
        # è·å–æ•°æ®åº“ä¼šè¯
        from app.db.init_db import get_database_url
        from sqlalchemy import create_engine
        from sqlalchemy.orm import sessionmaker
        
        engine = create_engine(get_database_url())
        SessionLocal = sessionmaker(bind=engine)
        db = SessionLocal()
        
        try:
            # éªŒè¯å°è¯´å­˜åœ¨
            novel = db.query(Novel).filter(Novel.id == novel_id).first()
            if not novel:
                await websocket.send_json({
                    'error': f'å°è¯´ ID={novel_id} ä¸å­˜åœ¨'
                })
                await websocket.close()
                return
            
            # è®°å½•æŸ¥è¯¢åˆå§‹åŒ–
            trace_logger.trace_step(
                query_id=temp_query_id,
                step_name="æŸ¥è¯¢åˆå§‹åŒ–",
                emoji="ğŸ“‹",
                input_data={
                    "å°è¯´ID": novel_id,
                    "å°è¯´åç§°": novel.title,
                    "æŸ¥è¯¢å†…å®¹": query,
                    "æ¨¡å‹": model,
                    "top_k_retrieval": top_k_retrieval,
                    "top_k_rerank": top_k_rerank,
                    "max_context_chunks": max_context_chunks,
                    "å¯ç”¨æŸ¥è¯¢æ”¹å†™": enable_query_rewrite
                },
                output_data="åˆå§‹åŒ–å®Œæˆ",
                status="success"
            )
            
            # é˜¶æ®µ1: æŸ¥è¯¢ç†è§£ï¼ˆå«æŸ¥è¯¢æ”¹å†™ï¼‰
            await websocket.send_json(StreamMessage(
                stage=QueryStage.UNDERSTANDING,
                content="æ­£åœ¨ç†è§£æ‚¨çš„é—®é¢˜...",
                progress=0.1
            ).model_dump())
            
            rag_engine = get_rag_engine()
            
            # æŸ¥è¯¢æ”¹å†™
            rewrite_result = rag_engine.query_rewriter.rewrite_query(
                query, 
                enable=enable_query_rewrite,
                query_id=temp_query_id
            )
            query_for_retrieval = rewrite_result["rewritten"]
            rewritten_query = query_for_retrieval if rewrite_result["rewrite_applied"] else None
            
            # å¦‚æœæŸ¥è¯¢è¢«æ”¹å†™ï¼Œå‘é€æ”¹å†™ç»“æœ
            if rewritten_query:
                await websocket.send_json(StreamMessage(
                    stage=QueryStage.UNDERSTANDING,
                    content=f"æŸ¥è¯¢å·²ä¼˜åŒ–: {rewritten_query}",
                    progress=0.15,
                    metadata={"rewritten_query": rewritten_query}
                ).model_dump())
            
            # é˜¶æ®µ2: æ£€ç´¢ä¸Šä¸‹æ–‡
            await websocket.send_json(StreamMessage(
                stage=QueryStage.RETRIEVING,
                content="æ­£åœ¨æ£€ç´¢ç›¸å…³å†…å®¹...",
                progress=0.3
            ).model_dump())
            
            # Tokenç»Ÿè®¡åˆå§‹åŒ–
            from app.services.token_stats_service import get_token_stats_service
            token_stats_service = get_token_stats_service()
            
            embedding_tokens = 0
            prompt_tokens = 0
            completion_tokens = 0
            
            # æŸ¥è¯¢å‘é‡åŒ–ï¼ˆç»Ÿè®¡Embedding tokensï¼Œä½¿ç”¨æ”¹å†™åçš„æŸ¥è¯¢ï¼‰
            from app.utils.token_counter import get_token_counter
            token_counter = get_token_counter()
            embedding_tokens += token_counter.count_tokens(query_for_retrieval)
            
            query_embedding = rag_engine.query_embedding(query_for_retrieval, query_id=temp_query_id)
            
            # è¯­ä¹‰æ£€ç´¢ï¼ˆä½¿ç”¨é…ç½®çš„top_k_retrievalï¼‰
            vector_results = rag_engine.vector_search(
                novel_id, 
                query_embedding,
                top_k=top_k_retrieval,
                query_id=temp_query_id
            )
            
            # Rerankï¼ˆå¸¦GraphRAGå¢å¼ºï¼Œä½¿ç”¨é…ç½®çš„top_k_rerankï¼‰
            reranked_chunks = rag_engine.rerank(
                query=query_for_retrieval,
                vector_results=vector_results,
                novel_id=novel_id,
                db=db,
                top_k=top_k_rerank,
                query_id=temp_query_id,
                recency_bias_weight=recency_bias_weight
            )
            
            if not reranked_chunks:
                await websocket.send_json({
                    'stage': 'finalizing',
                    'content': 'æŠ±æ­‰ï¼Œåœ¨å°è¯´ä¸­æœªæ‰¾åˆ°ç›¸å…³å†…å®¹ã€‚',
                    'progress': 1.0,
                    'done': True
                })
                await websocket.close()
                return
            
            # âœ¨ æ£€ç´¢å®Œæˆåç«‹å³æ„å»ºå¹¶å‘é€å¼•ç”¨åˆ—è¡¨
            logger.info("ğŸ“š æ£€ç´¢å®Œæˆï¼Œæ„å»ºå¼•ç”¨åˆ—è¡¨...")
            citations = []
            
            # è¿”å›å‰10æ¡å¼•ç”¨ï¼ˆæˆ–æ‰€æœ‰chunkï¼Œå–è¾ƒå°å€¼ï¼‰
            # ä¸è¿›è¡Œç« èŠ‚å»é‡ï¼Œå› ä¸ºåŒä¸€ç« èŠ‚å¯èƒ½æœ‰å¤šä¸ªç›¸å…³ç‰‡æ®µ
            max_citations = min(10, len(reranked_chunks))
            
            for chunk in reranked_chunks[:max_citations]:
                metadata = chunk['metadata']
                chapter_num = metadata.get('chapter_num')
                
                # è·å–ç« èŠ‚æ ‡é¢˜ï¼Œå¦‚æœmetadataä¸­æ²¡æœ‰ï¼Œä»æ•°æ®åº“æŸ¥è¯¢
                chapter_title = metadata.get('chapter_title')
                if not chapter_title and chapter_num:
                    try:
                        from app.models.database import Chapter
                        chapter = db.query(Chapter).filter(
                            Chapter.novel_id == novel_id,
                            Chapter.chapter_num == chapter_num
                        ).first()
                        if chapter:
                            chapter_title = chapter.chapter_title
                    except Exception as e:
                        logger.warning(f"è·å–ç« èŠ‚æ ‡é¢˜å¤±è´¥: {e}")
                
                citations.append({
                    'chapterNum': chapter_num,      # ä½¿ç”¨camelCaseåŒ¹é…å‰ç«¯
                    'chapterTitle': chapter_title,  # ä½¿ç”¨camelCaseåŒ¹é…å‰ç«¯
                    'text': chunk['content'][:200] + "...",
                    'score': chunk.get('score')
                })
            
            # å‘é€åŒ…å«å¼•ç”¨çš„æ£€ç´¢å®Œæˆæ¶ˆæ¯
            logger.info(f"ğŸ“¤ å‘é€å¼•ç”¨åˆ—è¡¨: {len(citations)} æ¡")
            await websocket.send_json({
                'stage': 'retrieving',
                'content': f"æ£€ç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(citations)} ä¸ªç›¸å…³ç« èŠ‚",
                'progress': 0.4,
                'citations': citations
            })
            
            # é˜¶æ®µ3: ç”Ÿæˆç­”æ¡ˆ
            await websocket.send_json(StreamMessage(
                stage=QueryStage.GENERATING,
                content="",
                progress=0.5
            ).model_dump())
            
            # æ„å»ºè‡ªé€‚åº”Promptï¼ˆä½¿ç”¨é…ç½®çš„max_context_chunksï¼Œä½¿ç”¨åŸå§‹æŸ¥è¯¢ï¼‰
            prompt = rag_engine.prompt_builder.build_prompt(
                db, 
                novel_id, 
                query, 
                reranked_chunks,
                max_chunks=max_context_chunks,
                query_id=temp_query_id
            )
            
            # æµå¼ç”Ÿæˆç­”æ¡ˆ
            full_answer = ""
            generation_usage = None
            finish_reason = None
            
            logger.info("ğŸ”„ å¼€å§‹æµå¼ç”Ÿæˆç­”æ¡ˆ...")
            
            for chunk_data in rag_engine.generate_answer_with_stats(prompt, model, stream=True):
                # chunk_dataå¯èƒ½åŒ…å«contentã€thinkingå’Œusage
                if isinstance(chunk_data, dict):
                    chunk = chunk_data.get('content', '')
                    thinking_chunk = chunk_data.get('reasoning_content')  # æå–thinkingå†…å®¹
                    usage = chunk_data.get('usage')
                    finish_reason_value = chunk_data.get('finish_reason')
                    
                    if usage:
                        # ä¿å­˜æœ€åçš„usageä¿¡æ¯
                        generation_usage = usage
                        logger.info(f"ğŸ’¡ [WebSocket] æ”¶åˆ°usage: {usage}")
                    
                    if finish_reason_value:
                        finish_reason = finish_reason_value
                        logger.info(f"ğŸ [WebSocket] æ”¶åˆ°finish_reason: {finish_reason}")
                else:
                    # å‘åå…¼å®¹ï¼šçº¯æ–‡æœ¬chunk
                    chunk = chunk_data if chunk_data else ''
                    thinking_chunk = None
                
                # å‘é€thinkingå†…å®¹ï¼ˆå¦‚æœæœ‰ï¼‰
                if thinking_chunk:
                    await websocket.send_json({
                        'stage': 'generating',
                        'thinking': thinking_chunk,  # å‘é€thinkingå¢é‡å†…å®¹
                        'content': '',
                        'progress': 0.6,
                        'is_delta': True
                    })
                
                # å‘é€ç­”æ¡ˆå†…å®¹ï¼ˆå¦‚æœæœ‰ï¼‰
                if chunk:
                    full_answer += chunk
                    await websocket.send_json({
                        'stage': 'generating',
                        'content': chunk,  # å‘é€å¢é‡å†…å®¹
                        'progress': 0.7,
                        'is_delta': True
                    })
            
            logger.info(f"âœ… æµå¼ç”Ÿæˆå®Œæˆï¼Œç­”æ¡ˆé•¿åº¦: {len(full_answer)}, æ˜¯å¦æœ‰usage: {generation_usage is not None}")
            
            # ä»generation_usageä¸­æå–Tokenç»Ÿè®¡
            if generation_usage:
                prompt_tokens = generation_usage.get('prompt_tokens', 0)
                completion_tokens = generation_usage.get('completion_tokens', 0)
                logger.info(f"âœ… ä½¿ç”¨APIè¿”å›çš„Tokenç»Ÿè®¡: prompt={prompt_tokens}, completion={completion_tokens}")
            else:
                # å¦‚æœæ²¡æœ‰ä»APIè·å–åˆ°usageï¼Œä½¿ç”¨ä¼°ç®—
                logger.warning("âš ï¸ APIæœªè¿”å›usageä¿¡æ¯ï¼Œä½¿ç”¨Tokenè®¡æ•°å™¨ä¼°ç®—")
                prompt_tokens = token_counter.count_tokens(prompt)
                completion_tokens = token_counter.count_tokens(full_answer)
                logger.info(f"ğŸ“Š ä¼°ç®—Tokenæ•°: prompt={prompt_tokens}, completion={completion_tokens}")
            
            # é˜¶æ®µ4: Self-RAGéªŒè¯
            # æ³¨æ„ï¼šä¸å‘é€ contentï¼Œé¿å…è¦†ç›–ä¹‹å‰çš„ç­”æ¡ˆ
            await websocket.send_json({
                'stage': 'validating',
                'progress': 0.8,
                'metadata': {'message': 'æ­£åœ¨éªŒè¯ç­”æ¡ˆå‡†ç¡®æ€§...'}
            })
            
            # Self-RAGéªŒè¯æµç¨‹
            from app.services.self_rag import (
                get_assertion_extractor,
                get_evidence_collector,
                get_evidence_scorer,
                get_consistency_checker,
                get_contradiction_detector,
                get_answer_corrector
            )
            
            contradictions_list = []
            confidence_level = "high"
            corrected_answer = full_answer
            
            try:
                # 1. æå–æ–­è¨€
                assertion_extractor = get_assertion_extractor()
                assertions = assertion_extractor.extract_assertions(full_answer, query_id=temp_query_id)
                logger.info(f"âœ… æå–æ–­è¨€: {len(assertions)} ä¸ª")
                
                if assertions:
                    # 2. æ”¶é›†è¯æ®
                    evidence_collector = get_evidence_collector()
                    evidence_map = {}
                    
                    for idx, assertion in enumerate(assertions):
                        evidence_list = evidence_collector.collect_evidence_for_assertion(
                            db, novel_id, assertion, top_k=3
                        )
                        evidence_map[idx] = evidence_list
                    
                    logger.info(f"âœ… æ”¶é›†è¯æ®å®Œæˆ")
                    
                    # è¯¦ç»†æ—¥å¿—ï¼šè¯æ®æ”¶é›†
                    trace_logger.trace_step(
                        query_id=temp_query_id,
                        step_name="Self-RAG: è¯æ®æ”¶é›†",
                        emoji="ğŸ“š",
                        input_data=f"ä¸º{len(assertions)}ä¸ªæ–­è¨€æ”¶é›†è¯æ®",
                        output_data={
                            "è¯æ®æ€»æ•°": sum(len(v) for v in evidence_map.values()),
                            "æ¯ä¸ªæ–­è¨€çš„è¯æ®æ•°": {f"æ–­è¨€{k}": len(v) for k, v in evidence_map.items()}
                        },
                        status="success"
                    )
                    
                    # 3. è¯„åˆ†è¯æ®
                    evidence_scorer = get_evidence_scorer()
                    for idx, assertion in enumerate(assertions):
                        evidence_list = evidence_map.get(idx, [])
                        # å¯¹æ¯æ¡è¯æ®è¿›è¡Œè¯„åˆ†
                        scored_evidence_list = []
                        for evidence in evidence_list:
                            scored = evidence_scorer.score_evidence(
                                db=db,
                                novel_id=novel_id,
                                evidence=evidence,
                                query_context={'assertion': assertion}
                            )
                            # å°†è¯„åˆ†ä¿¡æ¯æ·»åŠ åˆ°è¯æ®ä¸­
                            evidence['score'] = scored
                            scored_evidence_list.append(evidence)
                        evidence_map[idx] = scored_evidence_list
                    
                    # 4. ä¸€è‡´æ€§æ£€æŸ¥
                    consistency_checker = get_consistency_checker()
                    
                    # 4.1 æ—¶åºä¸€è‡´æ€§æ£€æŸ¥
                    temporal_issues = consistency_checker.check_temporal_consistency(
                        assertions, evidence_map
                    )
                    
                    # 4.2 è§’è‰²ä¸€è‡´æ€§æ£€æŸ¥
                    character_issues = consistency_checker.check_character_consistency(
                        db, novel_id, assertions, evidence_map
                    )
                    
                    # åˆå¹¶ä¸€è‡´æ€§æ£€æŸ¥ç»“æœ
                    consistency_report = {
                        'temporal_issues': temporal_issues,
                        'character_issues': character_issues,
                        'total_issues': len(temporal_issues) + len(character_issues)
                    }
                    
                    logger.info(f"âœ… ä¸€è‡´æ€§æ£€æŸ¥å®Œæˆ: {consistency_report['total_issues']} ä¸ªé—®é¢˜")
                    
                    # è¯¦ç»†æ—¥å¿—ï¼šä¸€è‡´æ€§æ£€æŸ¥
                    trace_logger.trace_step(
                        query_id=temp_query_id,
                        step_name="Self-RAG: ä¸€è‡´æ€§æ£€æŸ¥",
                        emoji="ğŸ”—",
                        input_data={
                            "æ–­è¨€æ•°é‡": len(assertions),
                            "è¯æ®æ€»æ•°": sum(len(v) for v in evidence_map.values())
                        },
                        output_data={
                            "æ—¶åºé—®é¢˜": len(temporal_issues),
                            "è§’è‰²ä¸€è‡´æ€§é—®é¢˜": len(character_issues),
                            "æ€»é—®é¢˜æ•°": consistency_report['total_issues']
                        },
                        status="success"
                    )
                    
                    # 5. æ£€æµ‹çŸ›ç›¾
                    contradiction_detector = get_contradiction_detector()
                    contradictions = contradiction_detector.detect_contradictions(
                        db, novel_id, assertions, evidence_map, consistency_report
                    )
                    
                    # è½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„å­—å…¸åˆ—è¡¨
                    contradictions_list = [
                        {
                            'type': c.type,
                            'earlyDescription': c.early_description,
                            'earlyChapter': c.early_chapter,
                            'lateDescription': c.late_description,
                            'lateChapter': c.late_chapter,
                            'analysis': c.analysis,
                            'confidence': c.confidence
                        }
                        for c in contradictions
                    ]
                    
                    logger.info(f"âœ… æ£€æµ‹åˆ°çŸ›ç›¾: {len(contradictions_list)} ä¸ª")
                    
                    # è¯¦ç»†æ—¥å¿—ï¼šçŸ›ç›¾æ£€æµ‹
                    trace_logger.trace_step(
                        query_id=temp_query_id,
                        step_name="Self-RAG: çŸ›ç›¾æ£€æµ‹",
                        emoji="âš ï¸",
                        input_data="åŸºäºæ–­è¨€ã€è¯æ®å’Œä¸€è‡´æ€§æ£€æŸ¥ç»“æœ",
                        output_data={
                            "çŸ›ç›¾æ•°é‡": len(contradictions_list),
                            "çŸ›ç›¾åˆ—è¡¨": contradictions_list
                        },
                        status="success"
                    )
                    
                    # 6. ä¿®æ­£ç­”æ¡ˆ
                    if contradictions:
                        answer_corrector = get_answer_corrector()
                        correction_result = answer_corrector.correct_answer(
                            full_answer, contradictions, "high"
                        )
                        corrected_answer = correction_result.get('corrected_answer', full_answer)
                        confidence_level = correction_result.get('final_confidence', 'high')
                        
                        logger.info(f"âœ… ç­”æ¡ˆä¿®æ­£å®Œæˆï¼Œç½®ä¿¡åº¦: {confidence_level}")
                        
                        # è¯¦ç»†æ—¥å¿—ï¼šç­”æ¡ˆä¿®æ­£
                        trace_logger.trace_step(
                            query_id=temp_query_id,
                            step_name="Self-RAG: ç­”æ¡ˆä¿®æ­£",
                            emoji="ğŸ”§",
                            input_data={
                                "åŸå§‹ç­”æ¡ˆé•¿åº¦": len(full_answer),
                                "çŸ›ç›¾æ•°é‡": len(contradictions)
                            },
                            output_data={
                                "ä¿®æ­£åç­”æ¡ˆé•¿åº¦": len(corrected_answer),
                                "æœ€ç»ˆç½®ä¿¡åº¦": confidence_level,
                                "æ˜¯å¦ä¿®æ”¹": corrected_answer != full_answer
                            },
                            status="success"
                        )
                
            except Exception as e:
                logger.error(f"âš ï¸ Self-RAGéªŒè¯å¤±è´¥: {e}")
                # Self-RAGå¤±è´¥ä¸å½±å“ä¸»æµç¨‹ï¼Œç»§ç»­è¿”å›åŸç­”æ¡ˆ
            
            # é˜¶æ®µ5: å®Œæˆæ±‡æ€»
            logger.info("ğŸ“‹ å¼€å§‹æ„å»ºæœ€ç»ˆç»“æœ...")
            # æ³¨æ„ï¼šä¸å‘é€ contentï¼Œé¿å…è¦†ç›–ä¹‹å‰çš„ç­”æ¡ˆ
            await websocket.send_json({
                'stage': 'finalizing',
                'progress': 0.9,
                'metadata': {'message': 'æ­£åœ¨æ•´ç†ç»“æœ...'}  # çŠ¶æ€ä¿¡æ¯æ”¾åœ¨ metadata ä¸­
            })
            
            # è®¡ç®—æ€»Tokenæ¶ˆè€—
            total_tokens = embedding_tokens + prompt_tokens + completion_tokens
            
            # æ„å»ºè¯¦ç»†çš„Tokenç»Ÿè®¡ä¿¡æ¯ï¼ˆåŒ…å«é˜¶æ®µçº§åˆ«ç»Ÿè®¡ï¼‰
            by_stage = [
                {
                    'stage': 'retrieving',
                    'model': 'embedding-3',
                    'inputTokens': embedding_tokens,
                    'outputTokens': 0,
                    'totalTokens': embedding_tokens
                },
                {
                    'stage': 'generating',
                    'model': model,
                    'inputTokens': prompt_tokens,
                    'outputTokens': completion_tokens,
                    'totalTokens': prompt_tokens + completion_tokens
                }
            ]
            
            token_stats = {
                'totalTokens': total_tokens,
                'inputTokens': embedding_tokens + prompt_tokens,
                'outputTokens': completion_tokens,
                'byModel': {
                    'embedding-3': {
                        'inputTokens': embedding_tokens,
                        'stage': 'retrieving'
                    },
                    model: {
                        'inputTokens': prompt_tokens,
                        'completionTokens': completion_tokens,
                        'totalTokens': prompt_tokens + completion_tokens,
                        'stage': 'generating'
                    }
                },
                'byStage': by_stage
            }
            
            logger.info(f"âœ… Tokenç»Ÿè®¡: æ€»è®¡ {total_tokens} tokens")
            logger.info(f"ğŸ’¾ ä¿å­˜æŸ¥è¯¢è®°å½•åˆ°æ•°æ®åº“...")
            
            # è®¡ç®—æ€»å“åº”æ—¶é—´
            total_response_time = time.time() - start_time
            
            # ä¿å­˜æŸ¥è¯¢å†å²ï¼ˆä½¿ç”¨ä¿®æ­£åçš„ç­”æ¡ˆï¼‰
            query_record = Query(
                novel_id=novel_id,
                query_text=query,
                answer_text=corrected_answer,
                model_used=model,
                response_time=total_response_time,  # è®°å½•æ€»å“åº”æ—¶é—´
                confidence=confidence_level,  # ä¿å­˜ç½®ä¿¡åº¦
                total_tokens=total_tokens  # ä¿å­˜Tokenæ¶ˆè€—
            )
            db.add(query_record)
            db.commit()
            db.refresh(query_record)
            logger.info(f"âœ… æŸ¥è¯¢è®°å½•å·²ä¿å­˜ï¼Œquery_id={query_record.id}")
            
            # è®°å½•Tokenä½¿ç”¨æƒ…å†µåˆ°ç»Ÿè®¡è¡¨
            try:
                # Embedding-3ä½¿ç”¨è®°å½•
                if embedding_tokens > 0:
                    token_stats_service.record_token_usage(
                        db=db,
                        operation_type='query',
                        operation_id=query_record.id,
                        model_name='embedding-3',
                        input_tokens=embedding_tokens,
                        output_tokens=0
                    )
                
                # GLMæ¨¡å‹ä½¿ç”¨è®°å½•
                if prompt_tokens > 0 or completion_tokens > 0:
                    token_stats_service.record_token_usage(
                        db=db,
                        operation_type='query',
                        operation_id=query_record.id,
                        model_name=model,
                        input_tokens=prompt_tokens,
                        output_tokens=completion_tokens
                    )
            except Exception as e:
                logger.warning(f"âš ï¸ Tokenç»Ÿè®¡è®°å½•å¤±è´¥ï¼ˆä¸å½±å“ä¸»æµç¨‹ï¼‰: {e}")
            
            # å‘é€æœ€ç»ˆç»“æœï¼ˆåŒ…å«query_idå’Œå®Œæ•´tokenç»Ÿè®¡ï¼‰
            # æ³¨æ„ï¼šcitations å·²åœ¨ retrieving é˜¶æ®µå‘é€ï¼Œè¿™é‡Œä¸å†é‡å¤å‘é€
            final_message = {
                'stage': 'finalizing',
                'content': corrected_answer,
                'progress': 1.0,
                'done': True,
                'contradictions': contradictions_list,
                'confidence': confidence_level,
                'query_id': query_record.id,
                'original_answer': full_answer if corrected_answer != full_answer else None,
                'metadata': {
                    'token_stats': token_stats  # ä½¿ç”¨å®Œæ•´çš„tokenç»Ÿè®¡ä¿¡æ¯
                }
            }
            
            logger.info(f"ğŸ“¤ å‡†å¤‡å‘é€æœ€ç»ˆæ¶ˆæ¯: query_id={query_record.id}, done=True, answer_length={len(corrected_answer)}")
            await websocket.send_json(final_message)
            logger.info(f"âœ… æµå¼æŸ¥è¯¢å®Œæˆï¼Œæœ€ç»ˆæ¶ˆæ¯å·²å‘é€")
            
            # è®°å½•æµå¼æŸ¥è¯¢å®Œæˆ
            trace_logger.trace_section(
                query_id=temp_query_id,
                section_name="æµå¼æŸ¥è¯¢å®Œæˆ",
                emoji="âœ…"
            )
            trace_logger.trace_step(
                query_id=temp_query_id,
                step_name="æŸ¥è¯¢ç»“æœ",
                emoji="ğŸ“Š",
                input_data="æŸ¥è¯¢å¤„ç†å®Œæˆ",
                output_data={
                    "ç­”æ¡ˆé•¿åº¦": len(corrected_answer),
                    "å¼•ç”¨æ•°é‡": len(citations),
                    "æ€»Tokenæ•°": total_tokens,
                    "çŸ›ç›¾æ•°é‡": len(contradictions_list),
                    "ç½®ä¿¡åº¦": confidence_level,
                    "æŸ¥è¯¢ID": query_record.id
                },
                status="success"
            )
            
        finally:
            db.close()
        
    except WebSocketDisconnect:
        logger.info("ğŸ”Œ WebSocketè¿æ¥å·²æ–­å¼€")
    except Exception as e:
        logger.error(f"âŒ æµå¼æŸ¥è¯¢å¤±è´¥: {e}")
        trace_logger.trace_step(
            query_id=temp_query_id,
            step_name="æµå¼æŸ¥è¯¢å¤±è´¥",
            emoji="âŒ",
            input_data=query if 'query' in locals() else "æœªçŸ¥æŸ¥è¯¢",
            output_data=f"é”™è¯¯: {str(e)}",
            status="failed"
        )
        try:
            await websocket.send_json({
                'error': f'æŸ¥è¯¢å¤±è´¥: {str(e)}'
            })
        except:
            pass
    finally:
        try:
            await websocket.close()
        except:
            pass


@router.get("/{query_id}/token-stats", response_model=TokenStats, summary="è·å–æŸ¥è¯¢Tokenç»Ÿè®¡")
async def get_query_token_stats(
    query_id: int,
    db: Session = Depends(get_db_session)
):
    """
    è·å–æŒ‡å®šæŸ¥è¯¢çš„Tokenæ¶ˆè€—ç»Ÿè®¡
    
    - ä»token_statsè¡¨æŸ¥è¯¢å¹¶èšåˆ
    - æŒ‰æ¨¡å‹åˆ†ç»„ç»Ÿè®¡
    - è¿”å›è¯¦ç»†çš„Tokenæ¶ˆè€—ä¿¡æ¯
    """
    try:
        from app.models.database import TokenStat
        
        # æŸ¥è¯¢è¯¥queryçš„æ‰€æœ‰tokenç»Ÿè®¡è®°å½•
        stats_records = db.query(TokenStat).filter(
            TokenStat.operation_type == 'query',
            TokenStat.operation_id == query_id
        ).all()
        
        if not stats_records:
            raise HTTPException(status_code=404, detail="æœªæ‰¾åˆ°Tokenç»Ÿè®¡è®°å½•")
        
        # æŒ‰æ¨¡å‹èšåˆ
        by_model = {}
        total_tokens = 0
        
        for record in stats_records:
            model_name = record.model_name
            
            if model_name not in by_model:
                by_model[model_name] = {}
            
            # æ ¹æ®æ¨¡å‹ç±»å‹è®¾ç½®ä¸åŒçš„å­—æ®µ
            if 'embedding' in model_name.lower():
                # Embeddingæ¨¡å‹åªæœ‰input_tokens
                by_model[model_name]['inputTokens'] = record.input_tokens or 0
            else:
                # LLMæ¨¡å‹æœ‰promptå’Œcompletion
                by_model[model_name]['promptTokens'] = record.prompt_tokens or 0
                by_model[model_name]['completionTokens'] = record.completion_tokens or 0
                by_model[model_name]['totalTokens'] = record.total_tokens or 0
            
            total_tokens += record.total_tokens or 0
        
        logger.info(f"âœ… è·å–æŸ¥è¯¢#{query_id}çš„Tokenç»Ÿè®¡: {total_tokens} tokens")
        
        return TokenStats(
            total_tokens=total_tokens,
            by_model=by_model
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ è·å–Tokenç»Ÿè®¡å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"è·å–Tokenç»Ÿè®¡å¤±è´¥: {str(e)}")


@router.get("/history", summary="è·å–æŸ¥è¯¢å†å²")
async def get_query_history(
    novel_id: Optional[int] = QueryParam(None, description="æŒ‰å°è¯´IDè¿‡æ»¤"),
    page: int = QueryParam(1, ge=1, description="é¡µç "),
    page_size: int = QueryParam(20, ge=1, le=100, description="æ¯é¡µæ•°é‡"),
    db: Session = Depends(get_db_session)
):
    """
    è·å–æŸ¥è¯¢å†å²
    
    - æ”¯æŒåˆ†é¡µ
    - æ”¯æŒæŒ‰å°è¯´IDè¿‡æ»¤
    - æŒ‰æ—¶é—´å€’åºæ’åˆ—
    """
    try:
        # æ„å»ºæŸ¥è¯¢
        query = db.query(Query)
        
        if novel_id:
            query = query.filter(Query.novel_id == novel_id)
        
        # è®¡ç®—æ€»æ•°
        total = query.count()
        
        # åˆ†é¡µæŸ¥è¯¢
        offset = (page - 1) * page_size
        queries = query.order_by(Query.created_at.desc()).offset(offset).limit(page_size).all()
        
        # æ„å»ºå“åº”
        items = []
        for q in queries:
            items.append({
                "id": q.id,
                "novel_id": q.novel_id,
                "query": q.query_text,
                "answer": q.answer_text[:200] + "..." if len(q.answer_text) > 200 else q.answer_text,
                "model": q.model_used,
                "total_tokens": q.total_tokens or 0,
                "confidence": q.confidence or "medium",
                "created_at": q.created_at if q.created_at else None,
                "feedback": "positive" if q.user_feedback == 1 else ("negative" if q.user_feedback == -1 else None)
            })
        
        logger.info(f"âœ… è·å–æŸ¥è¯¢å†å²æˆåŠŸ: {len(items)} æ¡")
        
        return {
            "items": items,
            "total": total,
            "page": page,
            "page_size": page_size,
            "total_pages": (total + page_size - 1) // page_size
        }
        
    except Exception as e:
        logger.error(f"âŒ è·å–æŸ¥è¯¢å†å²å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"è·å–æŸ¥è¯¢å†å²å¤±è´¥: {str(e)}")


@router.get("/{query_id}", response_model=QueryResponse, summary="è·å–æŸ¥è¯¢è¯¦æƒ…")
async def get_query_detail(
    query_id: int,
    db: Session = Depends(get_db_session)
):
    """
    è·å–å•ä¸ªæŸ¥è¯¢çš„å®Œæ•´è¯¦æƒ…
    
    - åŒ…å«å®Œæ•´ç­”æ¡ˆã€å¼•ç”¨ã€Tokenç»Ÿè®¡ç­‰
    - ç”¨äºæŸ¥è¯¢å†å²çš„è¯¦æƒ…æŸ¥çœ‹
    """
    try:
        # æŸ¥è¯¢è®°å½•
        query_record = db.query(Query).filter(Query.id == query_id).first()
        
        if not query_record:
            raise HTTPException(status_code=404, detail=f"æŸ¥è¯¢è®°å½• ID={query_id} ä¸å­˜åœ¨")
        
        # æ„å»ºå“åº”ï¼ˆå°½å¯èƒ½æ¢å¤åŸå§‹ç»“æ„ï¼‰
        response = QueryResponse(
            query_id=query_record.id,
            answer=query_record.answer_text,
            citations=[],  # å†å²æŸ¥è¯¢ä¸ä¿å­˜citationsï¼Œè¿”å›ç©ºåˆ—è¡¨
            graph_info={},  # å†å²æŸ¥è¯¢ä¸ä¿å­˜graph_info
            contradictions=[],  # å†å²æŸ¥è¯¢ä¸ä¿å­˜contradictions
            token_stats=TokenStats(
                total_tokens=query_record.total_tokens or 0,
                by_model={}
            ),
            response_time=query_record.response_time or 0.0,
            confidence=Confidence(query_record.confidence) if query_record.confidence else Confidence.MEDIUM,
            model=query_record.model_used or "unknown",
            timestamp=query_record.created_at if query_record.created_at else datetime.now().isoformat()
        )
        
        logger.info(f"âœ… è·å–æŸ¥è¯¢è¯¦æƒ…æˆåŠŸ: query_id={query_id}")
        
        return response
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ è·å–æŸ¥è¯¢è¯¦æƒ…å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"è·å–æŸ¥è¯¢è¯¦æƒ…å¤±è´¥: {str(e)}")


@router.post("/{query_id}/feedback", summary="æäº¤ç”¨æˆ·åé¦ˆ")
async def submit_feedback(
    query_id: int,
    feedback: str = QueryParam(..., regex="^(positive|negative)$", description="åé¦ˆç±»å‹"),
    note: Optional[str] = QueryParam(None, max_length=500, description="åé¦ˆå¤‡æ³¨"),
    db: Session = Depends(get_db_session)
):
    """
    æäº¤ç”¨æˆ·åé¦ˆ
    
    - positive: ç­”æ¡ˆå‡†ç¡®
    - negative: ç­”æ¡ˆä¸å‡†ç¡®
    """
    try:
        # æŸ¥è¯¢è®°å½•
        query_record = db.query(Query).filter(Query.id == query_id).first()
        
        if not query_record:
            raise HTTPException(status_code=404, detail=f"æŸ¥è¯¢è®°å½• ID={query_id} ä¸å­˜åœ¨")
        
        # æ›´æ–°åé¦ˆ
        query_record.user_feedback = 1 if feedback == "positive" else -1
        if note:
            query_record.feedback_note = note
        
        db.commit()
        
        logger.info(f"âœ… ç”¨æˆ·åé¦ˆå·²æäº¤: query_id={query_id}, feedback={feedback}")
        
        return {
            "success": True,
            "message": "æ„Ÿè°¢æ‚¨çš„åé¦ˆï¼",
            "query_id": query_id,
            "feedback": feedback
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ æäº¤åé¦ˆå¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"æäº¤åé¦ˆå¤±è´¥: {str(e)}")

