"""
BM25 æ£€ç´¢å™¨
å®ç°åŸºäºå…³é”®è¯çš„ç²¾ç¡®æ£€ç´¢ï¼Œå¼¥è¡¥å‘é‡æ£€ç´¢åœ¨ä¸“æœ‰åè¯ä¸Šçš„ä¸è¶³
"""

import os
import pickle
import logging
import jieba
from typing import List, Dict, Any, Optional
from pathlib import Path
from rank_bm25 import BM25Okapi

from app.core.config import settings

logger = logging.getLogger(__name__)

class BM25Retriever:
    """
    BM25 æ£€ç´¢å™¨
    è´Ÿè´£æ„å»ºã€å­˜å‚¨ã€åŠ è½½å’Œæ£€ç´¢ BM25 ç´¢å¼•
    """
    
    def __init__(self, novel_id: int):
        """
        åˆå§‹åŒ– BM25 æ£€ç´¢å™¨
        
        Args:
            novel_id: å°è¯´ID
        """
        self.novel_id = novel_id
        self.bm25 = None
        self.documents = []  # å­˜å‚¨åŸå§‹æ–‡æ¡£å†…å®¹ï¼ˆæˆ–å¼•ç”¨ï¼‰ï¼Œç”¨äºæ£€ç´¢è¿”å›
        self.metadatas = []  # å­˜å‚¨å…ƒæ•°æ®
        
        # ç´¢å¼•å­˜å‚¨è·¯å¾„
        self.index_dir = Path(settings.data_dir) / "indices"
        self.index_path = self.index_dir / f"novel_{novel_id}_bm25.pkl"
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        if not self.index_dir.exists():
            self.index_dir.mkdir(parents=True, exist_ok=True)
            
    def _tokenize(self, text: str) -> List[str]:
        """
        å¯¹ä¸­æ–‡æ–‡æœ¬è¿›è¡Œåˆ†è¯
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            
        Returns:
            List[str]: åˆ†è¯ç»“æœ
        """
        # ä½¿ç”¨ jieba è¿›è¡Œæœç´¢å¼•æ“æ¨¡å¼åˆ†è¯
        return list(jieba.cut_for_search(text))
        
    def build_index(self, chunks: List[Dict[str, Any]]):
        """
        æ„å»º BM25 ç´¢å¼•
        
        Args:
            chunks: æ–‡æœ¬å—åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å« 'content' å’Œ 'metadata'
        """
        logger.info(f"ğŸ—ï¸ å¼€å§‹æ„å»º BM25 ç´¢å¼• (Novel ID: {self.novel_id})...")
        
        texts = [chunk['content'] for chunk in chunks]
        self.documents = texts
        self.metadatas = [chunk.get('metadata', {}) for chunk in chunks]
        
        # åˆ†è¯
        tokenized_corpus = [self._tokenize(text) for text in texts]
        
        # æ„å»ºç´¢å¼•
        self.bm25 = BM25Okapi(tokenized_corpus)
        
        logger.info(f"âœ… BM25 ç´¢å¼•æ„å»ºå®Œæˆï¼Œå…± {len(texts)} ä¸ªæ–‡æ¡£")
        
        # ä¿å­˜ç´¢å¼•
        self.save_index()
        
    def save_index(self):
        """ä¿å­˜ç´¢å¼•åˆ°ç£ç›˜"""
        try:
            data = {
                'bm25': self.bm25,
                'documents': self.documents,
                'metadatas': self.metadatas
            }
            with open(self.index_path, 'wb') as f:
                pickle.dump(data, f)
            logger.info(f"ğŸ’¾ BM25 ç´¢å¼•å·²ä¿å­˜è‡³: {self.index_path}")
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜ BM25 ç´¢å¼•å¤±è´¥: {e}")
            raise
            
    def load_index(self) -> bool:
        """
        ä»ç£ç›˜åŠ è½½ç´¢å¼•
        
        Returns:
            bool: æ˜¯å¦åŠ è½½æˆåŠŸ
        """
        if not self.index_path.exists():
            logger.warning(f"âš ï¸ BM25 ç´¢å¼•æ–‡ä»¶ä¸å­˜åœ¨: {self.index_path}")
            return False
            
        try:
            with open(self.index_path, 'rb') as f:
                data = pickle.load(f)
                self.bm25 = data['bm25']
                self.documents = data['documents']
                self.metadatas = data['metadatas']
            logger.info(f"âœ… BM25 ç´¢å¼•åŠ è½½æˆåŠŸ (Novel ID: {self.novel_id})")
            return True
        except Exception as e:
            logger.error(f"âŒ åŠ è½½ BM25 ç´¢å¼•å¤±è´¥: {e}")
            return False
            
    def search(self, query: str, top_k: int = 10) -> List[Dict[str, Any]]:
        """
        æ‰§è¡Œå…³é”®è¯æ£€ç´¢
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›ç»“æœæ•°é‡
            
        Returns:
            List[Dict]: æ£€ç´¢ç»“æœåˆ—è¡¨ï¼ŒåŒ…å« content, metadata, score
        """
        if not self.bm25:
            if not self.load_index():
                return []
                
        tokenized_query = self._tokenize(query)
        
        # è·å–åˆ†æ•°
        scores = self.bm25.get_scores(tokenized_query)
        
        # è·å– top_k ç´¢å¼•
        # argsort è¿”å›ä»å°åˆ°å¤§çš„ç´¢å¼•ï¼Œæ‰€ä»¥å–æœ€å k ä¸ªå¹¶åè½¬
        top_n_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:top_k]
        
        results = []
        for i in top_n_indices:
            score = scores[i]
            if score > 0:  # åªè¿”å›æ­£ç›¸å…³ç»“æœ
                results.append({
                    'content': self.documents[i],
                    'metadata': self.metadatas[i],
                    'score': float(score),  # è½¬æ¢ä¸º float ä»¥ä¾¿åºåˆ—åŒ–
                    'rank': len(results) + 1
                })
                
        return results

    def delete_index(self):
        """åˆ é™¤ç´¢å¼•æ–‡ä»¶"""
        if self.index_path.exists():
            try:
                os.remove(self.index_path)
                logger.info(f"ğŸ—‘ï¸ BM25 ç´¢å¼•å·²åˆ é™¤: {self.index_path}")
            except Exception as e:
                logger.error(f"âŒ åˆ é™¤ BM25 ç´¢å¼•å¤±è´¥: {e}")

