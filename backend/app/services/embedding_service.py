"""
å‘é‡åŒ–æœåŠ¡
è°ƒç”¨æ™ºè°±AI Embedding-3è¿›è¡Œæ–‡æœ¬å‘é‡åŒ–
"""

import logging
import time
from typing import List, Dict, Optional, Tuple
import asyncio
from concurrent.futures import ThreadPoolExecutor

from app.services.zhipu_client import get_zhipu_client
from app.core.chromadb_client import get_chroma_client
from app.core.config import settings
from app.utils.token_counter import get_token_counter

logger = logging.getLogger(__name__)


class EmbeddingService:
    """å‘é‡åŒ–æœåŠ¡"""
    
    def __init__(self):
        """åˆå§‹åŒ–å‘é‡åŒ–æœåŠ¡"""
        self.zhipu_client = get_zhipu_client()
        self.chroma_client = get_chroma_client()
        self.token_counter = get_token_counter()
        self.batch_size = settings.embedding_batch_size  # æ‰¹é‡å¤„ç†å¤§å°ï¼ˆä»é…ç½®è¯»å–ï¼‰
        logger.info(f"âœ… å‘é‡åŒ–æœåŠ¡åˆå§‹åŒ–å®Œæˆ (batch_size={self.batch_size})")
    
    def embed_texts(
        self,
        texts: List[str],
        batch_size: Optional[int] = None
    ) -> Tuple[List[List[float]], int]:
        """
        æ‰¹é‡å‘é‡åŒ–æ–‡æœ¬
        
        Args:
            texts: æ–‡æœ¬åˆ—è¡¨
            batch_size: æ‰¹å¤„ç†å¤§å°
        
        Returns:
            Tuple[List[List[float]], int]: (å‘é‡åˆ—è¡¨, æ¶ˆè€—çš„tokenæ•°)
        """
        if not texts:
            return [], 0
        
        batch_size = batch_size or self.batch_size
        all_embeddings = []
        total_tokens = 0
        
        # åˆ†æ‰¹å¤„ç†
        for i in range(0, len(texts), batch_size):
            batch = texts[i:i + batch_size]
            batch_num = i//batch_size + 1
            total_batches = (len(texts) - 1)//batch_size + 1
            logger.info(f"ğŸ”„ æ­£åœ¨å‘é‡åŒ–æ‰¹æ¬¡ {batch_num}/{total_batches}...")
            
            # è®¡ç®—æœ¬æ‰¹æ¬¡çš„tokenæ¶ˆè€—
            batch_tokens = sum(self.token_counter.count_tokens(text) for text in batch)
            total_tokens += batch_tokens
            
            try:
                # è°ƒç”¨æ™ºè°±AI
                embeddings = self.zhipu_client.embed_texts(batch)
                all_embeddings.extend(embeddings)
                
            except Exception as e:
                logger.error(f"âŒ æ‰¹æ¬¡ {batch_num} å‘é‡åŒ–å¤±è´¥: {e}")
                # å¯¹å¤±è´¥çš„æ‰¹æ¬¡ä½¿ç”¨é›¶å‘é‡
                zero_embeddings = [[0.0] * settings.embedding_dimension for _ in batch]
                all_embeddings.extend(zero_embeddings)
            
            # æ‰¹æ¬¡é—´æ·»åŠ å»¶è¿Ÿï¼Œé¿å…è¯·æ±‚è¿‡äºå¯†é›†
            if i + batch_size < len(texts):
                time.sleep(0.5)
        
        logger.info(f"âœ… å®Œæˆ {len(all_embeddings)} ä¸ªæ–‡æœ¬çš„å‘é‡åŒ–ï¼Œæ¶ˆè€— {total_tokens} tokens")
        return all_embeddings, total_tokens
    
    def create_collection(self, novel_id: int) -> str:
        """
        ä¸ºå°è¯´åˆ›å»ºå‘é‡é›†åˆ
        
        Args:
            novel_id: å°è¯´ID
        
        Returns:
            str: é›†åˆåç§°
        """
        collection_name = f"novel_{novel_id}"
        
        try:
            self.chroma_client.get_or_create_collection(
                name=collection_name,
                metadata={
                    "novel_id": str(novel_id),
                    "hnsw:space": "cosine",
                    "hnsw:construction_ef": 200,
                    "hnsw:M": 16
                }
            )
            logger.info(f"âœ… åˆ›å»º/è·å–Collection: {collection_name}")
            return collection_name
            
        except Exception as e:
            logger.error(f"âŒ åˆ›å»ºCollectionå¤±è´¥: {e}")
            raise
    
    def add_chapter_chunks(
        self,
        collection_name: str,
        chunks: List[str],
        embeddings: List[List[float]],
        metadata_list: List[Dict]
    ) -> bool:
        """
        æ·»åŠ ç« èŠ‚å—åˆ°å‘é‡åº“
        
        Args:
            collection_name: é›†åˆåç§°
            chunks: æ–‡æœ¬å—åˆ—è¡¨
            embeddings: å‘é‡åˆ—è¡¨
            metadata_list: å…ƒæ•°æ®åˆ—è¡¨
        
        Returns:
            bool: æ˜¯å¦æˆåŠŸ
        """
        if not chunks or not embeddings:
            logger.warning("âš ï¸ æ²¡æœ‰å†…å®¹éœ€è¦æ·»åŠ ")
            return False
        
        try:
            # ç”ŸæˆID
            ids = [
                f"novel_{metadata['novel_id']}_ch{metadata['chapter_num']}_chunk{metadata['chunk_index']}"
                for metadata in metadata_list
            ]
            
            # æ·»åŠ åˆ°ChromaDB
            self.chroma_client.add_documents(
                collection_name=collection_name,
                documents=chunks,
                embeddings=embeddings,
                ids=ids,
                metadatas=metadata_list
            )
            
            logger.info(f"âœ… æˆåŠŸæ·»åŠ  {len(chunks)} ä¸ªå—åˆ° {collection_name}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ æ·»åŠ åˆ°ChromaDBå¤±è´¥: {e}")
            return False
    
    def process_chapter(
        self,
        novel_id: int,
        chapter_num: int,
        chapter_title: str,
        chapter_chunks: List[Dict]
    ) -> Tuple[bool, int]:
        """
        å¤„ç†å•ä¸ªç« èŠ‚ï¼ˆå‘é‡åŒ–å¹¶å­˜å‚¨ï¼‰
        
        Args:
            novel_id: å°è¯´ID
            chapter_num: ç« èŠ‚ç¼–å·
            chapter_title: ç« èŠ‚æ ‡é¢˜
            chapter_chunks: ç« èŠ‚å—åˆ—è¡¨ï¼ˆåŒ…å«contentå’Œmetadataï¼‰
        
        Returns:
            Tuple[bool, int]: (æ˜¯å¦æˆåŠŸ, æ¶ˆè€—çš„tokenæ•°)
        """
        if not chapter_chunks:
            logger.warning(f"âš ï¸ ç« èŠ‚ {chapter_num} æ²¡æœ‰å†…å®¹")
            return False, 0
        
        try:
            # æå–æ–‡æœ¬
            texts = [chunk['content'] for chunk in chapter_chunks]
            
            # å‘é‡åŒ–ï¼ˆè·å–tokenæ¶ˆè€—ï¼‰
            embeddings, tokens_used = self.embed_texts(texts)
            
            # å‡†å¤‡å…ƒæ•°æ®
            metadata_list = []
            for i, chunk in enumerate(chapter_chunks):
                metadata = {
                    'novel_id': novel_id,
                    'chapter_num': chapter_num,
                    'chapter_title': chapter_title,
                    'chunk_index': i,
                    'char_count': len(chunk['content']),
                }
                # åˆå¹¶chunkè‡ªå¸¦çš„metadata
                if 'metadata' in chunk:
                    metadata.update(chunk['metadata'])
                metadata_list.append(metadata)
            
            # å­˜å‚¨åˆ°ChromaDB
            collection_name = f"novel_{novel_id}"
            success = self.add_chapter_chunks(
                collection_name=collection_name,
                chunks=texts,
                embeddings=embeddings,
                metadata_list=metadata_list
            )
            
            return success, tokens_used
            
        except Exception as e:
            logger.error(f"âŒ å¤„ç†ç« èŠ‚ {chapter_num} å¤±è´¥: {e}")
            return False, 0
    
    async def process_novel_with_batch_api(
        self,
        novel_id: int,
        all_chapters_data: List[Dict]
    ) -> Tuple[bool, int, List[int]]:
        """
        ä½¿ç”¨ Batch API æ‰¹é‡å¤„ç†æ•´æœ¬å°è¯´çš„å‘é‡åŒ–
        
        æ ¹æ®æ™ºè°±AIæ–‡æ¡£ï¼ŒEmbedding-3æ”¯æŒBatch APIï¼Œé™åˆ¶ä¸º10000ä¸ªè¯·æ±‚/æ‰¹æ¬¡
        
        Args:
            novel_id: å°è¯´ID
            all_chapters_data: æ‰€æœ‰ç« èŠ‚æ•°æ®
                [
                    {
                        'chapter_num': 1,
                        'chapter_title': 'ç¬¬ä¸€ç« ',
                        'chunks': [chunk1, chunk2, ...]
                    },
                    ...
                ]
        
        Returns:
            Tuple[bool, int, List[int]]: (æ˜¯å¦æˆåŠŸ, æ€»tokenæ•°, å¤±è´¥çš„ç« èŠ‚åˆ—è¡¨)
        """
        from app.services.batch_api_client import get_batch_client
        
        # å…ˆç»Ÿè®¡æ€»è¯·æ±‚æ•°
        total_chunks_count = sum(len(chapter_data['chunks']) for chapter_data in all_chapters_data)
        
        # ğŸ¯ æ™ºèƒ½åˆ¤æ–­ï¼šè¯·æ±‚æ•° < é˜ˆå€¼æ—¶ä½¿ç”¨å®æ—¶API
        if total_chunks_count < settings.batch_api_threshold:
            logger.info(f"ğŸ“Š è¯·æ±‚æ•°({total_chunks_count}) < é˜ˆå€¼({settings.batch_api_threshold})ï¼Œä½¿ç”¨å®æ—¶APIï¼ˆæ›´å¿«ï¼‰")
            return await self._embed_chapters_realtime(novel_id, all_chapters_data)
        
        logger.info(f"ğŸš€ è¯·æ±‚æ•°({total_chunks_count}) â‰¥ é˜ˆå€¼({settings.batch_api_threshold})ï¼Œä½¿ç”¨Batch APIï¼ˆæ›´çœé’±ï¼‰")
        
        # æ”¶é›†æ‰€æœ‰chunkså¹¶æ„å»ºbatchä»»åŠ¡
        batch_tasks = []
        chunk_mapping = []  # è®°å½•æ¯ä¸ªchunkå¯¹åº”çš„ç« èŠ‚ä¿¡æ¯
        for chapter_data in all_chapters_data:
            chapter_num = chapter_data['chapter_num']
            chapter_title = chapter_data['chapter_title']
            chunks = chapter_data['chunks']
            
            for chunk_idx, chunk in enumerate(chunks):
                chunk_text = chunk['content']
                custom_id = f"embedding-novel{novel_id}-ch{chapter_num}-chunk{chunk_idx}"
                
                # æ„å»ºBatch APIä»»åŠ¡ï¼ˆä½¿ç”¨embeddingæ¨¡å‹ï¼‰
                # Embedding æ¨¡å‹éœ€è¦ä½¿ç”¨ /v4/embeddings ç«¯ç‚¹
                batch_tasks.append({
                    "custom_id": custom_id,
                    "method": "POST",
                    "url": "/v4/embeddings",
                    "body": {
                        "model": "embedding-3",
                        "input": chunk_text
                    }
                })
                
                # è®°å½•æ˜ å°„å…³ç³»
                chunk_mapping.append({
                    'chapter_num': chapter_num,
                    'chapter_title': chapter_title,
                    'chunk_index': chunk_idx,
                    'chunk': chunk,
                    'novel_id': novel_id
                })
        
        logger.info(f"ğŸ“Š å‡†å¤‡æ‰¹é‡å‘é‡åŒ– {total_chunks_count} ä¸ªæ–‡æœ¬å—")
        
        # æ£€æŸ¥æ˜¯å¦è¶…è¿‡é™åˆ¶ï¼ˆ10000ä¸ªè¯·æ±‚/æ‰¹æ¬¡ï¼‰
        if len(batch_tasks) > 10000:
            logger.warning(f"âš ï¸ æ–‡æœ¬å—æ•°é‡({len(batch_tasks)})è¶…è¿‡Batch APIé™åˆ¶(10000)ï¼Œå°†åˆ†æ‰¹å¤„ç†")
            return await self._process_novel_in_batches(
                novel_id, all_chapters_data, batch_tasks, chunk_mapping
            )
        
        # æäº¤Batch API
        batch_client = get_batch_client()
        
        def progress_callback(batch_id, status, progress, completed, total, failed):
            logger.info(f"ğŸ“Š å‘é‡åŒ–è¿›åº¦: {status} | {completed}/{total} ({progress*100:.1f}%) | å¤±è´¥: {failed}")
        
        try:
            results_map, token_stats = await asyncio.to_thread(
                batch_client.submit_and_wait,
                batch_tasks,
                check_interval=30,
                progress_callback=progress_callback
            )
            
            total_tokens = token_stats.get('total_tokens', 0)
            logger.info(f"ğŸ“Š Batch APIå‘é‡åŒ–Tokenç»Ÿè®¡: {token_stats}")
            
        except Exception as e:
            logger.error(f"âŒ Batch APIè°ƒç”¨å¤±è´¥ï¼Œé™çº§ä½¿ç”¨å®æ—¶API: {e}")
            return await self._fallback_to_realtime_api(novel_id, all_chapters_data)
        
        # è§£æç»“æœå¹¶å­˜å‚¨åˆ°ChromaDB
        collection_name = f"novel_{novel_id}"
        failed_chapters = set()
        
        embeddings_by_chapter = {}  # æŒ‰ç« èŠ‚ç»„ç»‡embeddings
        
        for i, chunk_info in enumerate(chunk_mapping):
            custom_id = f"embedding-novel{novel_id}-ch{chunk_info['chapter_num']}-chunk{chunk_info['chunk_index']}"
            
            if custom_id not in results_map:
                logger.warning(f"âš ï¸ æœªæ‰¾åˆ°ç»“æœ: {custom_id}")
                failed_chapters.add(chunk_info['chapter_num'])
                continue
            
            result = results_map[custom_id]
            
            if result['status'] != 'success':
                logger.warning(f"âš ï¸ å‘é‡åŒ–å¤±è´¥: {custom_id}, é”™è¯¯: {result.get('error')}")
                failed_chapters.add(chunk_info['chapter_num'])
                continue
            
            # ä»è¿”å›ç»“æœä¸­æå–embeddingå‘é‡
            # Batch APIè¿”å›æ ¼å¼: data[0].embedding
            try:
                usage = result.get('usage', {})
                embedding = None
                
                # ä»dataå­—æ®µæå–embeddingï¼ˆbatch_api_clientå·²è§£æï¼‰
                if 'data' in result and len(result['data']) > 0:
                    embedding = result['data'][0].get('embedding')
                
                if embedding is None or not isinstance(embedding, list):
                    logger.error(f"âŒ æ— æ³•æå–embeddingæˆ–æ ¼å¼é”™è¯¯: {custom_id}, result keys: {result.keys()}")
                    failed_chapters.add(chunk_info['chapter_num'])
                    continue
                
                # éªŒè¯embeddingç»´åº¦
                if len(embedding) != settings.embedding_dimension:
                    logger.error(f"âŒ Embeddingç»´åº¦é”™è¯¯: {custom_id}, expected={settings.embedding_dimension}, got={len(embedding)}")
                    failed_chapters.add(chunk_info['chapter_num'])
                    continue
                
                # æŒ‰ç« èŠ‚ç»„ç»‡
                chapter_num = chunk_info['chapter_num']
                if chapter_num not in embeddings_by_chapter:
                    embeddings_by_chapter[chapter_num] = {
                        'chapter_title': chunk_info['chapter_title'],
                        'chunks': [],
                        'embeddings': [],
                        'metadata_list': []
                    }
                
                embeddings_by_chapter[chapter_num]['chunks'].append(chunk_info['chunk']['content'])
                embeddings_by_chapter[chapter_num]['embeddings'].append(embedding)
                embeddings_by_chapter[chapter_num]['metadata_list'].append({
                    'novel_id': novel_id,
                    'chapter_num': chapter_num,
                    'chapter_title': chunk_info['chapter_title'],
                    'chunk_index': chunk_info['chunk_index'],
                    'char_count': len(chunk_info['chunk']['content'])
                })
                
            except Exception as e:
                logger.error(f"âŒ å¤„ç†embeddingç»“æœå¤±è´¥: {custom_id}, é”™è¯¯: {e}")
                failed_chapters.add(chunk_info['chapter_num'])
        
        # æ‰¹é‡å­˜å‚¨åˆ°ChromaDBï¼ˆæŒ‰ç« èŠ‚ï¼‰
        for chapter_num, chapter_data in embeddings_by_chapter.items():
            try:
                success = self.add_chapter_chunks(
                    collection_name=collection_name,
                    chunks=chapter_data['chunks'],
                    embeddings=chapter_data['embeddings'],
                    metadata_list=chapter_data['metadata_list']
                )
                
                if not success:
                    failed_chapters.add(chapter_num)
                    
            except Exception as e:
                logger.error(f"âŒ å­˜å‚¨ç« èŠ‚ {chapter_num} å¤±è´¥: {e}")
                failed_chapters.add(chapter_num)
        
        success = len(failed_chapters) == 0
        logger.info(f"âœ… Batch APIå‘é‡åŒ–å®Œæˆ: æ€»tokens={total_tokens}, å¤±è´¥ç« èŠ‚æ•°={len(failed_chapters)}")
        
        return success, total_tokens, list(failed_chapters)
    
    async def _process_novel_in_batches(
        self,
        novel_id: int,
        all_chapters_data: List[Dict],
        all_tasks: List[Dict],
        all_mappings: List[Dict]
    ) -> Tuple[bool, int, List[int]]:
        """
        åˆ†æ‰¹å¤„ç†å¤§é‡chunksï¼ˆè¶…è¿‡10000ä¸ªï¼‰
        """
        batch_size = 10000
        total_tokens = 0
        all_failed_chapters = set()
        
        for i in range(0, len(all_tasks), batch_size):
            logger.info(f"ğŸ“¦ å¤„ç†æ‰¹æ¬¡ {i//batch_size + 1}/{(len(all_tasks)-1)//batch_size + 1}")
            
            batch_tasks = all_tasks[i:i+batch_size]
            batch_mappings = all_mappings[i:i+batch_size]
            
            # é€’å½’è°ƒç”¨ï¼ˆå•æ‰¹æ¬¡ï¼‰
            success, tokens, failed = await self.process_novel_with_batch_api(
                novel_id, all_chapters_data
            )
            
            total_tokens += tokens
            all_failed_chapters.update(failed)
        
        return len(all_failed_chapters) == 0, total_tokens, list(all_failed_chapters)
    
    async def _fallback_to_realtime_api(
        self,
        novel_id: int,
        all_chapters_data: List[Dict]
    ) -> Tuple[bool, int, List[int]]:
        """
        é™çº§åˆ°å®æ—¶APIå¤„ç†
        """
        logger.warning("âš ï¸ é™çº§ä½¿ç”¨å®æ—¶APIå¤„ç†å‘é‡åŒ–")
        
        total_tokens = 0
        failed_chapters = []
        
        for chapter_data in all_chapters_data:
            chapter_num = chapter_data['chapter_num']
            chapter_title = chapter_data['chapter_title']
            chunks = chapter_data['chunks']
            
            try:
                success, chapter_tokens = self.process_chapter(
                    novel_id, chapter_num, chapter_title, chunks
                )
                
                if success:
                    total_tokens += chapter_tokens
                else:
                    failed_chapters.append(chapter_num)
                    
            except Exception as e:
                logger.error(f"âŒ ç« èŠ‚ {chapter_num} å¤„ç†å¤±è´¥: {e}")
                failed_chapters.append(chapter_num)
        
        return len(failed_chapters) == 0, total_tokens, failed_chapters
    
    def query_similar_chunks(
        self,
        novel_id: int,
        query_text: str,
        top_k: int = 30,
        chapter_filter: Optional[Dict] = None
    ) -> Dict:
        """
        æŸ¥è¯¢ç›¸ä¼¼æ–‡æœ¬å—
        
        Args:
            novel_id: å°è¯´ID
            query_text: æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›Top-Kç»“æœ
            chapter_filter: ç« èŠ‚è¿‡æ»¤æ¡ä»¶
        
        Returns:
            Dict: æŸ¥è¯¢ç»“æœ
        """
        try:
            # å‘é‡åŒ–æŸ¥è¯¢æ–‡æœ¬
            query_embedding = self.zhipu_client.embed_text(query_text)
            
            # ä»ChromaDBæ£€ç´¢
            collection_name = f"novel_{novel_id}"
            results = self.chroma_client.query_documents(
                collection_name=collection_name,
                query_embeddings=[query_embedding],
                n_results=top_k,
                where=chapter_filter
            )
            
            logger.info(f"âœ… æ£€ç´¢åˆ° {len(results.get('ids', [[]])[0])} ä¸ªç›¸ä¼¼å—")
            return results
            
        except Exception as e:
            logger.error(f"âŒ ç›¸ä¼¼å—æŸ¥è¯¢å¤±è´¥: {e}")
            return {'ids': [[]], 'documents': [[]], 'metadatas': [[]], 'distances': [[]]}


# å…¨å±€å‘é‡åŒ–æœåŠ¡å®ä¾‹
_embedding_service: Optional[EmbeddingService] = None


def get_embedding_service() -> EmbeddingService:
    """è·å–å…¨å±€å‘é‡åŒ–æœåŠ¡å®ä¾‹ï¼ˆå•ä¾‹ï¼‰"""
    global _embedding_service
    if _embedding_service is None:
        _embedding_service = EmbeddingService()
    return _embedding_service

