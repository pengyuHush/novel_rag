"""
æ¼”å˜åˆ†ææ¨¡å—

å®ç°è§’è‰²/å…³ç³»æ¼”å˜åˆ†æåŠŸèƒ½ï¼š
- æ—¶åºåˆ†æ®µæ£€ç´¢ï¼ˆæ—©æœŸ/ä¸­æœŸ/åæœŸï¼‰
- æ¼”å˜ç‚¹è¯†åˆ«ï¼ˆå…³é”®è½¬æŠ˜ï¼‰
- æ¼”å˜è½¨è¿¹ç”Ÿæˆ
"""

import logging
from typing import List, Dict, Optional, Tuple
from sqlalchemy.orm import Session

from app.models.database import Novel, Chapter
from app.services.embedding_service import get_embedding_service
from app.core.chromadb_client import get_chroma_client

logger = logging.getLogger(__name__)


class EvolutionAnalyzer:
    """æ¼”å˜åˆ†æå™¨"""
    
    # æ¼”å˜å…³é”®è¯ï¼ˆç”¨äºè¯†åˆ«è½¬æŠ˜ç‚¹ï¼‰
    EVOLUTION_KEYWORDS = [
        # æƒ…æ„Ÿå˜åŒ–
        "çˆ±ä¸Š", "è®¨åŒ", "æ¨", "å–œæ¬¢", "æ•¬ä½©", "é„™è§†", "èƒŒå›", "åŸè°…",
        # å…³ç³»å˜åŒ–
        "æˆä¸º", "å˜æˆ", "è½¬å˜", "æ”¹å˜", "ä¸å†", "å¼€å§‹", "ç»“æŸ",
        # èƒ½åŠ›å˜åŒ–
        "çªç ´", "é¢†æ‚Ÿ", "è§‰é†’", "å¤±å»", "è·å¾—", "æŒæ¡", "ä¸§å¤±",
        # èº«ä»½å˜åŒ–
        "ç»§æ‰¿", "æ¥ä»»", "ä¸Šä½", "é€€ä½", "æ™‹å‡", "é™èŒ",
        # æ€åº¦å˜åŒ–
        "æ„è¯†åˆ°", "å‘ç°", "æ˜ç™½", "äº†è§£", "è¯¯è§£", "æ€€ç–‘",
    ]
    
    def __init__(self):
        """åˆå§‹åŒ–æ¼”å˜åˆ†æå™¨"""
        self.embedding_service = get_embedding_service()
        self.chroma_client = get_chroma_client()
        
        logger.info("âœ… æ¼”å˜åˆ†æå™¨åˆå§‹åŒ–å®Œæˆ")
    
    def temporal_segmented_retrieval(
        self,
        db: Session,
        novel_id: int,
        query_embedding: List[float],
        top_k_per_period: int = 5
    ) -> Dict[str, List[Dict]]:
        """
        æ—¶åºåˆ†æ®µæ£€ç´¢
        
        å°†å°è¯´åˆ†ä¸ºæ—©æœŸ/ä¸­æœŸ/åæœŸä¸‰ä¸ªé˜¶æ®µï¼Œåˆ†åˆ«æ£€ç´¢
        
        Args:
            db: æ•°æ®åº“ä¼šè¯
            novel_id: å°è¯´ID
            query_embedding: æŸ¥è¯¢å‘é‡
            top_k_per_period: æ¯ä¸ªæ—¶æœŸè¿”å›çš„ç»“æœæ•°é‡
        
        Returns:
            Dict[str, List[Dict]]: æŒ‰æ—¶æœŸåˆ†ç»„çš„æ£€ç´¢ç»“æœ
        """
        # è·å–å°è¯´ç« èŠ‚ä¿¡æ¯
        novel = db.query(Novel).filter(Novel.id == novel_id).first()
        if not novel:
            logger.error(f"âŒ å°è¯´ä¸å­˜åœ¨: {novel_id}")
            return {"early": [], "middle": [], "late": []}
        
        total_chapters = novel.total_chapters
        
        # åˆ’åˆ†æ—¶æœŸï¼ˆæ—©æœŸ1/3ï¼Œä¸­æœŸ1/3ï¼ŒåæœŸ1/3ï¼‰
        early_end = total_chapters // 3
        middle_end = (total_chapters * 2) // 3
        
        periods = {
            "early": (1, early_end),
            "middle": (early_end + 1, middle_end),
            "late": (middle_end + 1, total_chapters)
        }
        
        logger.info(f"ğŸ“… æ—¶åºåˆ†æ®µ: æ—©æœŸ(1-{early_end}), ä¸­æœŸ({early_end+1}-{middle_end}), åæœŸ({middle_end+1}-{total_chapters})")
        
        results = {}
        collection_name = f"novel_{novel_id}"
        
        for period_name, (start_chapter, end_chapter) in periods.items():
            try:
                # æŒ‰ç« èŠ‚èŒƒå›´è¿‡æ»¤æ£€ç´¢
                period_results = self.chroma_client.query_documents(
                    collection_name=collection_name,
                    query_embeddings=[query_embedding],
                    n_results=top_k_per_period * 2,  # å¤šæ£€ç´¢ä¸€äº›ä»¥ä¾¿è¿‡æ»¤
                    where={
                        "chapter_num": {
                            "$gte": start_chapter,
                            "$lte": end_chapter
                        }
                    }
                )
                
                # è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
                documents = period_results.get('documents', [[]])[0]
                metadatas = period_results.get('metadatas', [[]])[0]
                distances = period_results.get('distances', [[]])[0]
                
                period_chunks = []
                for doc, metadata, distance in zip(documents, metadatas, distances):
                    period_chunks.append({
                        'content': doc,
                        'metadata': metadata,
                        'score': 1 - distance,
                        'period': period_name
                    })
                
                # æŒ‰åˆ†æ•°æ’åºï¼Œå–Top-K
                period_chunks.sort(key=lambda x: -x['score'])
                results[period_name] = period_chunks[:top_k_per_period]
                
                logger.info(f"âœ… {period_name}æœŸæ£€ç´¢å®Œæˆ: {len(results[period_name])} ä¸ªç»“æœ")
                
            except Exception as e:
                logger.error(f"âŒ {period_name}æœŸæ£€ç´¢å¤±è´¥: {e}")
                results[period_name] = []
        
        return results
    
    def identify_evolution_points(
        self,
        temporal_results: Dict[str, List[Dict]],
        threshold: float = 0.7
    ) -> List[Dict]:
        """
        è¯†åˆ«æ¼”å˜ç‚¹ï¼ˆå…³é”®è½¬æŠ˜ï¼‰
        
        é€šè¿‡å…³é”®è¯åŒ¹é…è¯†åˆ«å¯èƒ½çš„æ¼”å˜è½¬æŠ˜ç‚¹
        
        Args:
            temporal_results: æ—¶åºåˆ†æ®µæ£€ç´¢ç»“æœ
            threshold: ç›¸å…³æ€§é˜ˆå€¼ï¼ˆä½äºæ­¤å€¼çš„ç»“æœå°†è¢«è¿‡æ»¤ï¼‰
        
        Returns:
            List[Dict]: æ¼”å˜ç‚¹åˆ—è¡¨
        """
        evolution_points = []
        
        for period, chunks in temporal_results.items():
            for chunk in chunks:
                # æ£€æŸ¥æ˜¯å¦åŒ…å«æ¼”å˜å…³é”®è¯
                content = chunk['content']
                matched_keywords = []
                
                for keyword in self.EVOLUTION_KEYWORDS:
                    if keyword in content:
                        matched_keywords.append(keyword)
                
                # å¦‚æœåŒ…å«æ¼”å˜å…³é”®è¯ä¸”åˆ†æ•°è¶³å¤Ÿé«˜ï¼Œæ ‡è®°ä¸ºæ¼”å˜ç‚¹
                if matched_keywords and chunk['score'] >= threshold:
                    evolution_points.append({
                        'period': period,
                        'chapter_num': chunk['metadata'].get('chapter_num'),
                        'chapter_title': chunk['metadata'].get('chapter_title'),
                        'content': content,
                        'keywords': matched_keywords,
                        'score': chunk['score'],
                        'is_key_point': True
                    })
        
        # æŒ‰ç« èŠ‚å·æ’åº
        evolution_points.sort(key=lambda x: x['chapter_num'])
        
        logger.info(f"âœ… è¯†åˆ«åˆ° {len(evolution_points)} ä¸ªæ¼”å˜ç‚¹")
        
        return evolution_points
    
    def generate_evolution_trajectory(
        self,
        db: Session,
        novel_id: int,
        query: str,
        entity_name: Optional[str] = None
    ) -> Dict:
        """
        ç”Ÿæˆæ¼”å˜è½¨è¿¹
        
        å®Œæ•´çš„æ¼”å˜åˆ†ææµç¨‹ï¼š
        1. æ—¶åºåˆ†æ®µæ£€ç´¢
        2. è¯†åˆ«æ¼”å˜ç‚¹
        3. ç”Ÿæˆè½¨è¿¹æ‘˜è¦
        
        Args:
            db: æ•°æ®åº“ä¼šè¯
            novel_id: å°è¯´ID
            query: æŸ¥è¯¢æ–‡æœ¬ï¼ˆå¦‚"è§ç‚çš„å®åŠ›å¦‚ä½•æ¼”å˜"ï¼‰
            entity_name: å¯é€‰ï¼ŒæŒ‡å®šå®ä½“åç§°
        
        Returns:
            Dict: æ¼”å˜è½¨è¿¹åˆ†æç»“æœ
        """
        logger.info(f"ğŸ“ˆ å¼€å§‹æ¼”å˜åˆ†æ: {query}")
        
        # 1. æŸ¥è¯¢å‘é‡åŒ–
        from app.services.zhipu_client import get_zhipu_client
        zhipu_client = get_zhipu_client()
        query_embedding = zhipu_client.embed_text(query)
        
        # 2. æ—¶åºåˆ†æ®µæ£€ç´¢
        temporal_results = self.temporal_segmented_retrieval(
            db, novel_id, query_embedding, top_k_per_period=5
        )
        
        # 3. è¯†åˆ«æ¼”å˜ç‚¹
        evolution_points = self.identify_evolution_points(temporal_results)
        
        # 4. æŒ‰æ—¶æœŸæ±‡æ€»
        summary_by_period = {}
        for period in ["early", "middle", "late"]:
            period_chunks = temporal_results.get(period, [])
            period_evolution_points = [
                ep for ep in evolution_points if ep['period'] == period
            ]
            
            summary_by_period[period] = {
                'total_chunks': len(period_chunks),
                'evolution_points': len(period_evolution_points),
                'key_chapters': list(set([
                    ep['chapter_num'] for ep in period_evolution_points
                ])),
                'keywords': list(set([
                    kw for ep in period_evolution_points for kw in ep['keywords']
                ]))
            }
        
        # 5. æ„å»ºå®Œæ•´è½¨è¿¹
        trajectory = {
            'query': query,
            'entity_name': entity_name,
            'temporal_results': temporal_results,
            'evolution_points': evolution_points,
            'summary_by_period': summary_by_period,
            'total_evolution_points': len(evolution_points)
        }
        
        logger.info(f"âœ… æ¼”å˜åˆ†æå®Œæˆ: {len(evolution_points)} ä¸ªå…³é”®ç‚¹")
        
        return trajectory
    
    def extract_entity_from_query(self, query: str) -> Optional[str]:
        """
        ä»æŸ¥è¯¢ä¸­æå–å®ä½“åç§°
        
        ä½¿ç”¨ç®€å•çš„æ­£åˆ™åŒ¹é…æå–äººå/ç»„ç»‡å
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
        
        Returns:
            Optional[str]: å®ä½“åç§°
        """
        import re
        
        # åŒ¹é…æ¨¡å¼ï¼š"XXXçš„YYY"ï¼Œ"XXXå¦‚ä½•"ï¼Œ"XXXä¸ºä»€ä¹ˆ"
        patterns = [
            r'([^çš„]+?)çš„',
            r'([^å¦‚ä½•]+?)å¦‚ä½•',
            r'([^ä¸ºä»€ä¹ˆ]+?)ä¸ºä»€ä¹ˆ',
            r'([^æ€ä¹ˆ]+?)æ€ä¹ˆ',
        ]
        
        for pattern in patterns:
            match = re.search(pattern, query)
            if match:
                entity = match.group(1).strip()
                # è¿‡æ»¤æ— æ•ˆæå–
                if len(entity) >= 2 and len(entity) <= 10:
                    return entity
        
        return None


# å…¨å±€æ¼”å˜åˆ†æå™¨å®ä¾‹
_evolution_analyzer: Optional[EvolutionAnalyzer] = None


def get_evolution_analyzer() -> EvolutionAnalyzer:
    """è·å–å…¨å±€æ¼”å˜åˆ†æå™¨å®ä¾‹ï¼ˆå•ä¾‹ï¼‰"""
    global _evolution_analyzer
    if _evolution_analyzer is None:
        _evolution_analyzer = EvolutionAnalyzer()
    return _evolution_analyzer

