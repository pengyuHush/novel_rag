"""
å®ä½“å±æ€§æå–å™¨ - ä»ä¸Šä¸‹æ–‡ä¸­æå–å®ä½“å±æ€§

åŠŸèƒ½:
- æå–è§’è‰²æ€§åˆ«
- æå–è§’è‰²é˜µè¥/åŠ¿åŠ›
- æå–è§’è‰²èŒä¸š/èº«ä»½
- æå–è§’è‰²å®åŠ›ç­‰çº§
"""

import json
import asyncio
import logging
from typing import List, Dict, Optional, Tuple

from app.services.zhipu_client import get_zhipu_client
from app.services.batch_api_client import get_batch_client
from app.core.config import settings

logger = logging.getLogger(__name__)


class EntityAttributeExtractor:
    """æå–å®ä½“çš„å±æ€§ï¼ˆæ€§åˆ«ã€é˜µè¥ç­‰ï¼‰"""
    
    def __init__(self):
        """åˆå§‹åŒ–å±æ€§æå–å™¨"""
        self.llm_client = get_zhipu_client()
    
    async def extract_attributes(
        self,
        entity_name: str,
        entity_type: str,
        contexts: List[str]
    ) -> Dict:
        """
        ä»ä¸Šä¸‹æ–‡ä¸­æå–å®ä½“å±æ€§
        
        Args:
            entity_name: å®ä½“åç§°
            entity_type: å®ä½“ç±»å‹ï¼ˆcharacters/locations/organizationsï¼‰
            contexts: å®ä½“å‡ºç°çš„å…¸å‹ä¸Šä¸‹æ–‡ï¼ˆ3-5ä¸ªç‰‡æ®µï¼‰
        
        Returns:
            {
                "æ€§åˆ«": "ç”·",
                "é˜µè¥": "ä¸»è§’æ–¹",
                "èŒä¸š": "ç‚¼è¯å¸ˆ",
                "å®åŠ›": "æ–—åœ£"
            }
        """
        if entity_type != 'characters':
            return {}  # ä»…å¤„ç†è§’è‰²
        
        if not contexts or len(contexts) == 0:
            return {}
        
        # æ„å»ºæç¤ºè¯
        context_text = ""
        for i, ctx in enumerate(contexts[:3], 1):
            context_text += f"\nã€ç‰‡æ®µ{i}ã€‘{ctx[:200]}\n"
        
        prompt = f"""ä½ æ˜¯ç½‘ç»œå°è¯´è§’è‰²åˆ†æä¸“å®¶ã€‚è¯·ä»ä»¥ä¸‹æ–‡æœ¬ä¸­æå–"{entity_name}"çš„åŸºæœ¬å±æ€§ã€‚

æ–‡æœ¬ç‰‡æ®µï¼š
{context_text}

è¯·æå–ä»¥ä¸‹å±æ€§ï¼ˆå¦‚æ–‡æœ¬ä¸­æœªæ˜ç¡®æåŠåˆ™çœç•¥è¯¥å­—æ®µï¼‰ï¼š
- **æ€§åˆ«**ï¼šç”·/å¥³/æœªçŸ¥
- **é˜µè¥**ï¼šè§’è‰²æ‰€å±çš„åŠ¿åŠ›ã€é—¨æ´¾æˆ–é˜µè¥
- **èŒä¸š**ï¼šè§’è‰²çš„èŒä¸šã€èº«ä»½æˆ–è§’è‰²å®šä½
- **å®åŠ›**ï¼šè§’è‰²çš„å®åŠ›ç­‰çº§ã€ä¿®ä¸ºå¢ƒç•Œ

**è¦æ±‚**ï¼š
1. åªæå–æ–‡æœ¬ä¸­æ˜ç¡®æåˆ°çš„ä¿¡æ¯
2. ä¸è¦æ¨æµ‹æˆ–çŒœæµ‹
3. å¦‚æœæŸä¸ªå±æ€§å®Œå…¨æ²¡æœ‰æåŠï¼Œä¸è¦åŒ…å«è¯¥å­—æ®µ
4. ä¿æŒç®€æ´ï¼Œæ¯ä¸ªå±æ€§ä¸è¶…è¿‡10ä¸ªå­—

**è¾“å‡ºæ ¼å¼ï¼ˆå¿…é¡»æ˜¯çº¯JSONï¼‰**ï¼š
{{"æ€§åˆ«": "ç”·", "é˜µè¥": "è§å®¶", "èŒä¸š": "ç‚¼è¯å¸ˆ", "å®åŠ›": "æ–—è€…"}}

å¦‚æœæ²¡æœ‰ä»»ä½•å±æ€§ä¿¡æ¯ï¼Œè¾“å‡ºç©ºå¯¹è±¡ï¼š{{}}

è¯·åˆ†æï¼š"""
        
        content = ""  # åˆå§‹åŒ–ï¼Œé¿å…åœ¨å¼‚å¸¸å¤„ç†ä¸­æœªå®šä¹‰
        try:
            response = await asyncio.to_thread(
                self.llm_client.chat_completion,
                messages=[{"role": "user", "content": prompt}],
                model="GLM-4.5-Flash",
                max_tokens=256,  # å¢åŠ tokené™åˆ¶ï¼Œé¿å…JSONè¢«æˆªæ–­
                temperature=0.3,
                thinking={"type": "disabled"}  # ç¦ç”¨æ€è€ƒæ¨¡å¼ï¼Œç›´æ¥è¾“å‡ºJSON
            )
            
            content = response.get('content', '').strip()
            
            # æ£€æŸ¥ç©ºå“åº”
            if not content:
                logger.warning(f"LLMè¿”å›ç©ºå†…å®¹ï¼Œå®ä½“: {entity_name}")
                return {}
            
            # æå–JSONï¼ˆå¯èƒ½åŒ…å«åœ¨ä»£ç å—ä¸­ï¼‰
            if '```json' in content:
                content = content.split('```json')[1].split('```')[0].strip()
            elif '```' in content:
                content = content.split('```')[1].split('```')[0].strip()
            
            # å†æ¬¡æ£€æŸ¥æå–åæ˜¯å¦ä¸ºç©º
            if not content:
                logger.warning(f"æå–JSONåä¸ºç©ºï¼ŒåŸå§‹å†…å®¹: {response.get('content', '')[:100]}")
                return {}
            
            attributes = json.loads(content)
            
            # éªŒè¯å¹¶æ¸…ç†å±æ€§
            valid_attributes = {}
            if isinstance(attributes, dict):
                for key, value in attributes.items():
                    if value and isinstance(value, str) and len(value) <= 20:
                        valid_attributes[key] = value
            
            return valid_attributes
            
        except json.JSONDecodeError as e:
            logger.warning(f"JSONè§£æå¤±è´¥: {e}, å†…å®¹: {content[:100]}")
            return {}
        except Exception as e:
            logger.error(f"å±æ€§æå–å¤±è´¥: {e}")
            return {}
    
    async def extract_batch(
        self,
        tasks: List[tuple],
        max_concurrency: Optional[int] = None,
        use_batch_api: bool = False
    ) -> Tuple[List[Dict], Dict]:
        """
        æ‰¹é‡æå–å®ä½“å±æ€§
        
        Args:
            tasks: [(entity_name, entity_type, contexts), ...]
            max_concurrency: æœ€å¤§å¹¶å‘æ•°ï¼ˆä»…åœ¨use_batch_api=Falseæ—¶ç”Ÿæ•ˆï¼‰ï¼Œé»˜è®¤ä½¿ç”¨é…ç½®å€¼
            use_batch_api: æ˜¯å¦ä½¿ç”¨Batch API
        
        Returns:
            Tuple[List[Dict], Dict]: (å±æ€§å­—å…¸åˆ—è¡¨, tokenç»Ÿè®¡)
        """
        if use_batch_api:
            return await self._extract_batch_with_batch_api(tasks)
        
        # ä½¿ç”¨é€Ÿç‡é™åˆ¶çš„å¹¶å‘æ§åˆ¶
        max_concurrency = max_concurrency or settings.graph_attribute_concurrency
        
        logger.info(f"ğŸ“Š åˆ†æ‰¹å¤„ç† {len(tasks)} ä¸ªå®ä½“å±æ€§æå–ï¼ˆå¹¶å‘æ•°ï¼š{max_concurrency}ï¼Œæ¯æ‰¹é—´éš”1ç§’ï¼‰...")
        
        # åˆ†æ‰¹å¤„ç†ï¼Œæ¯æ‰¹max_concurrencyä¸ªä»»åŠ¡
        results = []
        for batch_idx in range(0, len(tasks), max_concurrency):
            batch = tasks[batch_idx:batch_idx + max_concurrency]
            batch_num = batch_idx // max_concurrency + 1
            total_batches = (len(tasks) + max_concurrency - 1) // max_concurrency
            
            logger.info(f"  å¤„ç†ç¬¬ {batch_num}/{total_batches} æ‰¹ ({len(batch)} ä¸ªä»»åŠ¡)...")
            
            # æ‰¹å†…ä»»åŠ¡å¹¶å‘æ‰§è¡Œ
            batch_results = await asyncio.gather(
                *[self.extract_attributes(entity_name, entity_type, contexts) 
                  for entity_name, entity_type, contexts in batch],
                return_exceptions=True
            )
            
            results.extend(batch_results)
            
            # æ‰¹æ¬¡é—´å»¶è¿Ÿï¼Œé¿å…æŒç»­é«˜é¢‘è¯·æ±‚
            if batch_idx + max_concurrency < len(tasks):
                await asyncio.sleep(1.0)
        
        logger.info(f"âœ… æ‰€æœ‰æ‰¹æ¬¡å¤„ç†å®Œæˆ")
        
        # å¤„ç†å¼‚å¸¸ç»“æœ
        valid_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                logger.error(f"å±æ€§æå–ä»»åŠ¡ {i} å¤±è´¥: {result}")
                valid_results.append({})
            else:
                valid_results.append(result)
        
        # ç»Ÿè®¡æå–ç»“æœ
        extracted_count = sum(1 for r in valid_results if r)
        logger.info(f"âœ… å±æ€§æå–å®Œæˆï¼ŒæˆåŠŸæå– {extracted_count}/{len(tasks)} ä¸ªå®ä½“çš„å±æ€§")
        
        # å®æ—¶APIæ¨¡å¼ï¼šä¼°ç®—tokenæ¶ˆè€—
        from app.utils.token_counter import get_token_counter
        token_counter = get_token_counter()
        
        total_input_tokens = 0
        for entity_name, entity_type, contexts in tasks:
            # ä¼°ç®—prompt tokenï¼ˆåŒ…å«æŒ‡ä»¤+ä¸Šä¸‹æ–‡ï¼‰
            prompt = f"è§’è‰²åï¼š{entity_name}\nç±»å‹ï¼š{entity_type}\nä¸Šä¸‹æ–‡ï¼š{''.join(contexts)}"
            total_input_tokens += token_counter.count_tokens(prompt)
        
        # ä¼°ç®—è¾“å‡ºtokenï¼ˆå±æ€§é€šå¸¸è¾ƒçŸ­ï¼Œå¹³å‡100 tokensï¼‰
        total_output_tokens = extracted_count * 100
        
        token_stats = {
            'input_tokens': total_input_tokens,
            'output_tokens': total_output_tokens,
            'total_tokens': total_input_tokens + total_output_tokens
        }
        
        logger.info(f"ğŸ“Š ä¼°ç®—Tokenæ¶ˆè€—: input={total_input_tokens}, output={total_output_tokens}, total={token_stats['total_tokens']}")
        
        return valid_results, token_stats
    
    async def _extract_batch_with_batch_api(
        self,
        tasks: List[tuple]
    ) -> Tuple[List[Dict], Dict]:
        """
        ä½¿ç”¨Batch APIæ‰¹é‡æå–å±æ€§
        
        Args:
            tasks: [(entity_name, entity_type, contexts), ...]
        
        Returns:
            Tuple[List[Dict], Dict]: (å±æ€§å­—å…¸åˆ—è¡¨, tokenç»Ÿè®¡)
        """
        logger.info(f"ğŸš€ ä½¿ç”¨Batch APIæå– {len(tasks)} ä¸ªå®ä½“çš„å±æ€§ï¼ˆæ— å¹¶å‘é™åˆ¶ï¼Œå…è´¹ï¼‰...")
        
        # æ£€æŸ¥æ˜¯å¦è¶…è¿‡æ™ºè°±AI Batch APIé™åˆ¶ï¼ˆChatæ¨¡å‹ï¼š50,000ä¸ªè¯·æ±‚/æ‰¹æ¬¡ï¼‰
        if len(tasks) > 50000:
            logger.error(f"âŒ å®ä½“æ•°é‡({len(tasks)})è¶…è¿‡Batch APIé™åˆ¶(50,000)ï¼Œè¿™ç§æƒ…å†µæå…¶ç½•è§")
            # ç†è®ºä¸Šä¸ä¼šå‘ç”Ÿï¼ˆå°è¯´é€šå¸¸åªæœ‰å‡ ç™¾ä¸ªå®ä½“ï¼‰ï¼Œä½†æ·»åŠ é˜²æŠ¤
            logger.error(f"   å»ºè®®ï¼šè”ç³»å¼€å‘è€…æˆ–æ‰‹åŠ¨å…³é—­Batch APIæ¨¡å¼")
            raise ValueError(f"å®ä½“æ•°é‡è¶…è¿‡Batch APIé™åˆ¶: {len(tasks)} > 50,000")
        
        # 1. æ„å»ºBatch APIä»»åŠ¡
        batch_tasks = []
        
        for i, (entity_name, entity_type, contexts) in enumerate(tasks):
            if entity_type != 'characters':
                continue  # ä»…å¤„ç†è§’è‰²
            
            # æ„å»ºPrompt
            context_text = ""
            for j, ctx in enumerate(contexts[:3], 1):
                context_text += f"\nã€ç‰‡æ®µ{j}ã€‘{ctx[:200]}\n"
            
            prompt = f"""ä½ æ˜¯ç½‘ç»œå°è¯´è§’è‰²åˆ†æä¸“å®¶ã€‚è¯·ä»ä»¥ä¸‹æ–‡æœ¬ä¸­æå–"{entity_name}"çš„åŸºæœ¬å±æ€§ã€‚

æ–‡æœ¬ç‰‡æ®µï¼š
{context_text}

è¯·æå–ä»¥ä¸‹å±æ€§ï¼ˆå¦‚æ–‡æœ¬ä¸­æœªæ˜ç¡®æåŠåˆ™çœç•¥è¯¥å­—æ®µï¼‰ï¼š
- **æ€§åˆ«**ï¼šç”·/å¥³/æœªçŸ¥
- **é˜µè¥**ï¼šè§’è‰²æ‰€å±çš„åŠ¿åŠ›ã€é—¨æ´¾æˆ–é˜µè¥
- **èŒä¸š**ï¼šè§’è‰²çš„èŒä¸šã€èº«ä»½æˆ–è§’è‰²å®šä½
- **å®åŠ›**ï¼šè§’è‰²çš„å®åŠ›ç­‰çº§ã€ä¿®ä¸ºå¢ƒç•Œ

**è¦æ±‚**ï¼š
1. åªæå–æ–‡æœ¬ä¸­æ˜ç¡®æåˆ°çš„ä¿¡æ¯
2. ä¸è¦æ¨æµ‹æˆ–çŒœæµ‹
3. å¦‚æœæŸä¸ªå±æ€§å®Œå…¨æ²¡æœ‰æåŠï¼Œä¸è¦åŒ…å«è¯¥å­—æ®µ
4. ä¿æŒç®€æ´ï¼Œæ¯ä¸ªå±æ€§ä¸è¶…è¿‡10ä¸ªå­—

**è¾“å‡ºæ ¼å¼ï¼ˆå¿…é¡»æ˜¯çº¯JSONï¼‰**ï¼š
{{"æ€§åˆ«": "ç”·", "é˜µè¥": "è§å®¶", "èŒä¸š": "ç‚¼è¯å¸ˆ", "å®åŠ›": "æ–—è€…"}}

å¦‚æœæ²¡æœ‰ä»»ä½•å±æ€§ä¿¡æ¯ï¼Œè¾“å‡ºç©ºå¯¹è±¡ï¼š{{}}

è¯·åˆ†æï¼š"""
            
            custom_id = f"attribute-{i}-{entity_name}"
            
            batch_tasks.append({
                "custom_id": custom_id,
                "method": "POST",
                "url": "/v4/chat/completions",
                "body": {
                    "model": "glm-4-flash",
                    "messages": [{"role": "user", "content": prompt}],
                    "temperature": 0.3,
                    "max_tokens": 150
                }
            })
        
        if not batch_tasks:
            logger.info("æ²¡æœ‰éœ€è¦æå–å±æ€§çš„è§’è‰²")
            empty_stats = {'input_tokens': 0, 'output_tokens': 0, 'total_tokens': 0}
            return [{}] * len(tasks), empty_stats
        
        # 2. æäº¤Batch API
        batch_client = get_batch_client()
        
        def progress_callback(batch_id, status, progress, completed, total, failed):
            logger.info(f"ğŸ“Š Batch APIè¿›åº¦: {status} | {completed}/{total} ({progress*100:.1f}%) | å¤±è´¥: {failed}")
        
        try:
            results_map, token_stats = await asyncio.to_thread(
                batch_client.submit_and_wait,
                batch_tasks,
                check_interval=30,
                progress_callback=progress_callback
            )
        except Exception as e:
            logger.error(f"âŒ Batch APIè°ƒç”¨å¤±è´¥: {e}")
            empty_stats = {'input_tokens': 0, 'output_tokens': 0, 'total_tokens': 0}
            return [{}] * len(tasks), empty_stats
        
        # 3. è§£æç»“æœå¹¶æŒ‰åŸé¡ºåºè¿”å›
        valid_results = []
        for i, (entity_name, entity_type, contexts) in enumerate(tasks):
            if entity_type != 'characters':
                valid_results.append({})
                continue
            
            custom_id = f"attribute-{i}-{entity_name}"
            
            if custom_id in results_map:
                result = results_map[custom_id]
                
                if result['status'] == 'success':
                    try:
                        content = result['content'].strip()
                        
                        if not content:
                            valid_results.append({})
                            continue
                        
                        # æå–JSON
                        if '```json' in content:
                            content = content.split('```json')[1].split('```')[0].strip()
                        elif '```' in content:
                            content = content.split('```')[1].split('```')[0].strip()
                        
                        if not content:
                            valid_results.append({})
                            continue
                        
                        attributes = json.loads(content)
                        
                        # éªŒè¯å¹¶æ¸…ç†å±æ€§
                        valid_attrs = {}
                        if isinstance(attributes, dict):
                            for key, value in attributes.items():
                                if value and isinstance(value, str) and len(value) <= 20:
                                    valid_attrs[key] = value
                        
                        valid_results.append(valid_attrs)
                    except Exception as e:
                        logger.warning(f"è§£æå±æ€§å¤±è´¥: {e}")
                        valid_results.append({})
                else:
                    valid_results.append({})
            else:
                valid_results.append({})
        
        # ç»Ÿè®¡æå–ç»“æœ
        extracted_count = sum(1 for r in valid_results if r)
        logger.info(f"âœ… Batch APIå±æ€§æå–å®Œæˆï¼ŒæˆåŠŸæå– {extracted_count}/{len(tasks)} ä¸ªå®ä½“çš„å±æ€§")
        logger.info(f"ğŸ“Š å±æ€§æå–Tokenç»Ÿè®¡: {token_stats}")
        
        return valid_results, token_stats


# å…¨å±€å®ä¾‹
_attribute_extractor = None

def get_attribute_extractor() -> EntityAttributeExtractor:
    """è·å–å±æ€§æå–å™¨å•ä¾‹"""
    global _attribute_extractor
    if _attribute_extractor is None:
        _attribute_extractor = EntityAttributeExtractor()
    return _attribute_extractor

