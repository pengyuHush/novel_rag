"""
å…³ç³»æ¼”å˜è¿½è¸ªå™¨ - è¿½è¸ªå®ä½“é—´å…³ç³»éšæ—¶é—´çš„æ¼”å˜

åŠŸèƒ½:
- å°†å…³ç³»æ—¶é—´è·¨åº¦åˆ†æ®µ
- æ£€æµ‹æ¯æ®µçš„å…³ç³»ç±»å‹
- è¯†åˆ«å…³ç³»ç±»å‹å˜åŒ–èŠ‚ç‚¹
- å»é‡ä¼˜åŒ–
"""

import logging
from typing import List, Dict, Tuple
from sqlalchemy.orm import Session

from app.services.graph.relation_classifier import RelationshipClassifier
from app.models.database import Novel

logger = logging.getLogger(__name__)


class RelationshipEvolutionTracker:
    """è¿½è¸ªå…³ç³»éšæ—¶é—´çš„æ¼”å˜"""
    
    def __init__(self):
        """åˆå§‹åŒ–æ¼”å˜è¿½è¸ªå™¨"""
        self.classifier = RelationshipClassifier()
    
    async def track_evolution(
        self,
        entity1: str,
        entity2: str,
        all_chapters: List[int],
        novel: Novel,
        db: Session
    ) -> List[Dict]:
        """
        å°†ç« èŠ‚åˆ†æ®µï¼Œæ£€æµ‹æ¯æ®µçš„å…³ç³»ç±»å‹
        
        Args:
            entity1: å®ä½“1åç§°
            entity2: å®ä½“2åç§°
            all_chapters: æ‰€æœ‰å…±ç°ç« èŠ‚åˆ—è¡¨
            novel: å°è¯´å¯¹è±¡
            db: æ•°æ®åº“ä¼šè¯
        
        Returns:
            [
                {"chapter": 10, "type": "é™Œç”Ÿ", "confidence": 0.8},
                {"chapter": 50, "type": "æœ‹å‹", "confidence": 0.9},
                {"chapter": 120, "type": "ç›Ÿå‹", "confidence": 0.95}
            ]
        """
        if not all_chapters:
            return []
        
        # åˆ†æ®µç­–ç•¥ï¼šæŒ‰ç« èŠ‚æ•°é‡åŠ¨æ€åˆ†æ®µ
        total_span = max(all_chapters) - min(all_chapters)
        
        if total_span <= 50:
            segments = 2  # æ—©æœŸ/åæœŸ
        elif total_span <= 200:
            segments = 3  # æ—©æœŸ/ä¸­æœŸ/åæœŸ
        else:
            segments = 5  # ç»†ç²’åº¦åˆ†æ®µ
        
        segment_size = len(all_chapters) // segments
        if segment_size == 0:
            segment_size = 1
        
        evolution = []
        
        logger.debug(f"è¿½è¸ª {entity1}-{entity2} çš„æ¼”å˜ï¼Œåˆ†{segments}æ®µï¼Œæ¯æ®µçº¦{segment_size}ç« ")
        
        for i in range(segments):
            start_idx = i * segment_size
            end_idx = start_idx + segment_size if i < segments - 1 else len(all_chapters)
            segment_chapters = all_chapters[start_idx:end_idx]
            
            if not segment_chapters:
                continue
            
            # æ™ºèƒ½é‡‡æ ·è¯¥æ®µçš„ç« èŠ‚ï¼ˆå–3ä¸ªæ ·æœ¬ï¼‰
            sampled_chapters = self.classifier._smart_chapter_sampling(segment_chapters, max_samples=3)
            
            # æå–è¯¥æ®µçš„ä¸Šä¸‹æ–‡
            from app.services.indexing_service import get_indexing_service
            indexing_service = get_indexing_service()
            
            contexts = await indexing_service._extract_cooccurrence_contexts(
                entity1, entity2, sampled_chapters, novel, db
            )
            
            if not contexts:
                logger.debug(f"æ®µ{i+1}æ— æ³•æå–ä¸Šä¸‹æ–‡ï¼Œè·³è¿‡")
                continue
            
            # åˆ†ç±»è¯¥æ®µçš„å…³ç³»ç±»å‹
            chapter_range = f"ç¬¬{min(segment_chapters)}ç« -ç¬¬{max(segment_chapters)}ç« "
            classification = await self.classifier.classify_relationship(
                entity1, entity2, contexts, len(segment_chapters), chapter_range
            )
            
            # è®°å½•æ¼”å˜ç‚¹ï¼ˆä½¿ç”¨è¯¥æ®µçš„èµ·å§‹ç« èŠ‚ï¼‰
            evolution.append({
                "chapter": segment_chapters[0],
                "type": classification['relation_type'],
                "confidence": classification['confidence']
            })
            
            logger.debug(f"æ®µ{i+1}ï¼ˆç¬¬{segment_chapters[0]}ç« ï¼‰: {classification['relation_type']} (ç½®ä¿¡åº¦{classification['confidence']:.2f})")
        
        # å»é‡ï¼šåªä¿ç•™å…³ç³»ç±»å‹å˜åŒ–çš„èŠ‚ç‚¹
        deduplicated = []
        if evolution:
            deduplicated.append(evolution[0])  # ä¿ç•™èµ·å§‹ç‚¹
            
            for i in range(1, len(evolution)):
                if evolution[i]['type'] != evolution[i-1]['type']:
                    deduplicated.append(evolution[i])
        
        logger.info(f"âœ… {entity1}-{entity2} æ¼”å˜è¿½è¸ªå®Œæˆ: {len(evolution)}æ®µ -> {len(deduplicated)}ä¸ªå˜åŒ–ç‚¹")
        
        return deduplicated
    
    async def track_batch(
        self,
        tasks: List[Tuple],
        novel: Novel,
        db: Session,
        use_batch_api: bool = False
    ) -> Tuple[Dict[Tuple[str, str], List[Dict]], Dict]:
        """
        æ‰¹é‡è¿½è¸ªå¤šå¯¹å…³ç³»çš„æ¼”å˜ï¼ˆæ”¯æŒ Batch APIï¼‰
        
        Args:
            tasks: [(entity1, entity2, chapters), ...]
            novel: å°è¯´å¯¹è±¡
            db: æ•°æ®åº“ä¼šè¯
            use_batch_api: æ˜¯å¦ä½¿ç”¨ Batch API
        
        Returns:
            Tuple[Dict, Dict]: (æ¼”å˜ç»“æœå­—å…¸, tokenç»Ÿè®¡)
            - {(entity1, entity2): evolution_list, ...}
            - tokenç»Ÿè®¡
        """
        from app.core.config import settings
        
        # ç¬¬1æ­¥ï¼šæ”¶é›†æ‰€æœ‰éœ€è¦åˆ†ç±»çš„æ®µ
        logger.info(f"ğŸ” å¼€å§‹è¿½è¸ª {len(tasks)} å¯¹å…³ç³»çš„æ¼”å˜...")
        
        all_classification_tasks = []  # [(entity1, entity2, contexts, count, chapter_range, segment_info), ...]
        segment_mapping = []  # è®°å½•æ¯ä¸ªåˆ†ç±»ä»»åŠ¡å¯¹åº”çš„å…³ç³»å¯¹å’Œæ®µç´¢å¼•
        
        for entity1, entity2, all_chapters in tasks:
            if not all_chapters:
                segment_mapping.append([])
                continue
            
            # åˆ†æ®µç­–ç•¥
            total_span = max(all_chapters) - min(all_chapters)
            if total_span <= 50:
                segments = 2
            elif total_span <= 200:
                segments = 3
            else:
                segments = 5
            
            segment_size = len(all_chapters) // segments
            if segment_size == 0:
                segment_size = 1
            
            relation_segments = []
            
            for i in range(segments):
                start_idx = i * segment_size
                end_idx = start_idx + segment_size if i < segments - 1 else len(all_chapters)
                segment_chapters = all_chapters[start_idx:end_idx]
                
                if not segment_chapters:
                    continue
                
                # æ™ºèƒ½é‡‡æ ·
                sampled_chapters = self.classifier._smart_chapter_sampling(segment_chapters, max_samples=3)
                
                # æå–ä¸Šä¸‹æ–‡
                from app.services.indexing_service import get_indexing_service
                indexing_service = get_indexing_service()
                
                contexts = await indexing_service._extract_cooccurrence_contexts(
                    entity1, entity2, sampled_chapters, novel, db
                )
                
                if not contexts:
                    continue
                
                chapter_range = f"ç¬¬{min(segment_chapters)}ç« -ç¬¬{max(segment_chapters)}ç« "
                
                # æ·»åŠ åˆ°æ‰¹é‡ä»»åŠ¡
                all_classification_tasks.append((
                    entity1, entity2, contexts, len(segment_chapters), chapter_range
                ))
                
                # è®°å½•æ®µä¿¡æ¯
                relation_segments.append({
                    'task_index': len(all_classification_tasks) - 1,
                    'start_chapter': segment_chapters[0],
                    'segment_chapters': segment_chapters
                })
            
            segment_mapping.append(relation_segments)
        
        logger.info(f"ğŸ“Š å…±æ”¶é›† {len(all_classification_tasks)} ä¸ªåˆ†ç±»ä»»åŠ¡")
        
        # ç¬¬2æ­¥ï¼šæ‰¹é‡åˆ†ç±»ï¼ˆæ”¯æŒ Batch API å’Œæ™ºèƒ½é˜ˆå€¼ï¼‰
        if len(all_classification_tasks) == 0:
            return {}, {'input_tokens': 0, 'output_tokens': 0, 'total_tokens': 0}
        
        # ğŸ¯ æ™ºèƒ½åˆ¤æ–­ï¼šè¯·æ±‚æ•° < é˜ˆå€¼æ—¶ä½¿ç”¨å®æ—¶API
        if len(all_classification_tasks) < settings.batch_api_threshold:
            logger.info(f"ğŸ“Š æ¼”å˜è¿½è¸ªåˆ†ç±»: è¯·æ±‚æ•°({len(all_classification_tasks)}) < é˜ˆå€¼({settings.batch_api_threshold})ï¼Œä½¿ç”¨å®æ—¶API")
            use_batch_api = False
        elif use_batch_api:
            logger.info(f"ğŸ“Š æ¼”å˜è¿½è¸ªåˆ†ç±»: è¯·æ±‚æ•°({len(all_classification_tasks)}) â‰¥ é˜ˆå€¼({settings.batch_api_threshold})ï¼Œä½¿ç”¨Batch API")
        
        classifications, token_stats = await self.classifier.classify_batch(
            all_classification_tasks,
            use_batch_api=use_batch_api
        )
        
        # ç¬¬3æ­¥ï¼šç»„ç»‡ç»“æœ
        results = {}
        task_idx = 0
        
        for (entity1, entity2, all_chapters), relation_segments in zip(tasks, segment_mapping):
            if not relation_segments:
                results[(entity1, entity2)] = []
                continue
            
            evolution = []
            for segment_info in relation_segments:
                classification = classifications[segment_info['task_index']]
                evolution.append({
                    "chapter": segment_info['start_chapter'],
                    "type": classification['relation_type'],
                    "confidence": classification['confidence']
                })
            
            # å»é‡ï¼šåªä¿ç•™å…³ç³»ç±»å‹å˜åŒ–çš„èŠ‚ç‚¹
            deduplicated = []
            if evolution:
                deduplicated.append(evolution[0])
                for i in range(1, len(evolution)):
                    if evolution[i]['type'] != evolution[i-1]['type']:
                        deduplicated.append(evolution[i])
            
            results[(entity1, entity2)] = deduplicated
            logger.info(f"âœ… {entity1}-{entity2} æ¼”å˜è¿½è¸ªå®Œæˆ: {len(evolution)}æ®µ -> {len(deduplicated)}ä¸ªå˜åŒ–ç‚¹")
        
        return results, token_stats


# å…¨å±€å®ä¾‹
_evolution_tracker = None

def get_evolution_tracker() -> RelationshipEvolutionTracker:
    """è·å–æ¼”å˜è¿½è¸ªå™¨å•ä¾‹"""
    global _evolution_tracker
    if _evolution_tracker is None:
        _evolution_tracker = RelationshipEvolutionTracker()
    return _evolution_tracker

