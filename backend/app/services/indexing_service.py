"""
ç´¢å¼•æœåŠ¡
æ•´åˆæ–‡ä»¶è§£æã€ç« èŠ‚è¯†åˆ«ã€æ–‡æœ¬åˆ†å—ã€å‘é‡åŒ–ç­‰åŠŸèƒ½
"""

import logging
import asyncio
from typing import Dict, Optional, Callable
from pathlib import Path
from sqlalchemy.orm import Session

from app.services.parser.txt_parser import TXTParser
from app.services.parser.epub_parser import EPUBParser
from app.services.parser.chapter_detector import ChapterDetector
from app.services.text_splitter import get_text_splitter
from app.services.embedding_service import get_embedding_service
from app.services.nlp.entity_extractor import EntityExtractor
from app.services.nlp.entity_merger import EntityMerger
from app.services.entity_service import EntityService
from app.services.graph.graph_builder import GraphBuilder
from app.services.graph.graph_analyzer import GraphAnalyzer
from app.models.database import Novel, Chapter
from app.models.schemas import IndexStatus, FileFormat

logger = logging.getLogger(__name__)


class IndexingService:
    """ç´¢å¼•æœåŠ¡"""
    
    def __init__(self):
        """åˆå§‹åŒ–ç´¢å¼•æœåŠ¡"""
        self.txt_parser = TXTParser()
        self.epub_parser = EPUBParser()
        self.chapter_detector = ChapterDetector()
        self.text_splitter = get_text_splitter()
        self.embedding_service = get_embedding_service()
        
        # Phase 5: çŸ¥è¯†å›¾è°±ç›¸å…³æœåŠ¡
        self.entity_extractor = EntityExtractor()
        self.entity_merger = EntityMerger()
        self.entity_service = EntityService()
        self.graph_builder = GraphBuilder()
        self.graph_analyzer = GraphAnalyzer()
        
        logger.info("âœ… ç´¢å¼•æœåŠ¡åˆå§‹åŒ–å®Œæˆï¼ˆåŒ…å«çŸ¥è¯†å›¾è°±åŠŸèƒ½ï¼‰")
    
    async def index_novel(
        self,
        db: Session,
        novel_id: int,
        file_path: str,
        file_format: FileFormat,
        progress_callback: Optional[Callable] = None
    ) -> bool:
        """
        ç´¢å¼•å°è¯´
        
        Args:
            db: æ•°æ®åº“ä¼šè¯
            novel_id: å°è¯´ID
            file_path: æ–‡ä»¶è·¯å¾„
            file_format: æ–‡ä»¶æ ¼å¼
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•°
        
        Returns:
            bool: æ˜¯å¦æˆåŠŸ
        """
        try:
            # æ›´æ–°çŠ¶æ€ä¸ºå¤„ç†ä¸­
            novel = db.query(Novel).filter(Novel.id == novel_id).first()
            if not novel:
                raise ValueError(f"å°è¯´ ID={novel_id} ä¸å­˜åœ¨")
            
            novel.index_status = IndexStatus.PROCESSING.value
            novel.index_progress = 0.0
            db.commit()
            
            if progress_callback:
                await progress_callback(novel_id, 0.0, "å¼€å§‹è§£ææ–‡ä»¶...")
            
            # 1. è§£ææ–‡ä»¶
            logger.info(f"ğŸ“– å¼€å§‹è§£ææ–‡ä»¶: {file_path}")
            if file_format == FileFormat.TXT:
                content, metadata = self.txt_parser.parse_file(file_path)
                chapters_data = self.chapter_detector.detect(content)
            elif file_format == FileFormat.EPUB:
                content, metadata = self.epub_parser.parse_file(file_path)
                chapters_data = self.epub_parser.detect_chapters(file_path)
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_format}")
            
            novel.total_chapters = len(chapters_data)
            novel.total_chars = metadata.get('total_chars', len(content))
            db.commit()
            
            if progress_callback:
                await progress_callback(novel_id, 0.1, f"æ–‡ä»¶è§£æå®Œæˆï¼Œæ£€æµ‹åˆ°{len(chapters_data)}ç« ")
            
            # 2. åˆ›å»ºChromaDBé›†åˆ
            collection_name = self.embedding_service.create_collection(novel_id)
            
            # 3. å¤„ç†æ¯ä¸ªç« èŠ‚
            total_chapters = len(chapters_data)
            total_chunks = 0
            total_tokens = 0
            
            for i, chapter_data in enumerate(chapters_data):
                chapter_num = chapter_data['chapter_num']
                chapter_title = chapter_data.get('title', f"ç¬¬{chapter_num}ç« ")
                
                logger.info(f"ğŸ“ å¤„ç†ç« èŠ‚ {chapter_num}/{total_chapters}: {chapter_title}")
                
                # æå–ç« èŠ‚å†…å®¹
                chapter_content = self.chapter_detector.extract_chapter_content(
                    content,
                    chapter_data['start_pos'],
                    chapter_data['end_pos'],
                    include_title=True
                )
                
                # ä¿å­˜ç« èŠ‚åˆ°æ•°æ®åº“
                chapter = Chapter(
                    novel_id=novel_id,
                    chapter_num=chapter_num,
                    chapter_title=chapter_title,
                    char_count=len(chapter_content),
                    start_pos=chapter_data['start_pos'],
                    end_pos=chapter_data['end_pos']
                )
                db.add(chapter)
                db.commit()
                
                # æ–‡æœ¬åˆ†å—
                chunks = self.text_splitter.split_chapter(
                    chapter_content,
                    novel_id,
                    chapter_num,
                    chapter_title
                )
                
                chapter.chunk_count = len(chunks)
                total_chunks += len(chunks)
                
                # å‘é‡åŒ–å¹¶å­˜å‚¨
                success = self.embedding_service.process_chapter(
                    novel_id,
                    chapter_num,
                    chapter_title,
                    chunks
                )
                
                if not success:
                    logger.warning(f"âš ï¸ ç« èŠ‚ {chapter_num} å¤„ç†å¤±è´¥")
                
                # æ›´æ–°è¿›åº¦
                progress = 0.1 + 0.9 * (i + 1) / total_chapters
                novel.index_progress = progress
                db.commit()
                
                if progress_callback:
                    await progress_callback(
                        novel_id,
                        progress,
                        f"å·²å®Œæˆ {i+1}/{total_chapters} ç« "
                    )
            
            # 4. Phase 5: æ„å»ºçŸ¥è¯†å›¾è°±
            logger.info(f"ğŸ•¸ï¸ å¼€å§‹æ„å»ºçŸ¥è¯†å›¾è°±...")
            if progress_callback:
                await progress_callback(novel_id, 0.95, "å¼€å§‹æ„å»ºçŸ¥è¯†å›¾è°±...")
            
            try:
                # 4.1 æå–å®ä½“
                logger.info(f"ğŸ“ æå–å®ä½“ä¸­...")
                
                # æ£€æŸ¥ HanLP æ˜¯å¦å¯ç”¨
                if not self.entity_extractor.hanlp_client.is_available():
                    logger.warning(f"âš ï¸ HanLP ä¸å¯ç”¨ï¼Œè·³è¿‡çŸ¥è¯†å›¾è°±æ„å»º")
                    logger.warning(f"   æç¤º: å¦‚éœ€ä½¿ç”¨çŸ¥è¯†å›¾è°±åŠŸèƒ½ï¼Œè¯·å®‰è£… HanLP:")
                    logger.warning(f"   pip install hanlp")
                    raise Exception("HanLP ä¸å¯ç”¨")  # è§¦å‘å¼‚å¸¸å¤„ç†ï¼Œè·³è¿‡çŸ¥è¯†å›¾è°±
                
                chapters_for_extraction = [
                    (ch.chapter_num, self.chapter_detector.extract_chapter_content(
                        content, ch.start_pos, ch.end_pos, include_title=True
                    ))
                    for ch in db.query(Chapter).filter(Chapter.novel_id == novel_id).all()
                ]
                
                # ä¼˜åŒ–ï¼šä¸€æ¬¡éå†åŒæ—¶å®Œæˆå®ä½“æå–ã€é¢‘ç‡ç»Ÿè®¡å’Œç« èŠ‚èŒƒå›´è®¡ç®—ï¼ˆæ€§èƒ½æå‡50%ï¼‰
                logger.info(f"ğŸ“ æå–å®ä½“ä¸­ï¼ˆå…±{len(chapters_for_extraction)}ç« ï¼‰...")
                from collections import Counter
                
                entity_counters = {
                    'characters': Counter(),
                    'locations': Counter(),
                    'organizations': Counter()
                }
                chapter_ranges = {}
                
                for chapter_num, chapter_text in chapters_for_extraction:
                    # åªè°ƒç”¨ä¸€æ¬¡ HanLP
                    chapter_entities = self.entity_extractor.extract_from_chapter(chapter_text, chapter_num)
                    
                    # åŒæ—¶å®Œæˆé¢‘ç‡ç»Ÿè®¡å’Œç« èŠ‚èŒƒå›´è®¡ç®—
                    for entity_type in ['characters', 'locations', 'organizations']:
                        for entity_name in chapter_entities.get(entity_type, []):
                            # ä»»åŠ¡1: ç»Ÿè®¡é¢‘ç‡
                            entity_counters[entity_type][entity_name] += 1
                            
                            # ä»»åŠ¡2: è®°å½•ç« èŠ‚èŒƒå›´
                            if entity_name not in chapter_ranges:
                                chapter_ranges[entity_name] = [chapter_num, chapter_num]
                            else:
                                chapter_ranges[entity_name][1] = chapter_num
                
                # è½¬æ¢ä¸ºå…ƒç»„
                chapter_ranges = {name: tuple(range_list) for name, range_list in chapter_ranges.items()}
                
                # æ£€æŸ¥æ˜¯å¦æå–åˆ°å®ä½“
                total_entities = sum(len(counter) for counter in entity_counters.values())
                if total_entities == 0:
                    logger.warning(f"âš ï¸ æœªæå–åˆ°ä»»ä½•å®ä½“ï¼Œè·³è¿‡çŸ¥è¯†å›¾è°±æ„å»º")
                    logger.warning(f"   å¯èƒ½åŸå› :")
                    logger.warning(f"   1. HanLP æ¨¡å‹æœªæ­£ç¡®åŠ è½½")
                    logger.warning(f"   2. æ–‡æœ¬å†…å®¹ä¸é€‚åˆå®ä½“è¯†åˆ«")
                    logger.warning(f"   3. æ–‡æœ¬æ ¼å¼é—®é¢˜")
                    raise Exception("æœªæå–åˆ°ä»»ä½•å®ä½“")  # è§¦å‘å¼‚å¸¸å¤„ç†ï¼Œè·³è¿‡çŸ¥è¯†å›¾è°±
                
                logger.info(f"âœ… å®ä½“æå–å®Œæˆ: è§’è‰²{len(entity_counters['characters'])} "
                           f"åœ°ç‚¹{len(entity_counters['locations'])} "
                           f"ç»„ç»‡{len(entity_counters['organizations'])}")
                
                # 4.2 å®ä½“å»é‡ä¸åˆå¹¶
                logger.info(f"ğŸ”€ å®ä½“å»é‡ä¸åˆå¹¶ä¸­...")
                merged_entities = {}
                merged_chapter_ranges = {}
                
                for entity_type in ['characters', 'locations', 'organizations']:
                    # è·å–è¯¥ç±»å‹çš„æ‰€æœ‰å®ä½“
                    entity_list = list(entity_counters.get(entity_type, {}).keys())
                    
                    # åˆå¹¶ç›¸ä¼¼å®ä½“
                    merge_mapping = self.entity_merger.merge_entities(entity_list)
                    
                    # æ›´æ–°è®¡æ•°å’Œç« èŠ‚èŒƒå›´
                    from collections import Counter
                    merged_counter = Counter()
                    for main_name, aliases in merge_mapping.items():
                        # åˆå¹¶è®¡æ•°
                        total_count = sum(entity_counters[entity_type].get(alias, 0) for alias in aliases)
                        merged_counter[main_name] = total_count
                        
                        # åˆå¹¶ç« èŠ‚èŒƒå›´ï¼ˆå–æœ€å°å’Œæœ€å¤§ï¼‰
                        min_chapter = min(chapter_ranges.get(alias, (9999, 9999))[0] for alias in aliases)
                        max_chapter = max(chapter_ranges.get(alias, (0, 0))[1] for alias in aliases)
                        merged_chapter_ranges[main_name] = (min_chapter, max_chapter)
                    
                    merged_entities[entity_type] = merged_counter
                
                # 4.3 å­˜å‚¨å®ä½“åˆ°æ•°æ®åº“
                logger.info(f"ğŸ’¾ ä¿å­˜å®ä½“åˆ°æ•°æ®åº“...")
                entity_count = self.entity_service.save_entities(
                    db, novel_id, merged_entities, merged_chapter_ranges
                )
                logger.info(f"âœ… ä¿å­˜äº† {entity_count} ä¸ªå®ä½“")
                
                # 4.4 æ„å»ºçŸ¥è¯†å›¾è°±
                logger.info(f"ğŸ•¸ï¸ æ„å»ºçŸ¥è¯†å›¾è°±...")
                graph = self.graph_builder.create_graph(novel_id)
                
                # æ·»åŠ å®ä½“èŠ‚ç‚¹
                for entity_type in ['characters', 'locations', 'organizations']:
                    for entity_name, count in merged_entities.get(entity_type, {}).items():
                        first_ch, last_ch = chapter_ranges.get(entity_name, (1, total_chapters))
                        self.graph_builder.add_entity(
                            graph,
                            entity_name=entity_name,
                            entity_type=entity_type,
                            first_chapter=first_ch,
                            last_chapter=last_ch,
                            mention_count=count
                        )
                
                # 4.5 è®¡ç®— PageRank é‡è¦æ€§
                logger.info(f"ğŸ“Š è®¡ç®— PageRank é‡è¦æ€§...")
                if graph.number_of_nodes() > 0:
                    self.graph_analyzer.compute_pagerank(graph)
                
                # 4.6 è®¡ç®—ç« èŠ‚é‡è¦æ€§
                logger.info(f"ğŸ“ˆ è®¡ç®—ç« èŠ‚é‡è¦æ€§...")
                if graph.number_of_nodes() > 0:
                    # ä¸ºæ¯ä¸ªç« èŠ‚è®¡ç®—é‡è¦æ€§
                    for chapter in db.query(Chapter).filter(Chapter.novel_id == novel_id).all():
                        importance = self.graph_analyzer.compute_chapter_importance(graph, chapter.chapter_num)
                        chapter.importance_score = importance
                    db.commit()
                    logger.info(f"âœ… ç« èŠ‚é‡è¦æ€§è®¡ç®—å®Œæˆ")
                else:
                    logger.warning(f"âš ï¸ å›¾è°±ä¸ºç©ºï¼Œè·³è¿‡ç« èŠ‚é‡è¦æ€§è®¡ç®—")
                
                # 4.7 ä¿å­˜çŸ¥è¯†å›¾è°±
                logger.info(f"ğŸ’¾ ä¿å­˜çŸ¥è¯†å›¾è°±...")
                self.graph_builder.save_graph(graph, novel_id)
                
                logger.info(f"âœ… çŸ¥è¯†å›¾è°±æ„å»ºå®Œæˆ: {graph.number_of_nodes()}èŠ‚ç‚¹, {graph.number_of_edges()}è¾¹")
                
            except Exception as e:
                logger.error(f"âš ï¸ çŸ¥è¯†å›¾è°±æ„å»ºå¤±è´¥: {e}")
                logger.exception(e)
                # çŸ¥è¯†å›¾è°±æ„å»ºå¤±è´¥ä¸å½±å“æ•´ä½“ç´¢å¼•æµç¨‹
            
            # 5. æ›´æ–°å°è¯´ç»Ÿè®¡ä¿¡æ¯
            novel.total_chunks = total_chunks
            novel.index_status = IndexStatus.COMPLETED.value
            novel.index_progress = 1.0
            novel.indexed_date = novel.updated_at
            db.commit()
            
            if progress_callback:
                await progress_callback(novel_id, 1.0, "ç´¢å¼•å®Œæˆ!")
            
            logger.info(f"âœ… å°è¯´ ID={novel_id} ç´¢å¼•å®Œæˆ: {total_chapters}ç« , {total_chunks}å—")
            return True
            
        except Exception as e:
            logger.error(f"âŒ ç´¢å¼•å¤±è´¥: {e}")
            
            # æ›´æ–°çŠ¶æ€ä¸ºå¤±è´¥
            novel = db.query(Novel).filter(Novel.id == novel_id).first()
            if novel:
                novel.index_status = IndexStatus.FAILED.value
                db.commit()
            
            if progress_callback:
                await progress_callback(novel_id, 0.0, f"ç´¢å¼•å¤±è´¥: {str(e)}")
            
            return False
    
    def get_indexing_progress(
        self,
        db: Session,
        novel_id: int
    ) -> Dict:
        """
        è·å–ç´¢å¼•è¿›åº¦
        
        Args:
            db: æ•°æ®åº“ä¼šè¯
            novel_id: å°è¯´ID
        
        Returns:
            Dict: è¿›åº¦ä¿¡æ¯
        """
        novel = db.query(Novel).filter(Novel.id == novel_id).first()
        if not novel:
            return {
                'found': False,
                'message': f'å°è¯´ ID={novel_id} ä¸å­˜åœ¨'
            }
        
        # ç»Ÿè®¡å·²å®Œæˆçš„ç« èŠ‚
        completed_chapters = db.query(Chapter).filter(
            Chapter.novel_id == novel_id
        ).count()
        
        return {
            'found': True,
            'novel_id': novel_id,
            'status': novel.index_status,
            'progress': novel.index_progress,
            'total_chapters': novel.total_chapters,
            'completed_chapters': completed_chapters,
            'total_chunks': novel.total_chunks,
            'message': self._get_status_message(novel.index_status, novel.index_progress)
        }
    
    @staticmethod
    def _get_status_message(status: str, progress: float) -> str:
        """è·å–çŠ¶æ€æ¶ˆæ¯"""
        if status == IndexStatus.PENDING.value:
            return "ç­‰å¾…ç´¢å¼•"
        elif status == IndexStatus.PROCESSING.value:
            return f"ç´¢å¼•ä¸­ ({progress*100:.1f}%)"
        elif status == IndexStatus.COMPLETED.value:
            return "ç´¢å¼•å®Œæˆ"
        elif status == IndexStatus.FAILED.value:
            return "ç´¢å¼•å¤±è´¥"
        else:
            return "æœªçŸ¥çŠ¶æ€"


# å…¨å±€ç´¢å¼•æœåŠ¡å®ä¾‹
_indexing_service: Optional[IndexingService] = None


def get_indexing_service() -> IndexingService:
    """è·å–å…¨å±€ç´¢å¼•æœåŠ¡å®ä¾‹ï¼ˆå•ä¾‹ï¼‰"""
    global _indexing_service
    if _indexing_service is None:
        _indexing_service = IndexingService()
    return _indexing_service

