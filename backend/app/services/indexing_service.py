"""
ç´¢å¼•æœåŠ¡
æ•´åˆæ–‡ä»¶è§£æã€ç« èŠ‚è¯†åˆ«ã€æ–‡æœ¬åˆ†å—ã€å‘é‡åŒ–ç­‰åŠŸèƒ½
"""

import logging
import asyncio
from typing import Dict, Optional, Callable, List
from pathlib import Path
from sqlalchemy.orm import Session

from app.services.parser.txt_parser import TXTParser
from app.services.parser.epub_parser import EPUBParser
from app.services.parser.chapter_detector import ChapterDetector
from app.services.text_splitter import get_text_splitter
from app.services.embedding_service import get_embedding_service
from app.services.nlp.entity_extractor import EntityExtractor
from app.services.nlp.entity_merger import EntityMerger
from app.services.entity_service import EntityService
from app.services.graph.graph_builder import GraphBuilder
from app.services.graph.graph_analyzer import GraphAnalyzer
from app.services.graph.relation_classifier import RelationshipClassifier
from app.services.graph.evolution_tracker import RelationshipEvolutionTracker
from app.services.graph.attribute_extractor import EntityAttributeExtractor
from app.models.database import Novel, Chapter
from app.models.schemas import IndexStatus, FileFormat
from app.core.config import settings

logger = logging.getLogger(__name__)


def clamp_progress(value: float) -> float:
    """
    å°†è¿›åº¦å€¼é™åˆ¶åœ¨0-1èŒƒå›´å†…ï¼Œå¤„ç†æµ®ç‚¹æ•°ç²¾åº¦é—®é¢˜
    
    Args:
        value: åŸå§‹è¿›åº¦å€¼
    
    Returns:
        é™åˆ¶åçš„è¿›åº¦å€¼ï¼ˆ0.0-1.0ï¼‰
    """
    return max(0.0, min(value, 1.0))


class IndexingService:
    """ç´¢å¼•æœåŠ¡"""
    
    def __init__(self):
        """åˆå§‹åŒ–ç´¢å¼•æœåŠ¡"""
        self.txt_parser = TXTParser()
        self.epub_parser = EPUBParser()
        self.chapter_detector = ChapterDetector()
        self.text_splitter = get_text_splitter()
        self.embedding_service = get_embedding_service()
        
        # Phase 5: çŸ¥è¯†å›¾è°±ç›¸å…³æœåŠ¡
        self.entity_extractor = EntityExtractor()
        self.entity_merger = EntityMerger()
        self.entity_service = EntityService()
        self.graph_builder = GraphBuilder()
        self.graph_analyzer = GraphAnalyzer()
        
        logger.info("âœ… ç´¢å¼•æœåŠ¡åˆå§‹åŒ–å®Œæˆï¼ˆåŒ…å«çŸ¥è¯†å›¾è°±åŠŸèƒ½ï¼‰")
    
    async def index_novel(
        self,
        db: Session,
        novel_id: int,
        file_path: str,
        file_format: FileFormat,
        progress_callback: Optional[Callable] = None
    ) -> bool:
        """
        ç´¢å¼•å°è¯´
        
        Args:
            db: æ•°æ®åº“ä¼šè¯
            novel_id: å°è¯´ID
            file_path: æ–‡ä»¶è·¯å¾„
            file_format: æ–‡ä»¶æ ¼å¼
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•°
        
        Returns:
            bool: æ˜¯å¦æˆåŠŸ
        """
        try:
            # æ›´æ–°çŠ¶æ€ä¸ºå¤„ç†ä¸­
            novel = db.query(Novel).filter(Novel.id == novel_id).first()
            if not novel:
                raise ValueError(f"å°è¯´ ID={novel_id} ä¸å­˜åœ¨")
            
            novel.index_status = IndexStatus.PROCESSING.value
            novel.index_progress = clamp_progress(0.0)
            db.commit()
            
            # ç«‹å³åˆå§‹åŒ–è¿›åº¦è¿½è¸ªï¼ˆä¼°è®¡ç« èŠ‚æ•°ä¸º0ï¼Œåç»­æ›´æ–°ï¼‰
            from app.services.indexing_progress_tracker import get_progress_tracker
            tracker = get_progress_tracker()
            tracker.init_progress(novel_id, 0)
            tracker.update_step(novel_id, 0, 'processing', 0.0, 'å¼€å§‹è§£ææ–‡ä»¶...')
            
            if progress_callback:
                await progress_callback(novel_id, 0.0, "å¼€å§‹è§£ææ–‡ä»¶...")
            
            # 1. è§£ææ–‡ä»¶
            logger.info(f"ğŸ“– å¼€å§‹è§£ææ–‡ä»¶: {file_path}")
            if file_format == FileFormat.TXT:
                content, metadata = self.txt_parser.parse_file(file_path)
                chapters_data = self.chapter_detector.detect(content)
            elif file_format == FileFormat.EPUB:
                content, metadata = self.epub_parser.parse_file(file_path)
                chapters_data = self.epub_parser.detect_chapters(file_path)
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_format}")
            
            novel.total_chapters = len(chapters_data)
            novel.total_chars = metadata.get('total_chars', len(content))
            db.commit()
            
            # æ›´æ–°è¿›åº¦è¿½è¸ªçš„æ€»ç« èŠ‚æ•°
            tracker._details[novel_id]['steps'][3]['message'] = f'å…±{len(chapters_data)}ç« å¾…å¤„ç†'
            tracker.update_step(novel_id, 0, 'completed', 1.0, f'è§£æå®Œæˆï¼Œå…±{len(chapters_data)}ç« ')
            tracker.update_step(novel_id, 1, 'completed', 1.0, f'æ£€æµ‹åˆ°{len(chapters_data)}ä¸ªç« èŠ‚')
            tracker.update_step(novel_id, 2, 'processing', 0.0, 'å‡†å¤‡å¤„ç†ç« èŠ‚...')
            
            # æ›´æ–°æ€»è¿›åº¦ï¼šæ–‡ä»¶è§£æå’Œç« èŠ‚æ£€æµ‹å®Œæˆï¼ˆ0%-5%ï¼‰
            novel.index_progress = clamp_progress(0.05)
            db.commit()
            
            if progress_callback:
                await progress_callback(novel_id, 0.05, f"æ–‡ä»¶è§£æå®Œæˆï¼Œæ£€æµ‹åˆ°{len(chapters_data)}ç« ")
            
            # 2. åˆ›å»ºChromaDBé›†åˆ
            collection_name = self.embedding_service.create_collection(novel_id)
            
            # 3. å¤„ç†æ¯ä¸ªç« èŠ‚ï¼ˆå‘é‡åŒ–å¹¶å­˜å‚¨ï¼‰
            total_chapters = len(chapters_data)
            total_chunks = 0
            total_embedding_tokens = 0  # åˆå§‹åŒ–tokenè®¡æ•°å™¨
            
            # æ›´æ–°æ­¥éª¤2ä¸ºcompletedï¼Œæ­¥éª¤3ä¸ºprocessing
            from app.services.indexing_progress_tracker import get_progress_tracker
            tracker = get_progress_tracker()
            tracker.update_step(novel_id, 2, 'completed', 1.0, f'å‡†å¤‡å¤„ç†{total_chapters}ä¸ªç« èŠ‚')
            tracker.update_step(novel_id, 3, 'processing', 0.0, 'å¼€å§‹å¤„ç†ç« èŠ‚...')
            
            # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨ Batch API è¿›è¡Œå‘é‡åŒ–
            use_batch_api_for_embedding = settings.use_batch_api_for_embedding
            
            if use_batch_api_for_embedding:
                logger.info(f"ğŸš€ å¯ç”¨å‘é‡åŒ– Batch API æ¨¡å¼ï¼ˆå®éªŒæ€§ï¼‰")
                
                # æ”¶é›†æ‰€æœ‰ç« èŠ‚çš„åˆ†å—æ•°æ®
                all_chapters_chunks = []
                
                for i, chapter_data in enumerate(chapters_data):
                    chapter_num = chapter_data['chapter_num']
                    chapter_title = chapter_data.get('title', f"ç¬¬{chapter_num}ç« ")
                    
                    # æå–ç« èŠ‚å†…å®¹
                    chapter_content = self.chapter_detector.extract_chapter_content(
                        content,
                        chapter_data['start_pos'],
                        chapter_data['end_pos'],
                        include_title=True
                    )
                    
                    # ä¿å­˜ç« èŠ‚åˆ°æ•°æ®åº“
                    chapter = Chapter(
                        novel_id=novel_id,
                        chapter_num=chapter_num,
                        chapter_title=chapter_title,
                        char_count=len(chapter_content),
                        start_pos=chapter_data['start_pos'],
                        end_pos=chapter_data['end_pos']
                    )
                    db.add(chapter)
                    db.commit()
                    
                    # æ–‡æœ¬åˆ†å—
                    chunks = self.text_splitter.split_chapter(
                        chapter_content,
                        novel_id,
                        chapter_num,
                        chapter_title
                    )
                    
                    chapter.chunk_count = len(chunks)
                    total_chunks += len(chunks)
                    
                    all_chapters_chunks.append({
                        'chapter_num': chapter_num,
                        'chapter_title': chapter_title,
                        'chunks': chunks
                    })
                
                db.commit()
                
                # ä½¿ç”¨ Batch API æ‰¹é‡å¤„ç†å‘é‡åŒ–
                success, total_embedding_tokens, failed_chapters = await self.embedding_service.process_novel_with_batch_api(
                    novel_id, all_chapters_chunks
                )
                
                if failed_chapters:
                    logger.warning(f"âš ï¸ {len(failed_chapters)} ä¸ªç« èŠ‚å‘é‡åŒ–å¤±è´¥: {failed_chapters}")
                    for failed_ch in failed_chapters:
                        tracker.add_failed_chapter(novel_id, failed_ch, f"ç¬¬{failed_ch}ç« ", "å‘é‡åŒ–å¤„ç†å¤±è´¥")
                
                # æ›´æ–°è¿›åº¦åˆ°80%
                novel.index_progress = clamp_progress(0.80)
                db.commit()
                
                if progress_callback:
                    token_stats = {
                        "embeddingTokens": total_embedding_tokens,
                        "totalTokens": total_embedding_tokens
                    }
                    await progress_callback(novel_id, 0.80, f"æ‰€æœ‰ç« èŠ‚å‘é‡åŒ–å®Œæˆ", token_stats)
            else:
                # åŸæœ‰çš„é€ç« èŠ‚å¤„ç†é€»è¾‘
                logger.info(f"âš¡ ä½¿ç”¨å®æ—¶ API æ¨¡å¼å¤„ç†å‘é‡åŒ–")
                
                for i, chapter_data in enumerate(chapters_data):
                    chapter_num = chapter_data['chapter_num']
                    chapter_title = chapter_data.get('title', f"ç¬¬{chapter_num}ç« ")
                    
                    logger.info(f"ğŸ“ å¤„ç†ç« èŠ‚ {chapter_num}/{total_chapters}: {chapter_title}")
                    
                    # æå–ç« èŠ‚å†…å®¹
                    chapter_content = self.chapter_detector.extract_chapter_content(
                        content,
                        chapter_data['start_pos'],
                        chapter_data['end_pos'],
                        include_title=True
                    )
                    
                    # ä¿å­˜ç« èŠ‚åˆ°æ•°æ®åº“
                    chapter = Chapter(
                        novel_id=novel_id,
                        chapter_num=chapter_num,
                        chapter_title=chapter_title,
                        char_count=len(chapter_content),
                        start_pos=chapter_data['start_pos'],
                        end_pos=chapter_data['end_pos']
                    )
                    db.add(chapter)
                    db.commit()
                    
                    # æ–‡æœ¬åˆ†å—
                    chunks = self.text_splitter.split_chapter(
                        chapter_content,
                        novel_id,
                        chapter_num,
                        chapter_title
                    )
                    
                    chapter.chunk_count = len(chunks)
                    total_chunks += len(chunks)
                    
                    # å‘é‡åŒ–å¹¶å­˜å‚¨ï¼ˆè·å–tokenæ¶ˆè€—ï¼‰
                    success, chapter_tokens = self.embedding_service.process_chapter(
                        novel_id,
                        chapter_num,
                        chapter_title,
                        chunks
                    )
                    
                    # ç´¯åŠ tokenæ¶ˆè€—
                    total_embedding_tokens += chapter_tokens
                    
                    if not success:
                        logger.warning(f"âš ï¸ ç« èŠ‚ {chapter_num} å¤„ç†å¤±è´¥")
                        # è®°å½•å¤±è´¥ç« èŠ‚
                        from app.services.indexing_progress_tracker import get_progress_tracker
                        tracker = get_progress_tracker()
                        tracker.add_failed_chapter(novel_id, chapter_num, chapter_title, "å‘é‡åŒ–å¤„ç†å¤±è´¥")
                    
                    # æ›´æ–°è¿›åº¦ï¼ˆç« èŠ‚å¤„ç†å 5%-80%ï¼Œå…±75%ï¼‰
                    progress = clamp_progress(0.05 + 0.75 * (i + 1) / total_chapters)
                    novel.index_progress = progress
                    db.commit()
                    
                    # æ›´æ–°æ­¥éª¤3çš„è¿›åº¦
                    from app.services.indexing_progress_tracker import get_progress_tracker
                    tracker = get_progress_tracker()
                    step_progress = clamp_progress((i + 1) / total_chapters)
                    tracker.update_step(novel_id, 3, 'processing', step_progress, f'å·²å®Œæˆ {i+1}/{total_chapters} ç« ')
                    
                    # æ„å»ºtokenç»Ÿè®¡ä¿¡æ¯
                    token_stats = {
                        "embeddingTokens": total_embedding_tokens,
                        "totalTokens": total_embedding_tokens
                    }
                    
                    if progress_callback:
                        await progress_callback(
                            novel_id,
                            progress,
                            f"å·²å®Œæˆ {i+1}/{total_chapters} ç« ",
                            token_stats
                        )
            
            # æ ‡è®°æ­¥éª¤3ä¸ºå®Œæˆï¼Œå¹¶è®°å½•æ€»Tokenæ¶ˆè€—
            from app.services.indexing_progress_tracker import get_progress_tracker
            from app.utils.token_counter import get_token_counter
            tracker = get_progress_tracker()
            tracker.update_step(novel_id, 3, 'completed', 1.0, f'æ‰€æœ‰ç« èŠ‚å¤„ç†å®Œæˆï¼ˆ{total_chapters}ç« ï¼‰')
            
            # è®°å½•æ€»çš„å‘é‡åŒ–Tokenæ¶ˆè€—
            if total_embedding_tokens > 0:
                token_counter = get_token_counter()
                cost = token_counter.calculate_cost(total_embedding_tokens, 0, 'embedding-3')
                tracker.add_token_usage(
                    novel_id=novel_id,
                    step_name='ç”ŸæˆåµŒå…¥å‘é‡',
                    model_name='embedding-3',
                    input_tokens=total_embedding_tokens,
                    output_tokens=0,
                    cost=cost
                )
            
            # 4. Phase 5: æ„å»ºçŸ¥è¯†å›¾è°±ï¼ˆå 80%-100%ï¼Œå…±20%ï¼‰
            logger.info(f"ğŸ•¸ï¸ å¼€å§‹æ„å»ºçŸ¥è¯†å›¾è°±...")
            
            # åˆå§‹åŒ–å›¾è°±tokenç»Ÿè®¡å˜é‡
            graph_attribute_tokens = 0
            graph_relation_tokens = 0
            graph_evolution_tokens = 0
            
            # æ›´æ–°æ­¥éª¤4ä¸ºprocessingï¼Œå¹¶æ›´æ–°æ€»è¿›åº¦åˆ°80%
            tracker.update_step(novel_id, 4, 'processing', 0.0, 'å¼€å§‹æ„å»ºçŸ¥è¯†å›¾è°±...')
            novel.index_progress = clamp_progress(0.80)
            db.commit()
            
            if progress_callback:
                await progress_callback(novel_id, 0.80, "å¼€å§‹æ„å»ºçŸ¥è¯†å›¾è°±...")
            
            try:
                # 4.1 æå–å®ä½“
                logger.info(f"ğŸ“ æå–å®ä½“ä¸­...")
                
                # æ£€æŸ¥ HanLP æ˜¯å¦å¯ç”¨
                if not self.entity_extractor.hanlp_client.is_available():
                    logger.warning(f"âš ï¸ HanLP ä¸å¯ç”¨ï¼Œè·³è¿‡çŸ¥è¯†å›¾è°±æ„å»º")
                    logger.warning(f"   æç¤º: å¦‚éœ€ä½¿ç”¨çŸ¥è¯†å›¾è°±åŠŸèƒ½ï¼Œè¯·å®‰è£… HanLP:")
                    logger.warning(f"   pip install hanlp")
                    raise Exception("HanLP ä¸å¯ç”¨")  # è§¦å‘å¼‚å¸¸å¤„ç†ï¼Œè·³è¿‡çŸ¥è¯†å›¾è°±
                
                chapters_for_extraction = [
                    (ch.chapter_num, self.chapter_detector.extract_chapter_content(
                        content, ch.start_pos, ch.end_pos, include_title=True
                    ))
                    for ch in db.query(Chapter).filter(Chapter.novel_id == novel_id).all()
                ]
                
                # ä¼˜åŒ–ï¼šä¸€æ¬¡éå†åŒæ—¶å®Œæˆå®ä½“æå–ã€é¢‘ç‡ç»Ÿè®¡å’Œç« èŠ‚èŒƒå›´è®¡ç®—ï¼ˆæ€§èƒ½æå‡50%ï¼‰
                logger.info(f"ğŸ“ æå–å®ä½“ä¸­ï¼ˆå…±{len(chapters_for_extraction)}ç« ï¼‰...")
                from collections import Counter
                
                entity_counters = {
                    'characters': Counter(),
                    'locations': Counter(),
                    'organizations': Counter()
                }
                chapter_ranges = {}
                chapter_entity_map = {}  # è®°å½•æ¯ç« çš„å®ä½“åˆ—è¡¨ï¼ˆç”¨äºæ„å»ºå…±ç°å…³ç³»ï¼‰
                
                for chapter_num, chapter_text in chapters_for_extraction:
                    # åªè°ƒç”¨ä¸€æ¬¡ HanLP
                    chapter_entities = self.entity_extractor.extract_from_chapter(chapter_text, chapter_num)
                    
                    # è®°å½•æœ¬ç« å‡ºç°çš„æ‰€æœ‰è§’è‰²å®ä½“ï¼ˆä»…è§’è‰²å‚ä¸å…³ç³»å›¾ï¼‰
                    chapter_entity_map[chapter_num] = set(chapter_entities.get('characters', []))
                    
                    # åŒæ—¶å®Œæˆé¢‘ç‡ç»Ÿè®¡å’Œç« èŠ‚èŒƒå›´è®¡ç®—
                    for entity_type in ['characters', 'locations', 'organizations']:
                        for entity_name in chapter_entities.get(entity_type, []):
                            # ä»»åŠ¡1: ç»Ÿè®¡é¢‘ç‡
                            entity_counters[entity_type][entity_name] += 1
                            
                            # ä»»åŠ¡2: è®°å½•ç« èŠ‚èŒƒå›´
                            if entity_name not in chapter_ranges:
                                chapter_ranges[entity_name] = [chapter_num, chapter_num]
                            else:
                                chapter_ranges[entity_name][1] = chapter_num
                
                # è½¬æ¢ä¸ºå…ƒç»„
                chapter_ranges = {name: tuple(range_list) for name, range_list in chapter_ranges.items()}
                
                # æ£€æŸ¥æ˜¯å¦æå–åˆ°å®ä½“
                total_entities = sum(len(counter) for counter in entity_counters.values())
                if total_entities == 0:
                    logger.warning(f"âš ï¸ æœªæå–åˆ°ä»»ä½•å®ä½“ï¼Œè·³è¿‡çŸ¥è¯†å›¾è°±æ„å»º")
                    logger.warning(f"   å¯èƒ½åŸå› :")
                    logger.warning(f"   1. HanLP æ¨¡å‹æœªæ­£ç¡®åŠ è½½")
                    logger.warning(f"   2. æ–‡æœ¬å†…å®¹ä¸é€‚åˆå®ä½“è¯†åˆ«")
                    logger.warning(f"   3. æ–‡æœ¬æ ¼å¼é—®é¢˜")
                    raise Exception("æœªæå–åˆ°ä»»ä½•å®ä½“")  # è§¦å‘å¼‚å¸¸å¤„ç†ï¼Œè·³è¿‡çŸ¥è¯†å›¾è°±
                
                logger.info(f"âœ… å®ä½“æå–å®Œæˆ: è§’è‰²{len(entity_counters['characters'])} "
                           f"åœ°ç‚¹{len(entity_counters['locations'])} "
                           f"ç»„ç»‡{len(entity_counters['organizations'])}")
                
                # æ›´æ–°è¿›åº¦ï¼šå®ä½“æå–å®Œæˆï¼ˆ80%-85%ï¼‰
                novel.index_progress = clamp_progress(0.85)
                db.commit()
                tracker.update_step(novel_id, 4, 'processing', 0.25, f'å®ä½“æå–å®Œæˆ')
                if progress_callback:
                    await progress_callback(novel_id, 0.85, "å®ä½“æå–å®Œæˆ")
                
                # 4.2 å®ä½“å»é‡ä¸åˆå¹¶
                logger.info(f"ğŸ”€ å®ä½“å»é‡ä¸åˆå¹¶ä¸­...")
                merged_entities = {}
                merged_chapter_ranges = {}
                alias_mapping = {}  # âœ… æ–°å¢ï¼šåŒæ—¶ä¿å­˜åˆ«åæ˜ å°„
                
                for entity_type in ['characters', 'locations', 'organizations']:
                    # è·å–è¯¥ç±»å‹çš„æ‰€æœ‰å®ä½“
                    entity_list = list(entity_counters.get(entity_type, {}).keys())
                    
                    # âœ… åªè°ƒç”¨ä¸€æ¬¡ merge_entities
                    merge_mapping = self.entity_merger.merge_entities(entity_list)
                    
                    # âœ… ç«‹å³ä¿å­˜åˆ«åæ˜ å°„ï¼ˆä¾›åç»­ä½¿ç”¨ï¼‰
                    alias_mapping[entity_type] = merge_mapping
                    
                    # æ›´æ–°è®¡æ•°å’Œç« èŠ‚èŒƒå›´
                    from collections import Counter
                    merged_counter = Counter()
                    for main_name, aliases in merge_mapping.items():
                        # åˆå¹¶è®¡æ•°
                        total_count = sum(entity_counters[entity_type].get(alias, 0) for alias in aliases)
                        merged_counter[main_name] = total_count
                        
                        # åˆå¹¶ç« èŠ‚èŒƒå›´ï¼ˆå–æœ€å°å’Œæœ€å¤§ï¼‰
                        min_chapter = min(chapter_ranges.get(alias, (9999, 9999))[0] for alias in aliases)
                        max_chapter = max(chapter_ranges.get(alias, (0, 0))[1] for alias in aliases)
                        merged_chapter_ranges[main_name] = (min_chapter, max_chapter)
                    
                    merged_entities[entity_type] = merged_counter
                
                # 4.3 å­˜å‚¨å®ä½“åˆ°æ•°æ®åº“
                logger.info(f"ğŸ’¾ ä¿å­˜å®ä½“åˆ°æ•°æ®åº“...")
                entity_count = self.entity_service.save_entities(
                    db, novel_id, merged_entities, merged_chapter_ranges
                )
                logger.info(f"âœ… ä¿å­˜äº† {entity_count} ä¸ªå®ä½“")
                
                # æ›´æ–°è¿›åº¦ï¼šå®ä½“ä¿å­˜å®Œæˆï¼ˆ85%-87%ï¼‰
                novel.index_progress = clamp_progress(0.87)
                db.commit()
                tracker.update_step(novel_id, 4, 'processing', 0.35, f'ä¿å­˜äº†{entity_count}ä¸ªå®ä½“')
                if progress_callback:
                    await progress_callback(novel_id, 0.87, f"ä¿å­˜äº†{entity_count}ä¸ªå®ä½“")
                
                # 4.3.5 å­˜å‚¨å®ä½“åˆ«åæ˜ å°„ - âœ… ä½¿ç”¨å·²æœ‰çš„ alias_mapping
                logger.info(f"ğŸ”— ä¿å­˜å®ä½“åˆ«åæ˜ å°„...")
                alias_count = self.entity_service.save_entity_aliases(
                    db, novel_id, alias_mapping  # ç›´æ¥ä½¿ç”¨ç¼“å­˜çš„ç»“æœ
                )
                logger.info(f"âœ… ä¿å­˜äº† {alias_count} ä¸ªå®ä½“åˆ«å")
                
                # 4.4 æ„å»ºçŸ¥è¯†å›¾è°±
                logger.info(f"ğŸ•¸ï¸ æ„å»ºçŸ¥è¯†å›¾è°±...")
                
                # æ›´æ–°è¿›åº¦ï¼šå¼€å§‹æ„å»ºå›¾è°±ï¼ˆ87%-89%ï¼‰
                novel.index_progress = clamp_progress(0.89)
                db.commit()
                tracker.update_step(novel_id, 4, 'processing', 0.45, 'æ„å»ºå›¾è°±ç»“æ„ä¸­...')
                if progress_callback:
                    await progress_callback(novel_id, 0.89, "æ„å»ºå›¾è°±ç»“æ„ä¸­...")
                
                graph = self.graph_builder.create_graph(novel_id)
                
                # å‡†å¤‡å±æ€§æå–ä»»åŠ¡ï¼ˆä»…å¯¹ä¸»è¦è§’è‰²ï¼Œå‡ºç°â‰¥10æ¬¡ï¼‰
                attribute_extractor = EntityAttributeExtractor()
                attribute_tasks = []
                entity_list = []  # è®°å½•å®ä½“ä¿¡æ¯ï¼Œç”¨äºåç»­æ·»åŠ 
                
                # æ ¹æ®ç« èŠ‚æ•°åŠ¨æ€è°ƒæ•´å±æ€§æå–é˜ˆå€¼
                # çŸ­ç¯‡ï¼ˆ<20ç« ï¼‰ï¼šå‡ºç°3æ¬¡ä»¥ä¸Š
                # ä¸­ç¯‡ï¼ˆ20-50ç« ï¼‰ï¼šå‡ºç°5æ¬¡ä»¥ä¸Š
                # é•¿ç¯‡ï¼ˆ>50ç« ï¼‰ï¼šå‡ºç°10æ¬¡ä»¥ä¸Š
                if total_chapters < 20:
                    attribute_threshold = 3
                elif total_chapters < 50:
                    attribute_threshold = 5
                else:
                    attribute_threshold = 10
                
                logger.info(f"ğŸ“Š å±æ€§æå–é˜ˆå€¼: {attribute_threshold}æ¬¡ï¼ˆåŸºäº{total_chapters}ç« ï¼‰")
                
                for entity_type in ['characters', 'locations', 'organizations']:
                    for entity_name, count in merged_entities.get(entity_type, {}).items():
                        first_ch, last_ch = merged_chapter_ranges.get(entity_name, (1, total_chapters))
                        entity_list.append((entity_name, entity_type, first_ch, last_ch, count))
                        
                        # ä¸»è¦è§’è‰²éœ€è¦æå–å±æ€§ï¼ˆä½¿ç”¨åŠ¨æ€é˜ˆå€¼ï¼‰
                        if entity_type == 'characters' and count >= attribute_threshold:
                            attribute_tasks.append((entity_name, entity_type))
                
                # æ‰¹é‡æå–å±æ€§
                logger.info(f"ğŸ“Š æå– {len(attribute_tasks)} ä¸ªä¸»è¦è§’è‰²çš„å±æ€§...")
                attributes_map = {}
                tasks_with_contexts = []  # åˆå§‹åŒ–ï¼Œé¿å…åç»­è®¿é—®æ—¶å˜é‡æœªå®šä¹‰
                
                if attribute_tasks:
                    # ä¸ºæ¯ä¸ªå®ä½“æå–ä¸Šä¸‹æ–‡
                    # æ³¨æ„ï¼šè¿™é‡Œä½¿ç”¨å·²ç»è¯»å–çš„contentå˜é‡ï¼ˆæ¥è‡ªå‘é‡åŒ–é˜¶æ®µï¼‰
                    for entity_name, entity_type in attribute_tasks:
                        # è·å–å®ä½“å‡ºç°çš„å‰3ä¸ªç« èŠ‚çš„ä¸Šä¸‹æ–‡
                        entity_chapters = []
                        for chapter_num, entities in chapter_entity_map.items():
                            if entity_name in entities:
                                entity_chapters.append(chapter_num)
                        
                        if entity_chapters:
                            # å–å‰3ä¸ªç« èŠ‚
                            sampled_chapters = sorted(entity_chapters)[:3]
                            contexts = []
                            
                            for ch_num in sampled_chapters:
                                try:
                                    chapter = db.query(Chapter).filter(
                                        Chapter.novel_id == novel.id,
                                        Chapter.chapter_num == ch_num
                                    ).first()
                                    
                                    if chapter:
                                        # ä½¿ç”¨å­—ç¬¦ä½ç½®åˆ‡ç‰‡ï¼ˆä¸å‘é‡åŒ–é˜¶æ®µä¸€è‡´ï¼Œé¿å…ç¼–ç é—®é¢˜ï¼‰
                                        # contentå˜é‡åœ¨index_novelå¼€å¤´å·²è¯»å–
                                        chapter_content = content[chapter.start_pos:chapter.end_pos]
                                        
                                        if not chapter_content:
                                            continue
                                        
                                        # åªå–å‰1000å­—ç¬¦ï¼ˆé¿å…è¿‡é•¿ï¼‰
                                        chapter_content = chapter_content[:1000]
                                        
                                        # æŸ¥æ‰¾åŒ…å«å®ä½“çš„æ®µè½
                                        if entity_name in chapter_content:
                                            idx = chapter_content.find(entity_name)
                                            start = max(0, idx - 100)
                                            end = min(len(chapter_content), idx + 200)
                                            context_snippet = chapter_content[start:end]
                                            contexts.append(f"[ç¬¬{ch_num}ç« ] {context_snippet}")
                                except Exception as e:
                                    logger.warning(f"æå–{entity_name}ä¸Šä¸‹æ–‡å¤±è´¥: {e}")
                            
                            if contexts:
                                tasks_with_contexts.append((entity_name, entity_type, contexts))
                    
                # æ‰¹é‡æå–å±æ€§ï¼ˆæ ¹æ®é…ç½®é€‰æ‹©Batch APIæˆ–å®æ—¶APIï¼‰
                graph_attribute_tokens = 0
                if tasks_with_contexts:
                    use_batch = settings.use_batch_api_for_graph
                    if use_batch:
                        logger.info(f"ğŸš€ å¯ç”¨Batch APIæ¨¡å¼ï¼šæ— å¹¶å‘é™åˆ¶ï¼Œå®Œå…¨å…è´¹ï¼Œéœ€ç­‰å¾…å¤„ç†å®Œæˆ")
                    else:
                        logger.info(f"âš¡ ä½¿ç”¨å®æ—¶APIæ¨¡å¼ï¼šå¹¶å‘é™åˆ¶3ï¼Œç«‹å³è¿”å›")
                    
                    attributes_list, attr_token_stats = await attribute_extractor.extract_batch(
                        tasks_with_contexts,
                        use_batch_api=use_batch
                    )
                    
                    # è®°å½•å±æ€§æå–çš„tokenæ¶ˆè€—
                    graph_attribute_tokens = attr_token_stats.get('total_tokens', 0)
                    
                    # æ„å»ºå±æ€§æ˜ å°„
                    for i, (entity_name, _, _) in enumerate(tasks_with_contexts):
                        if attributes_list[i]:
                            attributes_map[entity_name] = attributes_list[i]
                
                # æ·»åŠ å®ä½“èŠ‚ç‚¹ï¼ˆå¸¦å±æ€§ï¼‰
                logger.info(f"ğŸ“ æ·»åŠ  {len(entity_list)} ä¸ªå®ä½“èŠ‚ç‚¹...")
                for entity_name, entity_type, first_ch, last_ch, count in entity_list:
                    attributes = attributes_map.get(entity_name, {})
                    
                    self.graph_builder.add_entity(
                        graph,
                        entity_name=entity_name,
                        entity_type=entity_type,
                        first_chapter=first_ch,
                        last_chapter=last_ch,
                        mention_count=count,
                        attributes=attributes  # æ·»åŠ å±æ€§
                    )
                
                # æ·»åŠ è§’è‰²é—´çš„å…±ç°å…³ç³»è¾¹
                logger.info(f"ğŸ”— æ„å»ºè§’è‰²å…³ç³»...")
                cooccurrence_count = {}  # (entity1, entity2) -> count
                cooccurrence_chapters = {}  # (entity1, entity2) -> [chapter_nums]
                
                for chapter_num, entities in chapter_entity_map.items():
                    entity_list = list(entities)
                    # å¯¹è¯¥ç« èŠ‚çš„ä»»æ„ä¸¤ä¸ªè§’è‰²å»ºç«‹å…±ç°å…³ç³»
                    for i in range(len(entity_list)):
                        for j in range(i + 1, len(entity_list)):
                            entity1, entity2 = sorted([entity_list[i], entity_list[j]])
                            pair = (entity1, entity2)
                            
                            if pair not in cooccurrence_count:
                                cooccurrence_count[pair] = 0
                                cooccurrence_chapters[pair] = []
                            
                            cooccurrence_count[pair] += 1
                            cooccurrence_chapters[pair].append(chapter_num)
                
                # æ ¹æ®ç« èŠ‚æ•°åŠ¨æ€è°ƒæ•´å…³ç³»åˆ†ç±»é˜ˆå€¼
                # çŸ­ç¯‡ï¼ˆ<20ç« ï¼‰ï¼šå…±ç°2æ¬¡å³åˆ†ç±»ï¼Œ1æ¬¡ä¸ºå¼±å…³ç³»
                # ä¸­ç¯‡ï¼ˆ20-50ç« ï¼‰ï¼šå…±ç°3æ¬¡å³åˆ†ç±»ï¼Œ2æ¬¡ä¸ºå¼±å…³ç³»
                # é•¿ç¯‡ï¼ˆ>50ç« ï¼‰ï¼šå…±ç°5æ¬¡å³åˆ†ç±»ï¼Œ3æ¬¡ä¸ºå¼±å…³ç³»
                if total_chapters < 20:
                    min_cooccurrence_for_classification = 2
                    min_cooccurrence_for_weak = 1
                elif total_chapters < 50:
                    min_cooccurrence_for_classification = 3
                    min_cooccurrence_for_weak = 2
                else:
                    min_cooccurrence_for_classification = 5
                    min_cooccurrence_for_weak = 3
                
                logger.info(f"ğŸ“Š å…³ç³»åˆ†ç±»é˜ˆå€¼: {min_cooccurrence_for_classification}æ¬¡ï¼ˆåŸºäº{total_chapters}ç« ï¼‰")
                
                classification_tasks = []
                weak_relations = []  # ä½é¢‘å…³ç³»ï¼Œä¸åˆ†ç±»ç›´æ¥æ ‡è®°ä¸º"å…±ç°"
                
                for (entity1, entity2), count in cooccurrence_count.items():
                    chapters = cooccurrence_chapters[(entity1, entity2)]
                    
                    if count >= min_cooccurrence_for_classification:
                        # é«˜é¢‘å…³ç³»ï¼Œéœ€è¦LLMåˆ†ç±»
                        classification_tasks.append((entity1, entity2, chapters, count))
                    elif count >= min_cooccurrence_for_weak:
                        # ä½é¢‘å…³ç³»ï¼Œç›´æ¥æ ‡è®°ä¸º"å…±ç°"
                        weak_relations.append((entity1, entity2, chapters, count))
                
                logger.info(f"ğŸ“Š å‘ç° {len(classification_tasks)} å¯¹é«˜é¢‘å…³ç³»éœ€è¦åˆ†ç±»ï¼Œ{len(weak_relations)} å¯¹ä½é¢‘å…³ç³»")
                
                # å¹¶å‘åˆ†ç±»é«˜é¢‘å…³ç³»
                relation_classifier = RelationshipClassifier()
                classifications = []
                tasks_with_contexts = []  # åˆå§‹åŒ–ï¼Œé¿å…åç»­è®¿é—®æ—¶å˜é‡æœªå®šä¹‰
                
                if classification_tasks:
                    # æå–ä¸Šä¸‹æ–‡å¹¶å‡†å¤‡åˆ†ç±»ä»»åŠ¡
                    logger.info(f"ğŸ” æå–ä¸Šä¸‹æ–‡ç‰‡æ®µ...")
                    
                    for entity1, entity2, chapters, count in classification_tasks:
                        # æ™ºèƒ½é‡‡æ ·ç« èŠ‚ï¼ˆæ—©æœŸ+ä¸­æœŸ+åæœŸ+å‡åŒ€åˆ†å¸ƒï¼‰
                        sampled_chapters = relation_classifier._smart_chapter_sampling(chapters, max_samples=5)
                        
                        # æå–ä¸Šä¸‹æ–‡
                        contexts = await self._extract_cooccurrence_contexts(
                            entity1, entity2, sampled_chapters, novel, db
                        )
                        
                        if contexts:
                            tasks_with_contexts.append((entity1, entity2, contexts, count, chapters))
                        else:
                            # å¦‚æœæ— æ³•æå–ä¸Šä¸‹æ–‡ï¼Œé™çº§ä¸º"å…±ç°"
                            weak_relations.append((entity1, entity2, chapters, count))
                    
                    logger.info(f"âœ… æˆåŠŸæå– {len(tasks_with_contexts)} å¯¹å…³ç³»çš„ä¸Šä¸‹æ–‡")
                    
                # æ‰¹é‡åˆ†ç±»ï¼ˆæ ¹æ®é…ç½®é€‰æ‹©Batch APIæˆ–å®æ—¶APIï¼‰
                graph_relation_tokens = 0
                if tasks_with_contexts:
                    use_batch = settings.use_batch_api_for_graph
                    if use_batch:
                        logger.info(f"ğŸš€ å¯ç”¨Batch APIæ¨¡å¼ï¼šæ— å¹¶å‘é™åˆ¶ï¼Œå®Œå…¨å…è´¹ï¼Œéœ€ç­‰å¾…å¤„ç†å®Œæˆ")
                    else:
                        logger.info(f"âš¡ ä½¿ç”¨å®æ—¶APIæ¨¡å¼ï¼šå¹¶å‘é™åˆ¶5ï¼Œç«‹å³è¿”å›")
                    
                    classifications, rel_token_stats = await relation_classifier.classify_batch(
                        tasks_with_contexts,
                        use_batch_api=use_batch
                    )
                    
                    # è®°å½•å…³ç³»åˆ†ç±»çš„tokenæ¶ˆè€—
                    graph_relation_tokens = rel_token_stats.get('total_tokens', 0)
                
                # æ ¹æ®ç« èŠ‚æ•°åŠ¨æ€è°ƒæ•´æ¼”å˜è¿½è¸ªé˜ˆå€¼
                # çŸ­ç¯‡ï¼ˆ<20ç« ï¼‰ï¼šå…±ç°4æ¬¡ä»¥ä¸Š
                # ä¸­ç¯‡ï¼ˆ20-50ç« ï¼‰ï¼šå…±ç°6æ¬¡ä»¥ä¸Š
                # é•¿ç¯‡ï¼ˆ>50ç« ï¼‰ï¼šå…±ç°10æ¬¡ä»¥ä¸Š
                if total_chapters < 20:
                    evolution_threshold = 4
                elif total_chapters < 50:
                    evolution_threshold = 6
                else:
                    evolution_threshold = 10
                
                evolution_tracker = RelationshipEvolutionTracker()
                evolution_tasks = []
                
                for i, (entity1, entity2, contexts, count, chapters) in enumerate(tasks_with_contexts):
                    if count >= evolution_threshold:  # é«˜é¢‘å…³ç³»æ‰è¿½è¸ªæ¼”å˜
                        evolution_tasks.append((entity1, entity2, chapters))
                
                logger.info(f"ğŸ”„ è¿½è¸ª {len(evolution_tasks)} å¯¹é«˜é¢‘å…³ç³»çš„æ¼”å˜ï¼ˆé˜ˆå€¼: {evolution_threshold}æ¬¡ï¼‰...")
                evolutions = {}
                graph_evolution_tokens = 0
                
                if evolution_tasks:
                    evolutions, evo_token_stats = await evolution_tracker.track_batch(
                        evolution_tasks, novel, db
                    )
                    # æ¼”å˜è¿½è¸ªçš„tokenå·²åœ¨å…³ç³»åˆ†ç±»ä¸­ç»Ÿè®¡ï¼Œè¿™é‡Œä¸é‡å¤è®¡æ•°
                    graph_evolution_tokens = evo_token_stats.get('total_tokens', 0)
                
                # æ·»åŠ åˆ†ç±»åçš„å…³ç³»è¾¹
                relation_count = 0
                
                for i, (entity1, entity2, contexts, count, chapters) in enumerate(tasks_with_contexts):
                    classification = classifications[i]
                    start_chapter = min(chapters)
                    end_chapter = max(chapters)
                    strength = min(count / 20.0, 1.0)
                    
                    # è·å–æ¼”å˜è½¨è¿¹ï¼ˆå¦‚æœæœ‰ï¼‰
                    evolution = evolutions.get((entity1, entity2), [])
                    
                    # å¦‚æœæœ‰æ¼”å˜ï¼Œä½¿ç”¨æœ€åä¸€ä¸ªæ—¶æœŸçš„å…³ç³»ç±»å‹
                    final_relation_type = evolution[-1]['type'] if evolution else classification['relation_type']
                    
                    # æ·»åŠ åŒå‘è¾¹
                    self.graph_builder.add_relation(
                        graph,
                        source=entity1,
                        target=entity2,
                        relation_type=final_relation_type,
                        start_chapter=start_chapter,
                        end_chapter=end_chapter,
                        strength=strength,
                        confidence=classification['confidence'],
                        cooccurrence_count=count,
                        evolution=evolution  # æ·»åŠ æ¼”å˜è½¨è¿¹
                    )
                    
                    self.graph_builder.add_relation(
                        graph,
                        source=entity2,
                        target=entity1,
                        relation_type=final_relation_type,
                        start_chapter=start_chapter,
                        end_chapter=end_chapter,
                        strength=strength,
                        confidence=classification['confidence'],
                        cooccurrence_count=count,
                        evolution=evolution  # æ·»åŠ æ¼”å˜è½¨è¿¹
                    )
                    
                    relation_count += 1
                
                # æ·»åŠ ä½é¢‘"å…±ç°"å…³ç³»è¾¹
                for entity1, entity2, chapters, count in weak_relations:
                    start_chapter = min(chapters)
                    end_chapter = max(chapters)
                    strength = min(count / 20.0, 1.0)
                    
                    self.graph_builder.add_relation(
                        graph,
                        source=entity1,
                        target=entity2,
                        relation_type='å…±ç°',
                        start_chapter=start_chapter,
                        end_chapter=end_chapter,
                        strength=strength,
                        confidence=0.5,
                        cooccurrence_count=count
                    )
                    
                    self.graph_builder.add_relation(
                        graph,
                        source=entity2,
                        target=entity1,
                        relation_type='å…±ç°',
                        start_chapter=start_chapter,
                        end_chapter=end_chapter,
                        strength=strength,
                        confidence=0.5,
                        cooccurrence_count=count
                    )
                    
                    relation_count += 1
                
                logger.info(f"âœ… æ·»åŠ äº† {relation_count} å¯¹åŒå‘å…³ç³»ï¼ˆå…± {relation_count * 2} æ¡è¾¹ï¼‰")
                
                # 4.5 è®¡ç®— PageRank é‡è¦æ€§
                logger.info(f"ğŸ“Š è®¡ç®— PageRank é‡è¦æ€§...")
                if graph.number_of_nodes() > 0:
                    pagerank = self.graph_analyzer.compute_pagerank(graph)
                    self.graph_analyzer.update_node_importance(graph, pagerank)
                
                # æ›´æ–°è¿›åº¦ï¼šPageRankè®¡ç®—å®Œæˆï¼ˆ89%-93%ï¼‰
                novel.index_progress = clamp_progress(0.93)
                db.commit()
                tracker.update_step(novel_id, 4, 'processing', 0.65, 'PageRankè®¡ç®—å®Œæˆ')
                if progress_callback:
                    await progress_callback(novel_id, 0.93, "PageRankè®¡ç®—å®Œæˆ")
                
                # 4.6 è®¡ç®—ç« èŠ‚é‡è¦æ€§
                logger.info(f"ğŸ“ˆ è®¡ç®—ç« èŠ‚é‡è¦æ€§...")
                if graph.number_of_nodes() > 0:
                    # ä¸ºæ¯ä¸ªç« èŠ‚è®¡ç®—é‡è¦æ€§
                    for chapter in db.query(Chapter).filter(Chapter.novel_id == novel_id).all():
                        importance = self.graph_analyzer.compute_chapter_importance(graph, chapter.chapter_num)
                        chapter.importance_score = importance
                    db.commit()
                    logger.info(f"âœ… ç« èŠ‚é‡è¦æ€§è®¡ç®—å®Œæˆ")
                else:
                    logger.warning(f"âš ï¸ å›¾è°±ä¸ºç©ºï¼Œè·³è¿‡ç« èŠ‚é‡è¦æ€§è®¡ç®—")
                
                # æ›´æ–°è¿›åº¦ï¼šç« èŠ‚é‡è¦æ€§è®¡ç®—å®Œæˆï¼ˆ93%-96%ï¼‰
                novel.index_progress = clamp_progress(0.96)
                db.commit()
                tracker.update_step(novel_id, 4, 'processing', 0.80, 'ç« èŠ‚é‡è¦æ€§è®¡ç®—å®Œæˆ')
                if progress_callback:
                    await progress_callback(novel_id, 0.96, "ç« èŠ‚é‡è¦æ€§è®¡ç®—å®Œæˆ")
                
                # 4.7 ä¿å­˜çŸ¥è¯†å›¾è°±
                logger.info(f"ğŸ’¾ ä¿å­˜çŸ¥è¯†å›¾è°±...")
                self.graph_builder.save_graph(graph, novel_id)
                
                logger.info(f"âœ… çŸ¥è¯†å›¾è°±æ„å»ºå®Œæˆ: {graph.number_of_nodes()}èŠ‚ç‚¹, {graph.number_of_edges()}è¾¹")
                
                # æ›´æ–°è¿›åº¦ï¼šçŸ¥è¯†å›¾è°±ä¿å­˜å®Œæˆï¼ˆ96%-98%ï¼‰
                novel.index_progress = clamp_progress(0.98)
                db.commit()
                
                # æ›´æ–°æ­¥éª¤4ä¸ºå®Œæˆ
                from app.services.indexing_progress_tracker import get_progress_tracker
                tracker = get_progress_tracker()
                tracker.update_step(novel_id, 4, 'completed', 1.0, f"çŸ¥è¯†å›¾è°±æ„å»ºå®Œæˆ({graph.number_of_nodes()}èŠ‚ç‚¹)")
                
                if progress_callback:
                    await progress_callback(novel_id, 0.98, f"çŸ¥è¯†å›¾è°±æ„å»ºå®Œæˆ({graph.number_of_nodes()}èŠ‚ç‚¹)")
                
            except Exception as e:
                logger.error(f"âš ï¸ çŸ¥è¯†å›¾è°±æ„å»ºå¤±è´¥: {e}")
                logger.exception(e)
                # çŸ¥è¯†å›¾è°±æ„å»ºå¤±è´¥ä¸å½±å“æ•´ä½“ç´¢å¼•æµç¨‹
                
                # æ›´æ–°æ­¥éª¤4ä¸ºå¤±è´¥ï¼Œå¹¶å°†è¿›åº¦è®¾ç½®ä¸º98%ï¼ˆè·³è¿‡å›¾è°±æ„å»ºï¼‰
                from app.services.indexing_progress_tracker import get_progress_tracker
                tracker = get_progress_tracker()
                tracker.update_step(novel_id, 4, 'failed', 0.0, "çŸ¥è¯†å›¾è°±æ„å»ºå¤±è´¥", error=str(e))
                tracker.add_warning(novel_id, f"çŸ¥è¯†å›¾è°±æ„å»ºå¤±è´¥: {str(e)}")
                
                # å³ä½¿å›¾è°±å¤±è´¥ï¼Œä¹Ÿå°†è¿›åº¦æ¨è¿›åˆ°98%
                novel.index_progress = clamp_progress(0.98)
                db.commit()
                
                if progress_callback:
                    await progress_callback(novel_id, 0.98, "çŸ¥è¯†å›¾è°±æ„å»ºå¤±è´¥ï¼Œç»§ç»­å®Œæˆç´¢å¼•")
            
            # è®°å½•å›¾è°±æ„å»ºé˜¶æ®µçš„Tokenæ¶ˆè€—ï¼ˆæ— è®ºæˆåŠŸè¿˜æ˜¯å¤±è´¥éƒ½è®°å½•ï¼‰
            from app.services.indexing_progress_tracker import get_progress_tracker
            from app.utils.token_counter import get_token_counter
            tracker = get_progress_tracker()
            token_counter = get_token_counter()
            
            # è®°å½•å±æ€§æå–çš„tokenï¼ˆå³ä½¿ä¸º0ä¹Ÿè®°å½•ï¼Œæ˜¾ç¤ºå®Œæ•´æµç¨‹ï¼‰
            attr_cost = token_counter.calculate_cost(graph_attribute_tokens, 0, 'glm-4-flash')
            tracker.add_token_usage(
                novel_id=novel_id,
                step_name='å›¾è°±-å±æ€§æå–',
                model_name='glm-4-flash',
                input_tokens=graph_attribute_tokens,
                output_tokens=0,
                cost=attr_cost
            )
            logger.info(f"ğŸ“Š å±æ€§æå–Token: {graph_attribute_tokens}")
            
            # è®°å½•å…³ç³»åˆ†ç±»çš„tokenï¼ˆå³ä½¿ä¸º0ä¹Ÿè®°å½•ï¼Œæ˜¾ç¤ºå®Œæ•´æµç¨‹ï¼‰
            rel_cost = token_counter.calculate_cost(graph_relation_tokens, 0, 'glm-4-flash')
            tracker.add_token_usage(
                novel_id=novel_id,
                step_name='å›¾è°±-å…³ç³»åˆ†ç±»',
                model_name='glm-4-flash',
                input_tokens=graph_relation_tokens,
                output_tokens=0,
                cost=rel_cost
            )
            logger.info(f"ğŸ“Š å…³ç³»åˆ†ç±»Token: {graph_relation_tokens}")
            
            # 5. æ›´æ–°å°è¯´ç»Ÿè®¡ä¿¡æ¯å¹¶ä¿å­˜tokenç»Ÿè®¡
            novel.total_chunks = total_chunks
            novel.embedding_tokens = total_embedding_tokens  # ä¿å­˜embedding tokenæ¶ˆè€—
            novel.index_status = IndexStatus.COMPLETED.value
            novel.index_progress = clamp_progress(1.0)  # ç¡®ä¿ç²¾ç¡®ä¸º1.0
            novel.indexed_date = novel.updated_at
            db.commit()
            
            # è®¡ç®—å›¾è°±æ„å»ºæ€»token
            total_graph_tokens = graph_attribute_tokens + graph_relation_tokens + graph_evolution_tokens
            
            # ä¿å­˜tokenç»Ÿè®¡åˆ°token_statsè¡¨
            try:
                from app.services.token_stats_service import get_token_stats_service
                token_stats_service = get_token_stats_service()
                
                # è®°å½•Embedding-3æ¨¡å‹çš„tokenä½¿ç”¨
                token_stats_service.record_token_usage(
                    db=db,
                    operation_type='index',
                    operation_id=novel_id,
                    model_name='embedding-3',
                    input_tokens=total_embedding_tokens,
                    output_tokens=0
                )
                
                # è®°å½•GLM-4-Flashå›¾è°±æ„å»ºçš„tokenä½¿ç”¨
                if total_graph_tokens > 0:
                    token_stats_service.record_token_usage(
                        db=db,
                        operation_type='index',
                        operation_id=novel_id,
                        model_name='glm-4-flash',
                        input_tokens=total_graph_tokens,
                        output_tokens=0
                    )
                
                logger.info(f"âœ… Tokenç»Ÿè®¡å·²ä¿å­˜: embedding={total_embedding_tokens}, graph={total_graph_tokens} tokens")
            except Exception as e:
                logger.warning(f"âš ï¸ Tokenç»Ÿè®¡ä¿å­˜å¤±è´¥ï¼ˆä¸å½±å“ç´¢å¼•ï¼‰: {e}")
            
            # å‘é€æœ€ç»ˆè¿›åº¦ï¼ˆåŒ…å«å®Œæ•´çš„tokenç»Ÿè®¡ï¼‰
            final_token_stats = {
                "embeddingTokens": total_embedding_tokens,
                "graphAttributeTokens": graph_attribute_tokens,
                "graphRelationTokens": graph_relation_tokens,
                "graphEvolutionTokens": graph_evolution_tokens,
                "graphTotalTokens": total_graph_tokens,
                "totalTokens": total_embedding_tokens + total_graph_tokens
            }
            
            logger.info(f"ğŸ“Š æœ€ç»ˆTokenç»Ÿè®¡: embedding={total_embedding_tokens}, graph={total_graph_tokens}, total={final_token_stats['totalTokens']}")
            
            if progress_callback:
                await progress_callback(novel_id, 1.0, "ç´¢å¼•å®Œæˆ!", final_token_stats)
            
            logger.info(f"âœ… å°è¯´ ID={novel_id} ç´¢å¼•å®Œæˆ: {total_chapters}ç« , {total_chunks}å—, {total_embedding_tokens} tokens")
            return True
            
        except Exception as e:
            logger.error(f"âŒ ç´¢å¼•å¤±è´¥: {e}")
            
            # æ›´æ–°çŠ¶æ€ä¸ºå¤±è´¥
            novel = db.query(Novel).filter(Novel.id == novel_id).first()
            if novel:
                novel.index_status = IndexStatus.FAILED.value
                db.commit()
            
            if progress_callback:
                await progress_callback(novel_id, 0.0, f"ç´¢å¼•å¤±è´¥: {str(e)}")
            
            return False
    
    def get_indexing_progress(
        self,
        db: Session,
        novel_id: int
    ) -> Dict:
        """
        è·å–ç´¢å¼•è¿›åº¦ï¼ˆåŒ…å«è¯¦ç»†ä¿¡æ¯ï¼‰
        
        Args:
            db: æ•°æ®åº“ä¼šè¯
            novel_id: å°è¯´ID
        
        Returns:
            Dict: è¿›åº¦ä¿¡æ¯
        """
        novel = db.query(Novel).filter(Novel.id == novel_id).first()
        if not novel:
            return {
                'found': False,
                'message': f'å°è¯´ ID={novel_id} ä¸å­˜åœ¨'
            }
        
        # ç»Ÿè®¡å·²å®Œæˆçš„ç« èŠ‚
        completed_chapters = db.query(Chapter).filter(
            Chapter.novel_id == novel_id
        ).count()
        
        # è·å–è¯¦ç»†ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
        from app.services.indexing_progress_tracker import get_progress_tracker
        tracker = get_progress_tracker()
        detail = tracker.get_detail(novel_id)
        
        return {
            'found': True,
            'novel_id': novel_id,
            'status': novel.index_status,
            'progress': novel.index_progress,
            'total_chapters': novel.total_chapters,
            'total_chars': novel.total_chars,
            'completed_chapters': completed_chapters,
            'total_chunks': novel.total_chunks,
            'message': self._get_status_message(novel.index_status, novel.index_progress),
            'detail': detail  # æ·»åŠ è¯¦ç»†ä¿¡æ¯
        }
    
    async def _extract_cooccurrence_contexts(
        self,
        entity1: str,
        entity2: str,
        chapter_nums: List[int],
        novel: Novel,
        db: Session
    ) -> List[str]:
        """
        æå–ä¸¤ä¸ªå®ä½“å…±ç°çš„ä¸Šä¸‹æ–‡ç‰‡æ®µ
        
        Args:
            entity1: å®ä½“1åç§°
            entity2: å®ä½“2åç§°
            chapter_nums: ç« èŠ‚å·åˆ—è¡¨
            novel: å°è¯´å¯¹è±¡
            db: æ•°æ®åº“ä¼šè¯
        
        Returns:
            ä¸Šä¸‹æ–‡ç‰‡æ®µåˆ—è¡¨
        """
        contexts = []
        relation_classifier = RelationshipClassifier()
        
        # è¯»å–å®Œæ•´æ–‡ä»¶å†…å®¹ï¼ˆä½¿ç”¨parserï¼Œé¿å…ç¼–ç é—®é¢˜ï¼‰
        file_path = Path(novel.file_path)
        if not file_path.exists():
            logger.warning(f"å°è¯´æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            return contexts
        
        try:
            # ä½¿ç”¨ä¸å‘é‡åŒ–é˜¶æ®µç›¸åŒçš„parserè¯»å–æ–‡ä»¶
            if novel.file_format == 'txt':
                full_content, _ = self.txt_parser.parse_file(str(file_path))
            elif novel.file_format == 'epub':
                full_content, _ = self.epub_parser.parse_file(str(file_path))
            else:
                logger.error(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {novel.file_format}")
                return contexts
        except Exception as e:
            logger.error(f"è¯»å–æ–‡ä»¶å¤±è´¥: {e}")
            return contexts
        
        for chapter_num in chapter_nums:
            try:
                # æŸ¥è¯¢ç« èŠ‚è®°å½•
                chapter = db.query(Chapter).filter(
                    Chapter.novel_id == novel.id,
                    Chapter.chapter_num == chapter_num
                ).first()
                
                if not chapter:
                    continue
                
                # ä½¿ç”¨å­—ç¬¦ä½ç½®åˆ‡ç‰‡ï¼ˆä¸å‘é‡åŒ–é˜¶æ®µä¸€è‡´ï¼Œé¿å…ç¼–ç é—®é¢˜ï¼‰
                content = full_content[chapter.start_pos:chapter.end_pos]
                
                if not content:
                    logger.warning(f"ç« èŠ‚{chapter_num}å†…å®¹ä¸ºç©º")
                    continue
                
                # æå–åŒ…å«ä¸¤ä¸ªå®ä½“çš„æ®µè½
                paragraph = relation_classifier._extract_paragraph_with_entities(
                    content, entity1, entity2, chapter_num
                )
                
                if paragraph:
                    contexts.append(paragraph)
                
                # æœ€å¤šæå–5ä¸ªä¸Šä¸‹æ–‡
                if len(contexts) >= 5:
                    break
                    
            except Exception as e:
                logger.warning(f"æå–ç« èŠ‚{chapter_num}ä¸Šä¸‹æ–‡å¤±è´¥: {e}")
                continue
        
        return contexts
    
    @staticmethod
    def _get_status_message(status: str, progress: float) -> str:
        """è·å–çŠ¶æ€æ¶ˆæ¯"""
        if status == IndexStatus.PENDING.value:
            return "ç­‰å¾…ç´¢å¼•"
        elif status == IndexStatus.PROCESSING.value:
            return f"ç´¢å¼•ä¸­ ({progress*100:.1f}%)"
        elif status == IndexStatus.COMPLETED.value:
            return "ç´¢å¼•å®Œæˆ"
        elif status == IndexStatus.FAILED.value:
            return "ç´¢å¼•å¤±è´¥"
        else:
            return "æœªçŸ¥çŠ¶æ€"


# å…¨å±€ç´¢å¼•æœåŠ¡å®ä¾‹
_indexing_service: Optional[IndexingService] = None


def get_indexing_service() -> IndexingService:
    """è·å–å…¨å±€ç´¢å¼•æœåŠ¡å®ä¾‹ï¼ˆå•ä¾‹ï¼‰"""
    global _indexing_service
    if _indexing_service is None:
        _indexing_service = IndexingService()
    return _indexing_service

