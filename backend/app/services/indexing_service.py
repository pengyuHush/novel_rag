"""
ç´¢å¼•æœåŠ¡
æ•´åˆæ–‡ä»¶è§£æã€ç« èŠ‚è¯†åˆ«ã€æ–‡æœ¬åˆ†å—ã€å‘é‡åŒ–ç­‰åŠŸèƒ½
"""

import logging
import asyncio
from typing import Dict, Optional, Callable
from pathlib import Path
from sqlalchemy.orm import Session

from app.services.parser.txt_parser import TXTParser
from app.services.parser.epub_parser import EPUBParser
from app.services.parser.chapter_detector import ChapterDetector
from app.services.text_splitter import get_text_splitter
from app.services.embedding_service import get_embedding_service
from app.services.nlp.entity_extractor import EntityExtractor
from app.services.nlp.entity_merger import EntityMerger
from app.services.entity_service import EntityService
from app.services.graph.graph_builder import GraphBuilder
from app.services.graph.graph_analyzer import GraphAnalyzer
from app.models.database import Novel, Chapter
from app.models.schemas import IndexStatus, FileFormat

logger = logging.getLogger(__name__)


def clamp_progress(value: float) -> float:
    """
    å°†è¿›åº¦å€¼é™åˆ¶åœ¨0-1èŒƒå›´å†…ï¼Œå¤„ç†æµ®ç‚¹æ•°ç²¾åº¦é—®é¢˜
    
    Args:
        value: åŸå§‹è¿›åº¦å€¼
    
    Returns:
        é™åˆ¶åçš„è¿›åº¦å€¼ï¼ˆ0.0-1.0ï¼‰
    """
    return max(0.0, min(value, 1.0))


class IndexingService:
    """ç´¢å¼•æœåŠ¡"""
    
    def __init__(self):
        """åˆå§‹åŒ–ç´¢å¼•æœåŠ¡"""
        self.txt_parser = TXTParser()
        self.epub_parser = EPUBParser()
        self.chapter_detector = ChapterDetector()
        self.text_splitter = get_text_splitter()
        self.embedding_service = get_embedding_service()
        
        # Phase 5: çŸ¥è¯†å›¾è°±ç›¸å…³æœåŠ¡
        self.entity_extractor = EntityExtractor()
        self.entity_merger = EntityMerger()
        self.entity_service = EntityService()
        self.graph_builder = GraphBuilder()
        self.graph_analyzer = GraphAnalyzer()
        
        logger.info("âœ… ç´¢å¼•æœåŠ¡åˆå§‹åŒ–å®Œæˆï¼ˆåŒ…å«çŸ¥è¯†å›¾è°±åŠŸèƒ½ï¼‰")
    
    async def index_novel(
        self,
        db: Session,
        novel_id: int,
        file_path: str,
        file_format: FileFormat,
        progress_callback: Optional[Callable] = None
    ) -> bool:
        """
        ç´¢å¼•å°è¯´
        
        Args:
            db: æ•°æ®åº“ä¼šè¯
            novel_id: å°è¯´ID
            file_path: æ–‡ä»¶è·¯å¾„
            file_format: æ–‡ä»¶æ ¼å¼
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•°
        
        Returns:
            bool: æ˜¯å¦æˆåŠŸ
        """
        try:
            # æ›´æ–°çŠ¶æ€ä¸ºå¤„ç†ä¸­
            novel = db.query(Novel).filter(Novel.id == novel_id).first()
            if not novel:
                raise ValueError(f"å°è¯´ ID={novel_id} ä¸å­˜åœ¨")
            
            novel.index_status = IndexStatus.PROCESSING.value
            novel.index_progress = clamp_progress(0.0)
            db.commit()
            
            # ç«‹å³åˆå§‹åŒ–è¿›åº¦è¿½è¸ªï¼ˆä¼°è®¡ç« èŠ‚æ•°ä¸º0ï¼Œåç»­æ›´æ–°ï¼‰
            from app.services.indexing_progress_tracker import get_progress_tracker
            tracker = get_progress_tracker()
            tracker.init_progress(novel_id, 0)
            tracker.update_step(novel_id, 0, 'processing', 0.0, 'å¼€å§‹è§£ææ–‡ä»¶...')
            
            if progress_callback:
                await progress_callback(novel_id, 0.0, "å¼€å§‹è§£ææ–‡ä»¶...")
            
            # 1. è§£ææ–‡ä»¶
            logger.info(f"ğŸ“– å¼€å§‹è§£ææ–‡ä»¶: {file_path}")
            if file_format == FileFormat.TXT:
                content, metadata = self.txt_parser.parse_file(file_path)
                chapters_data = self.chapter_detector.detect(content)
            elif file_format == FileFormat.EPUB:
                content, metadata = self.epub_parser.parse_file(file_path)
                chapters_data = self.epub_parser.detect_chapters(file_path)
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_format}")
            
            novel.total_chapters = len(chapters_data)
            novel.total_chars = metadata.get('total_chars', len(content))
            db.commit()
            
            # æ›´æ–°è¿›åº¦è¿½è¸ªçš„æ€»ç« èŠ‚æ•°
            tracker._details[novel_id]['steps'][3]['message'] = f'å…±{len(chapters_data)}ç« å¾…å¤„ç†'
            tracker.update_step(novel_id, 0, 'completed', 1.0, f'è§£æå®Œæˆï¼Œå…±{len(chapters_data)}ç« ')
            tracker.update_step(novel_id, 1, 'completed', 1.0, f'æ£€æµ‹åˆ°{len(chapters_data)}ä¸ªç« èŠ‚')
            tracker.update_step(novel_id, 2, 'processing', 0.0, 'å‡†å¤‡å¤„ç†ç« èŠ‚...')
            
            if progress_callback:
                await progress_callback(novel_id, 0.1, f"æ–‡ä»¶è§£æå®Œæˆï¼Œæ£€æµ‹åˆ°{len(chapters_data)}ç« ")
            
            # 2. åˆ›å»ºChromaDBé›†åˆ
            collection_name = self.embedding_service.create_collection(novel_id)
            
            # 3. å¤„ç†æ¯ä¸ªç« èŠ‚
            total_chapters = len(chapters_data)
            total_chunks = 0
            total_embedding_tokens = 0  # åˆå§‹åŒ–tokenè®¡æ•°å™¨
            
            # æ›´æ–°æ­¥éª¤2ä¸ºcompletedï¼Œæ­¥éª¤3ä¸ºprocessing
            from app.services.indexing_progress_tracker import get_progress_tracker
            tracker = get_progress_tracker()
            tracker.update_step(novel_id, 2, 'completed', 1.0, f'å‡†å¤‡å¤„ç†{total_chapters}ä¸ªç« èŠ‚')
            tracker.update_step(novel_id, 3, 'processing', 0.0, 'å¼€å§‹å¤„ç†ç« èŠ‚...')
            
            for i, chapter_data in enumerate(chapters_data):
                chapter_num = chapter_data['chapter_num']
                chapter_title = chapter_data.get('title', f"ç¬¬{chapter_num}ç« ")
                
                logger.info(f"ğŸ“ å¤„ç†ç« èŠ‚ {chapter_num}/{total_chapters}: {chapter_title}")
                
                # æå–ç« èŠ‚å†…å®¹
                chapter_content = self.chapter_detector.extract_chapter_content(
                    content,
                    chapter_data['start_pos'],
                    chapter_data['end_pos'],
                    include_title=True
                )
                
                # ä¿å­˜ç« èŠ‚åˆ°æ•°æ®åº“
                chapter = Chapter(
                    novel_id=novel_id,
                    chapter_num=chapter_num,
                    chapter_title=chapter_title,
                    char_count=len(chapter_content),
                    start_pos=chapter_data['start_pos'],
                    end_pos=chapter_data['end_pos']
                )
                db.add(chapter)
                db.commit()
                
                # æ–‡æœ¬åˆ†å—
                chunks = self.text_splitter.split_chapter(
                    chapter_content,
                    novel_id,
                    chapter_num,
                    chapter_title
                )
                
                chapter.chunk_count = len(chunks)
                total_chunks += len(chunks)
                
                # å‘é‡åŒ–å¹¶å­˜å‚¨ï¼ˆè·å–tokenæ¶ˆè€—ï¼‰
                success, chapter_tokens = self.embedding_service.process_chapter(
                    novel_id,
                    chapter_num,
                    chapter_title,
                    chunks
                )
                
                # ç´¯åŠ tokenæ¶ˆè€—
                total_embedding_tokens += chapter_tokens
                
                if not success:
                    logger.warning(f"âš ï¸ ç« èŠ‚ {chapter_num} å¤„ç†å¤±è´¥")
                    # è®°å½•å¤±è´¥ç« èŠ‚
                    from app.services.indexing_progress_tracker import get_progress_tracker
                    tracker = get_progress_tracker()
                    tracker.add_failed_chapter(novel_id, chapter_num, chapter_title, "å‘é‡åŒ–å¤„ç†å¤±è´¥")
                
                # æ›´æ–°è¿›åº¦ï¼ˆç¡®ä¿ä¸è¶…è¿‡1.0ï¼‰
                progress = clamp_progress(0.1 + 0.9 * (i + 1) / total_chapters)
                novel.index_progress = progress
                db.commit()
                
                # æ›´æ–°æ­¥éª¤3çš„è¿›åº¦
                from app.services.indexing_progress_tracker import get_progress_tracker
                tracker = get_progress_tracker()
                step_progress = clamp_progress((i + 1) / total_chapters)
                tracker.update_step(novel_id, 3, 'processing', step_progress, f'å·²å®Œæˆ {i+1}/{total_chapters} ç« ')
                
                # æ„å»ºtokenç»Ÿè®¡ä¿¡æ¯
                token_stats = {
                    "embeddingTokens": total_embedding_tokens,
                    "totalTokens": total_embedding_tokens
                }
                
                if progress_callback:
                    await progress_callback(
                        novel_id,
                        progress,
                        f"å·²å®Œæˆ {i+1}/{total_chapters} ç« ",
                        token_stats
                    )
            
            # æ ‡è®°æ­¥éª¤3ä¸ºå®Œæˆï¼Œå¹¶è®°å½•æ€»Tokenæ¶ˆè€—
            from app.services.indexing_progress_tracker import get_progress_tracker
            from app.utils.token_counter import get_token_counter
            tracker = get_progress_tracker()
            tracker.update_step(novel_id, 3, 'completed', 1.0, f'æ‰€æœ‰ç« èŠ‚å¤„ç†å®Œæˆï¼ˆ{total_chapters}ç« ï¼‰')
            
            # è®°å½•æ€»çš„å‘é‡åŒ–Tokenæ¶ˆè€—
            if total_embedding_tokens > 0:
                token_counter = get_token_counter()
                cost = token_counter.calculate_cost(total_embedding_tokens, 0, 'embedding-3')
                tracker.add_token_usage(
                    novel_id=novel_id,
                    step_name='ç”ŸæˆåµŒå…¥å‘é‡',
                    model_name='embedding-3',
                    input_tokens=total_embedding_tokens,
                    output_tokens=0,
                    cost=cost
                )
            
            # 4. Phase 5: æ„å»ºçŸ¥è¯†å›¾è°±
            logger.info(f"ğŸ•¸ï¸ å¼€å§‹æ„å»ºçŸ¥è¯†å›¾è°±...")
            
            # æ›´æ–°æ­¥éª¤4ä¸ºprocessing
            tracker.update_step(novel_id, 4, 'processing', 0.0, 'å¼€å§‹æ„å»ºçŸ¥è¯†å›¾è°±...')
            
            if progress_callback:
                await progress_callback(novel_id, 0.95, "å¼€å§‹æ„å»ºçŸ¥è¯†å›¾è°±...")
            
            try:
                # 4.1 æå–å®ä½“
                logger.info(f"ğŸ“ æå–å®ä½“ä¸­...")
                
                # æ£€æŸ¥ HanLP æ˜¯å¦å¯ç”¨
                if not self.entity_extractor.hanlp_client.is_available():
                    logger.warning(f"âš ï¸ HanLP ä¸å¯ç”¨ï¼Œè·³è¿‡çŸ¥è¯†å›¾è°±æ„å»º")
                    logger.warning(f"   æç¤º: å¦‚éœ€ä½¿ç”¨çŸ¥è¯†å›¾è°±åŠŸèƒ½ï¼Œè¯·å®‰è£… HanLP:")
                    logger.warning(f"   pip install hanlp")
                    raise Exception("HanLP ä¸å¯ç”¨")  # è§¦å‘å¼‚å¸¸å¤„ç†ï¼Œè·³è¿‡çŸ¥è¯†å›¾è°±
                
                chapters_for_extraction = [
                    (ch.chapter_num, self.chapter_detector.extract_chapter_content(
                        content, ch.start_pos, ch.end_pos, include_title=True
                    ))
                    for ch in db.query(Chapter).filter(Chapter.novel_id == novel_id).all()
                ]
                
                # ä¼˜åŒ–ï¼šä¸€æ¬¡éå†åŒæ—¶å®Œæˆå®ä½“æå–ã€é¢‘ç‡ç»Ÿè®¡å’Œç« èŠ‚èŒƒå›´è®¡ç®—ï¼ˆæ€§èƒ½æå‡50%ï¼‰
                logger.info(f"ğŸ“ æå–å®ä½“ä¸­ï¼ˆå…±{len(chapters_for_extraction)}ç« ï¼‰...")
                from collections import Counter
                
                entity_counters = {
                    'characters': Counter(),
                    'locations': Counter(),
                    'organizations': Counter()
                }
                chapter_ranges = {}
                chapter_entity_map = {}  # è®°å½•æ¯ç« çš„å®ä½“åˆ—è¡¨ï¼ˆç”¨äºæ„å»ºå…±ç°å…³ç³»ï¼‰
                
                for chapter_num, chapter_text in chapters_for_extraction:
                    # åªè°ƒç”¨ä¸€æ¬¡ HanLP
                    chapter_entities = self.entity_extractor.extract_from_chapter(chapter_text, chapter_num)
                    
                    # è®°å½•æœ¬ç« å‡ºç°çš„æ‰€æœ‰è§’è‰²å®ä½“ï¼ˆä»…è§’è‰²å‚ä¸å…³ç³»å›¾ï¼‰
                    chapter_entity_map[chapter_num] = set(chapter_entities.get('characters', []))
                    
                    # åŒæ—¶å®Œæˆé¢‘ç‡ç»Ÿè®¡å’Œç« èŠ‚èŒƒå›´è®¡ç®—
                    for entity_type in ['characters', 'locations', 'organizations']:
                        for entity_name in chapter_entities.get(entity_type, []):
                            # ä»»åŠ¡1: ç»Ÿè®¡é¢‘ç‡
                            entity_counters[entity_type][entity_name] += 1
                            
                            # ä»»åŠ¡2: è®°å½•ç« èŠ‚èŒƒå›´
                            if entity_name not in chapter_ranges:
                                chapter_ranges[entity_name] = [chapter_num, chapter_num]
                            else:
                                chapter_ranges[entity_name][1] = chapter_num
                
                # è½¬æ¢ä¸ºå…ƒç»„
                chapter_ranges = {name: tuple(range_list) for name, range_list in chapter_ranges.items()}
                
                # æ£€æŸ¥æ˜¯å¦æå–åˆ°å®ä½“
                total_entities = sum(len(counter) for counter in entity_counters.values())
                if total_entities == 0:
                    logger.warning(f"âš ï¸ æœªæå–åˆ°ä»»ä½•å®ä½“ï¼Œè·³è¿‡çŸ¥è¯†å›¾è°±æ„å»º")
                    logger.warning(f"   å¯èƒ½åŸå› :")
                    logger.warning(f"   1. HanLP æ¨¡å‹æœªæ­£ç¡®åŠ è½½")
                    logger.warning(f"   2. æ–‡æœ¬å†…å®¹ä¸é€‚åˆå®ä½“è¯†åˆ«")
                    logger.warning(f"   3. æ–‡æœ¬æ ¼å¼é—®é¢˜")
                    raise Exception("æœªæå–åˆ°ä»»ä½•å®ä½“")  # è§¦å‘å¼‚å¸¸å¤„ç†ï¼Œè·³è¿‡çŸ¥è¯†å›¾è°±
                
                logger.info(f"âœ… å®ä½“æå–å®Œæˆ: è§’è‰²{len(entity_counters['characters'])} "
                           f"åœ°ç‚¹{len(entity_counters['locations'])} "
                           f"ç»„ç»‡{len(entity_counters['organizations'])}")
                
                # 4.2 å®ä½“å»é‡ä¸åˆå¹¶
                logger.info(f"ğŸ”€ å®ä½“å»é‡ä¸åˆå¹¶ä¸­...")
                merged_entities = {}
                merged_chapter_ranges = {}
                
                for entity_type in ['characters', 'locations', 'organizations']:
                    # è·å–è¯¥ç±»å‹çš„æ‰€æœ‰å®ä½“
                    entity_list = list(entity_counters.get(entity_type, {}).keys())
                    
                    # åˆå¹¶ç›¸ä¼¼å®ä½“
                    merge_mapping = self.entity_merger.merge_entities(entity_list)
                    
                    # æ›´æ–°è®¡æ•°å’Œç« èŠ‚èŒƒå›´
                    from collections import Counter
                    merged_counter = Counter()
                    for main_name, aliases in merge_mapping.items():
                        # åˆå¹¶è®¡æ•°
                        total_count = sum(entity_counters[entity_type].get(alias, 0) for alias in aliases)
                        merged_counter[main_name] = total_count
                        
                        # åˆå¹¶ç« èŠ‚èŒƒå›´ï¼ˆå–æœ€å°å’Œæœ€å¤§ï¼‰
                        min_chapter = min(chapter_ranges.get(alias, (9999, 9999))[0] for alias in aliases)
                        max_chapter = max(chapter_ranges.get(alias, (0, 0))[1] for alias in aliases)
                        merged_chapter_ranges[main_name] = (min_chapter, max_chapter)
                    
                    merged_entities[entity_type] = merged_counter
                
                # 4.3 å­˜å‚¨å®ä½“åˆ°æ•°æ®åº“
                logger.info(f"ğŸ’¾ ä¿å­˜å®ä½“åˆ°æ•°æ®åº“...")
                entity_count = self.entity_service.save_entities(
                    db, novel_id, merged_entities, merged_chapter_ranges
                )
                logger.info(f"âœ… ä¿å­˜äº† {entity_count} ä¸ªå®ä½“")
                
                # 4.4 æ„å»ºçŸ¥è¯†å›¾è°±
                logger.info(f"ğŸ•¸ï¸ æ„å»ºçŸ¥è¯†å›¾è°±...")
                graph = self.graph_builder.create_graph(novel_id)
                
                # æ·»åŠ å®ä½“èŠ‚ç‚¹
                for entity_type in ['characters', 'locations', 'organizations']:
                    for entity_name, count in merged_entities.get(entity_type, {}).items():
                        # ä½¿ç”¨åˆå¹¶åçš„ç« èŠ‚èŒƒå›´
                        first_ch, last_ch = merged_chapter_ranges.get(entity_name, (1, total_chapters))
                        self.graph_builder.add_entity(
                            graph,
                            entity_name=entity_name,
                            entity_type=entity_type,
                            first_chapter=first_ch,
                            last_chapter=last_ch,
                            mention_count=count
                        )
                
                # æ·»åŠ è§’è‰²é—´çš„å…±ç°å…³ç³»è¾¹
                logger.info(f"ğŸ”— æ„å»ºè§’è‰²å…³ç³»...")
                cooccurrence_count = {}  # (entity1, entity2) -> count
                cooccurrence_chapters = {}  # (entity1, entity2) -> [chapter_nums]
                
                for chapter_num, entities in chapter_entity_map.items():
                    entity_list = list(entities)
                    # å¯¹è¯¥ç« èŠ‚çš„ä»»æ„ä¸¤ä¸ªè§’è‰²å»ºç«‹å…±ç°å…³ç³»
                    for i in range(len(entity_list)):
                        for j in range(i + 1, len(entity_list)):
                            entity1, entity2 = sorted([entity_list[i], entity_list[j]])
                            pair = (entity1, entity2)
                            
                            if pair not in cooccurrence_count:
                                cooccurrence_count[pair] = 0
                                cooccurrence_chapters[pair] = []
                            
                            cooccurrence_count[pair] += 1
                            cooccurrence_chapters[pair].append(chapter_num)
                
                # æ·»åŠ å…±ç°å…³ç³»è¾¹ï¼ˆè¿‡æ»¤æ‰å…±ç°æ¬¡æ•°å°‘äº3æ¬¡çš„å¼±å…³ç³»ï¼‰
                min_cooccurrence = 3
                relation_count = 0
                for (entity1, entity2), count in cooccurrence_count.items():
                    if count >= min_cooccurrence:
                        chapters = cooccurrence_chapters[(entity1, entity2)]
                        start_chapter = min(chapters)
                        end_chapter = max(chapters)
                        
                        # æ ¹æ®å…±ç°é¢‘ç‡è®¡ç®—å…³ç³»å¼ºåº¦ï¼ˆå½’ä¸€åŒ–åˆ°0-1ï¼‰
                        strength = min(count / 20.0, 1.0)  # å…±ç°20æ¬¡ä»¥ä¸Šè§†ä¸ºå¼ºå…³ç³»
                        
                        # æ·»åŠ åŒå‘è¾¹ï¼ˆå…±ç°å…³ç³»æ˜¯å¯¹ç§°çš„ï¼‰
                        self.graph_builder.add_relation(
                            graph,
                            source=entity1,
                            target=entity2,
                            relation_type='å…±ç°',
                            start_chapter=start_chapter,
                            end_chapter=end_chapter,
                            strength=strength,
                            cooccurrence_count=count
                        )
                        
                        # æ·»åŠ åå‘è¾¹
                        self.graph_builder.add_relation(
                            graph,
                            source=entity2,
                            target=entity1,
                            relation_type='å…±ç°',
                            start_chapter=start_chapter,
                            end_chapter=end_chapter,
                            strength=strength,
                            cooccurrence_count=count
                        )
                        
                        relation_count += 1
                
                logger.info(f"âœ… æ·»åŠ äº† {relation_count} å¯¹åŒå‘å…±ç°å…³ç³»ï¼ˆå…± {relation_count * 2} æ¡è¾¹ï¼‰")
                
                # 4.5 è®¡ç®— PageRank é‡è¦æ€§
                logger.info(f"ğŸ“Š è®¡ç®— PageRank é‡è¦æ€§...")
                if graph.number_of_nodes() > 0:
                    pagerank = self.graph_analyzer.compute_pagerank(graph)
                    self.graph_analyzer.update_node_importance(graph, pagerank)
                
                # 4.6 è®¡ç®—ç« èŠ‚é‡è¦æ€§
                logger.info(f"ğŸ“ˆ è®¡ç®—ç« èŠ‚é‡è¦æ€§...")
                if graph.number_of_nodes() > 0:
                    # ä¸ºæ¯ä¸ªç« èŠ‚è®¡ç®—é‡è¦æ€§
                    for chapter in db.query(Chapter).filter(Chapter.novel_id == novel_id).all():
                        importance = self.graph_analyzer.compute_chapter_importance(graph, chapter.chapter_num)
                        chapter.importance_score = importance
                    db.commit()
                    logger.info(f"âœ… ç« èŠ‚é‡è¦æ€§è®¡ç®—å®Œæˆ")
                else:
                    logger.warning(f"âš ï¸ å›¾è°±ä¸ºç©ºï¼Œè·³è¿‡ç« èŠ‚é‡è¦æ€§è®¡ç®—")
                
                # 4.7 ä¿å­˜çŸ¥è¯†å›¾è°±
                logger.info(f"ğŸ’¾ ä¿å­˜çŸ¥è¯†å›¾è°±...")
                self.graph_builder.save_graph(graph, novel_id)
                
                logger.info(f"âœ… çŸ¥è¯†å›¾è°±æ„å»ºå®Œæˆ: {graph.number_of_nodes()}èŠ‚ç‚¹, {graph.number_of_edges()}è¾¹")
                
                # æ›´æ–°æ­¥éª¤4
                from app.services.indexing_progress_tracker import get_progress_tracker
                tracker = get_progress_tracker()
                tracker.update_step(novel_id, 4, 'completed', 1.0, f"çŸ¥è¯†å›¾è°±æ„å»ºå®Œæˆ({graph.number_of_nodes()}èŠ‚ç‚¹)")
                
            except Exception as e:
                logger.error(f"âš ï¸ çŸ¥è¯†å›¾è°±æ„å»ºå¤±è´¥: {e}")
                logger.exception(e)
                # çŸ¥è¯†å›¾è°±æ„å»ºå¤±è´¥ä¸å½±å“æ•´ä½“ç´¢å¼•æµç¨‹
                
                # æ›´æ–°æ­¥éª¤4ä¸ºå¤±è´¥
                from app.services.indexing_progress_tracker import get_progress_tracker
                tracker = get_progress_tracker()
                tracker.update_step(novel_id, 4, 'failed', 0.0, "çŸ¥è¯†å›¾è°±æ„å»ºå¤±è´¥", error=str(e))
                tracker.add_warning(novel_id, f"çŸ¥è¯†å›¾è°±æ„å»ºå¤±è´¥: {str(e)}")
            
            # 5. æ›´æ–°å°è¯´ç»Ÿè®¡ä¿¡æ¯å¹¶ä¿å­˜tokenç»Ÿè®¡
            novel.total_chunks = total_chunks
            novel.embedding_tokens = total_embedding_tokens  # ä¿å­˜embedding tokenæ¶ˆè€—
            novel.index_status = IndexStatus.COMPLETED.value
            novel.index_progress = clamp_progress(1.0)  # ç¡®ä¿ç²¾ç¡®ä¸º1.0
            novel.indexed_date = novel.updated_at
            db.commit()
            
            # ä¿å­˜tokenç»Ÿè®¡åˆ°token_statsè¡¨
            try:
                from app.services.token_stats_service import get_token_stats_service
                token_stats_service = get_token_stats_service()
                
                # è®°å½•Embedding-3æ¨¡å‹çš„tokenä½¿ç”¨
                token_stats_service.record_token_usage(
                    db=db,
                    operation_type='index',
                    operation_id=novel_id,
                    model_name='embedding-3',
                    input_tokens=total_embedding_tokens,
                    output_tokens=0
                )
                logger.info(f"âœ… Tokenç»Ÿè®¡å·²ä¿å­˜: {total_embedding_tokens} tokens")
            except Exception as e:
                logger.warning(f"âš ï¸ Tokenç»Ÿè®¡ä¿å­˜å¤±è´¥ï¼ˆä¸å½±å“ç´¢å¼•ï¼‰: {e}")
            
            # å‘é€æœ€ç»ˆè¿›åº¦ï¼ˆåŒ…å«å®Œæ•´çš„tokenç»Ÿè®¡ï¼‰
            final_token_stats = {
                "embeddingTokens": total_embedding_tokens,
                "totalTokens": total_embedding_tokens
            }
            
            if progress_callback:
                await progress_callback(novel_id, 1.0, "ç´¢å¼•å®Œæˆ!", final_token_stats)
            
            logger.info(f"âœ… å°è¯´ ID={novel_id} ç´¢å¼•å®Œæˆ: {total_chapters}ç« , {total_chunks}å—, {total_embedding_tokens} tokens")
            return True
            
        except Exception as e:
            logger.error(f"âŒ ç´¢å¼•å¤±è´¥: {e}")
            
            # æ›´æ–°çŠ¶æ€ä¸ºå¤±è´¥
            novel = db.query(Novel).filter(Novel.id == novel_id).first()
            if novel:
                novel.index_status = IndexStatus.FAILED.value
                db.commit()
            
            if progress_callback:
                await progress_callback(novel_id, 0.0, f"ç´¢å¼•å¤±è´¥: {str(e)}")
            
            return False
    
    def get_indexing_progress(
        self,
        db: Session,
        novel_id: int
    ) -> Dict:
        """
        è·å–ç´¢å¼•è¿›åº¦ï¼ˆåŒ…å«è¯¦ç»†ä¿¡æ¯ï¼‰
        
        Args:
            db: æ•°æ®åº“ä¼šè¯
            novel_id: å°è¯´ID
        
        Returns:
            Dict: è¿›åº¦ä¿¡æ¯
        """
        novel = db.query(Novel).filter(Novel.id == novel_id).first()
        if not novel:
            return {
                'found': False,
                'message': f'å°è¯´ ID={novel_id} ä¸å­˜åœ¨'
            }
        
        # ç»Ÿè®¡å·²å®Œæˆçš„ç« èŠ‚
        completed_chapters = db.query(Chapter).filter(
            Chapter.novel_id == novel_id
        ).count()
        
        # è·å–è¯¦ç»†ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
        from app.services.indexing_progress_tracker import get_progress_tracker
        tracker = get_progress_tracker()
        detail = tracker.get_detail(novel_id)
        
        return {
            'found': True,
            'novel_id': novel_id,
            'status': novel.index_status,
            'progress': novel.index_progress,
            'total_chapters': novel.total_chapters,
            'total_chars': novel.total_chars,
            'completed_chapters': completed_chapters,
            'total_chunks': novel.total_chunks,
            'message': self._get_status_message(novel.index_status, novel.index_progress),
            'detail': detail  # æ·»åŠ è¯¦ç»†ä¿¡æ¯
        }
    
    @staticmethod
    def _get_status_message(status: str, progress: float) -> str:
        """è·å–çŠ¶æ€æ¶ˆæ¯"""
        if status == IndexStatus.PENDING.value:
            return "ç­‰å¾…ç´¢å¼•"
        elif status == IndexStatus.PROCESSING.value:
            return f"ç´¢å¼•ä¸­ ({progress*100:.1f}%)"
        elif status == IndexStatus.COMPLETED.value:
            return "ç´¢å¼•å®Œæˆ"
        elif status == IndexStatus.FAILED.value:
            return "ç´¢å¼•å¤±è´¥"
        else:
            return "æœªçŸ¥çŠ¶æ€"


# å…¨å±€ç´¢å¼•æœåŠ¡å®ä¾‹
_indexing_service: Optional[IndexingService] = None


def get_indexing_service() -> IndexingService:
    """è·å–å…¨å±€ç´¢å¼•æœåŠ¡å®ä¾‹ï¼ˆå•ä¾‹ï¼‰"""
    global _indexing_service
    if _indexing_service is None:
        _indexing_service = IndexingService()
    return _indexing_service

