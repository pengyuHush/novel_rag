"""
é˜¿é‡Œé€šä¹‰åƒé—®LLMå®¢æˆ·ç«¯
"""

import logging
from typing import Dict, List, Generator, Any
from http import HTTPStatus
from dashscope import Generation
from app.services.llm.base import BaseLLMClient
from app.core.config import settings

logger = logging.getLogger(__name__)


class AliLLMClient(BaseLLMClient):
    """é˜¿é‡Œé€šä¹‰åƒé—®LLMå®¢æˆ·ç«¯"""
    
    def __init__(self):
        """åˆå§‹åŒ–é˜¿é‡Œå®¢æˆ·ç«¯"""
        import dashscope
        dashscope.api_key = settings.ali_api_key
        logger.info(f"âœ… é˜¿é‡Œé€šä¹‰åƒé—®å®¢æˆ·ç«¯åˆå§‹åŒ–å®Œæˆ")
    
    @property
    def provider_name(self) -> str:
        """æä¾›å•†åç§°"""
        return "ali"
    
    def supports_thinking(self, model: str) -> bool:
        """
        æ£€æŸ¥æ¨¡å‹æ˜¯å¦æ”¯æŒthinkingæ¨¡å¼
        
        Args:
            model: æ¨¡å‹åç§°
        
        Returns:
            æ˜¯å¦æ”¯æŒthinking
        """
        # Qwen3 åŠä»¥ä¸Šç‰ˆæœ¬çš„ä¸»æµæ¨¡å‹éƒ½æ”¯æŒæ€è€ƒæ¨¡å¼
        thinking_models = ["qwen-max", "qwen-plus", "qwen-turbo", "qwen3"]
        return any(thinking_model in model.lower() for thinking_model in thinking_models)
    
    def chat_completion(
        self,
        messages: List[Dict[str, str]],
        model: str,
        **kwargs
    ) -> Dict[str, Any]:
        """
        éæµå¼å¯¹è¯ç”Ÿæˆ
        
        Args:
            messages: å¯¹è¯æ¶ˆæ¯åˆ—è¡¨
            model: æ¨¡å‹åç§°ï¼ˆå¦‚"qwen-max"ï¼‰
            **kwargs: å…¶ä»–å‚æ•°ï¼ŒåŒ…æ‹¬ï¼š
                - enable_thinking: æ˜¯å¦å¯ç”¨æ€è€ƒæ¨¡å¼ï¼ˆé»˜è®¤Trueï¼‰
                - thinking_budget: æœ€å¤§æ¨ç†è¿‡ç¨‹Tokenæ•°
        
        Returns:
            åŒ…å«contentã€usageã€reasoning_contentç­‰ä¿¡æ¯çš„å­—å…¸
        """
        try:
            # å‡†å¤‡APIè°ƒç”¨å‚æ•°
            api_params = {
                "model": model,
                "messages": messages,
                "result_format": "message",
                "stream": False,
            }
            
            # å¦‚æœæ¨¡å‹æ”¯æŒæ€è€ƒæ¨¡å¼ä¸”æœªæ˜¾å¼è®¾ç½®ï¼Œé»˜è®¤å¯ç”¨
            enable_thinking = False
            if self.supports_thinking(model):
                enable_thinking = kwargs.pop('enable_thinking', True)
                if enable_thinking:
                    api_params['enable_thinking'] = True
                    # å¦‚æœæœ‰thinking_budgetå‚æ•°ï¼Œä¹Ÿä¼ é€’
                    if 'thinking_budget' in kwargs:
                        api_params['thinking_budget'] = kwargs.pop('thinking_budget')
            
            # æ·»åŠ å…¶ä»–å‚æ•°
            api_params.update(kwargs)
            
            logger.info(f"ğŸ“¤ è°ƒç”¨é˜¿é‡Œé€šä¹‰åƒé—®æ¨¡å‹: {model} (æ€è€ƒæ¨¡å¼: {enable_thinking})")
            
            # è°ƒç”¨DashScope API
            response = Generation.call(**api_params)
            
            # æ£€æŸ¥å“åº”çŠ¶æ€
            if response.status_code != HTTPStatus.OK:
                error_msg = f"é˜¿é‡ŒAPIè°ƒç”¨å¤±è´¥: {response.code} - {response.message}"
                logger.error(error_msg)
                raise Exception(error_msg)
            
            # æå–å“åº”å†…å®¹
            choice = response.output.choices[0]
            message = choice.message
            content = message.content
            
            # æå–æ€è€ƒå†…å®¹ï¼ˆå¦‚æœå¯ç”¨äº†æ€è€ƒæ¨¡å¼ï¼‰
            reasoning_content = None
            if enable_thinking:
                try:
                    # ä½¿ç”¨ getattr å®‰å…¨è·å–ï¼Œé¿å…å±æ€§ä¸å­˜åœ¨æ—¶æŠ›å‡ºå¼‚å¸¸
                    reasoning_content = getattr(message, 'reasoning_content', None)
                    if reasoning_content:
                        logger.info(f"ğŸ§  è·å–åˆ°æ€è€ƒå†…å®¹: {len(reasoning_content)} å­—ç¬¦")
                except Exception as e:
                    logger.warning(f"è·å–æ€è€ƒå†…å®¹æ—¶å‡ºé”™: {e}")
            
            # æå–tokenä½¿ç”¨ç»Ÿè®¡
            usage = {}
            if hasattr(response, 'usage') and response.usage:
                usage = {
                    "prompt_tokens": response.usage.input_tokens,
                    "completion_tokens": response.usage.output_tokens,
                    "total_tokens": response.usage.total_tokens,
                }
            
            result = {
                "content": content,
                "usage": usage,
                "finish_reason": choice.finish_reason,
            }
            
            # å¦‚æœæœ‰æ€è€ƒå†…å®¹ï¼Œæ·»åŠ åˆ°ç»“æœä¸­
            if reasoning_content:
                result["reasoning_content"] = reasoning_content
            
            logger.info(f"âœ… é˜¿é‡ŒAPIè°ƒç”¨æˆåŠŸï¼Œç”Ÿæˆ {len(content)} å­—ç¬¦")
            return result
            
        except Exception as e:
            logger.error(f"âŒ é˜¿é‡ŒAPIè°ƒç”¨å¤±è´¥: {e}")
            raise
    
    def chat_completion_stream(
        self,
        messages: List[Dict[str, str]],
        model: str,
        **kwargs
    ) -> Generator[Dict[str, Any], None, None]:
        """
        æµå¼å¯¹è¯ç”Ÿæˆ
        
        Args:
            messages: å¯¹è¯æ¶ˆæ¯åˆ—è¡¨
            model: æ¨¡å‹åç§°
            **kwargs: å…¶ä»–å‚æ•°ï¼ŒåŒ…æ‹¬ï¼š
                - enable_thinking: æ˜¯å¦å¯ç”¨æ€è€ƒæ¨¡å¼ï¼ˆé»˜è®¤Trueï¼‰
                - thinking_budget: æœ€å¤§æ¨ç†è¿‡ç¨‹Tokenæ•°
        
        Yields:
            åŒ…å«å¢é‡å†…å®¹å’Œæ€è€ƒå†…å®¹çš„å­—å…¸
        """
        try:
            # å‡†å¤‡APIè°ƒç”¨å‚æ•°
            api_params = {
                "model": model,
                "messages": messages,
                "result_format": "message",
                "stream": True,
                "incremental_output": True,  # å¢é‡è¾“å‡º
            }
            
            # å¦‚æœæ¨¡å‹æ”¯æŒæ€è€ƒæ¨¡å¼ä¸”æœªæ˜¾å¼è®¾ç½®ï¼Œé»˜è®¤å¯ç”¨
            enable_thinking = False
            if self.supports_thinking(model):
                enable_thinking = kwargs.pop('enable_thinking', True)
                if enable_thinking:
                    api_params['enable_thinking'] = True
                    # å¦‚æœæœ‰thinking_budgetå‚æ•°ï¼Œä¹Ÿä¼ é€’
                    if 'thinking_budget' in kwargs:
                        api_params['thinking_budget'] = kwargs.pop('thinking_budget')
            
            # æ·»åŠ å…¶ä»–å‚æ•°
            api_params.update(kwargs)
            
            logger.info(f"ğŸ“¤ è°ƒç”¨é˜¿é‡Œé€šä¹‰åƒé—®æ¨¡å‹ï¼ˆæµå¼ï¼‰: {model} (æ€è€ƒæ¨¡å¼: {enable_thinking})")
            
            # è°ƒç”¨DashScope APIï¼ˆæµå¼ï¼‰
            responses = Generation.call(**api_params)
            
            # æµå¼å¤„ç†å“åº”
            for response in responses:
                try:
                    if response.status_code != HTTPStatus.OK:
                        error_msg = f"é˜¿é‡ŒAPIæµå¼è°ƒç”¨å¤±è´¥: {response.code} - {response.message}"
                        logger.error(error_msg)
                        yield {
                            "content": "",
                            "error": error_msg,
                            "finish_reason": "error"
                        }
                        break
                    
                    # æå–å¢é‡å†…å®¹
                    chunk_data = {
                        "content": "",
                        "reasoning_content": None,
                        "usage": None,
                        "finish_reason": None,
                    }
                    
                    # è·å–å¢é‡æ–‡æœ¬
                    if hasattr(response.output, 'choices') and len(response.output.choices) > 0:
                        choice = response.output.choices[0]
                        message = choice.message
                        
                        # è·å–æ­£å¸¸å†…å®¹
                        if hasattr(message, 'content'):
                            chunk_data["content"] = message.content or ""
                        
                        # è·å–æ€è€ƒå†…å®¹ï¼ˆå¦‚æœå¯ç”¨äº†æ€è€ƒæ¨¡å¼ï¼‰
                        if enable_thinking:
                            try:
                                # ä½¿ç”¨ getattr å®‰å…¨è·å–ï¼Œé¿å…å±æ€§ä¸å­˜åœ¨æ—¶æŠ›å‡ºå¼‚å¸¸
                                reasoning = getattr(message, 'reasoning_content', None)
                                if reasoning:
                                    chunk_data["reasoning_content"] = reasoning
                                    logger.debug(f"ğŸ§  æµå¼æ€è€ƒå†…å®¹: {len(reasoning)} å­—ç¬¦")
                            except AttributeError as ae:
                                # å±æ€§ä¸å­˜åœ¨æˆ–è®¿é—®å¤±è´¥ï¼Œç»§ç»­å¤„ç†æ­£å¸¸å†…å®¹
                                logger.debug(f"æ€è€ƒå†…å®¹å­—æ®µä¸å­˜åœ¨: {ae}")
                            except Exception as te:
                                # å…¶ä»–å¼‚å¸¸ä¹Ÿæ•è·ï¼Œé¿å…ä¸­æ–­æµå¼è¾“å‡º
                                logger.warning(f"è·å–æ€è€ƒå†…å®¹æ—¶å‡ºé”™: {te}")
                        
                        # æ£€æŸ¥æ˜¯å¦ç»“æŸ
                        if hasattr(choice, 'finish_reason') and choice.finish_reason:
                            chunk_data["finish_reason"] = choice.finish_reason
                            
                            # åœ¨æœ€åä¸€ä¸ªchunkä¸­è¿”å›tokenç»Ÿè®¡
                            if hasattr(response, 'usage') and response.usage:
                                chunk_data["usage"] = {
                                    "prompt_tokens": response.usage.input_tokens,
                                    "completion_tokens": response.usage.output_tokens,
                                    "total_tokens": response.usage.total_tokens,
                                }
                    
                    yield chunk_data
                    
                except Exception as chunk_error:
                    # å•ä¸ªchunkå¤„ç†é”™è¯¯ä¸åº”è¯¥ä¸­æ–­æ•´ä¸ªæµ
                    logger.warning(f"å¤„ç†chunkæ—¶å‡ºé”™: {chunk_error}, ç»§ç»­å¤„ç†...")
                    continue
            
            logger.info(f"âœ… é˜¿é‡Œæµå¼APIè°ƒç”¨å®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ é˜¿é‡Œæµå¼APIè°ƒç”¨å¤±è´¥: {e}")
            yield {
                "content": "",
                "error": str(e),
                "finish_reason": "error"
            }

