"""
æŸ¥è¯¢åˆ†è§£æœåŠ¡

è´Ÿè´£å°†å¤æ‚æŸ¥è¯¢æ™ºèƒ½åˆ†è§£ä¸ºå¤šä¸ªå­æŸ¥è¯¢ï¼Œæå‡æ£€ç´¢è¦†ç›–ç‡
"""

import logging
import json
import re
from typing import List, Optional, Dict, Tuple
from app.services.zhipu_client import ZhipuAIClient
from app.core.trace_logger import get_trace_logger

logger = logging.getLogger(__name__)
trace_logger = get_trace_logger()


class QueryDecomposer:
    """
    æŸ¥è¯¢åˆ†è§£å™¨
    
    åŠŸèƒ½ï¼š
    1. åˆ¤æ–­æŸ¥è¯¢æ˜¯å¦éœ€è¦åˆ†è§£ï¼ˆå¤æ‚åº¦æ£€æµ‹ï¼‰
    2. ä½¿ç”¨LLMæ™ºèƒ½åˆ†è§£æŸ¥è¯¢ä¸ºå¤šä¸ªå­æŸ¥è¯¢
    3. éªŒè¯å’Œæ¸…ç†å­æŸ¥è¯¢
    """
    
    # æšä¸¾å…³é”®è¯ï¼ˆæç¤ºæŸ¥è¯¢åŒ…å«å¤šä¸ªä¿¡æ¯ç»´åº¦ï¼‰
    ENUMERATION_KEYWORDS = [
        "åŒ…å«", "åŒ…æ‹¬", "ä»¥åŠ", "å’Œ", "è¿˜æœ‰", "å¦å¤–",
        "ã€", "ï¼Œ", "ï¼›", "ç­‰", "ç­‰ç­‰"
    ]
    
    # å¤šé—®å¥æ ‡è®°
    QUESTION_MARKERS = [
        "æ˜¯è°", "æ˜¯ä»€ä¹ˆ", "åœ¨å“ªé‡Œ", "æ€ä¹ˆæ ·", "ä¸ºä»€ä¹ˆ",
        "å¦‚ä½•", "å“ªäº›", "å¤šå°‘", "å‡ ä¸ª", "ä»€ä¹ˆæ—¶å€™"
    ]
    
    def __init__(
        self,
        zhipu_client: ZhipuAIClient,
        max_subqueries: int = 5,
        complexity_threshold: int = 30,
        model: str = "glm-4-flash"
    ):
        """
        åˆå§‹åŒ–æŸ¥è¯¢åˆ†è§£å™¨
        
        Args:
            zhipu_client: æ™ºè°±AIå®¢æˆ·ç«¯
            max_subqueries: æœ€å¤šåˆ†è§£ä¸ºå‡ ä¸ªå­æŸ¥è¯¢
            complexity_threshold: æŸ¥è¯¢å­—æ•°é˜ˆå€¼
            model: ä½¿ç”¨çš„LLMæ¨¡å‹
        """
        self.zhipu_client = zhipu_client
        self.max_subqueries = max_subqueries
        self.complexity_threshold = complexity_threshold
        self.model = model
    
    def should_decompose(self, query: str) -> Tuple[bool, str]:
        """
        åˆ¤æ–­æŸ¥è¯¢æ˜¯å¦éœ€è¦åˆ†è§£
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            
        Returns:
            Tuple[bool, str]: (æ˜¯å¦éœ€è¦åˆ†è§£, åˆ¤æ–­åŸå› )
        """
        logger.info(f"ğŸ”§ [DEBUG] should_decomposeè¢«è°ƒç”¨: query='{query}', threshold={self.complexity_threshold}")
        
        # 1. æ£€æŸ¥æŸ¥è¯¢é•¿åº¦
        query_length = len(query)
        logger.info(f"ğŸ”§ [DEBUG] æ£€æŸ¥1 - æŸ¥è¯¢é•¿åº¦: {query_length} (é˜ˆå€¼={self.complexity_threshold})")
        if query_length > self.complexity_threshold:
            reason = f"æŸ¥è¯¢å­—æ•°è¶…è¿‡é˜ˆå€¼ï¼ˆ{query_length} > {self.complexity_threshold}ï¼‰"
            logger.info(f"âœ… [DEBUG] è§¦å‘åˆ†è§£: {reason}")
            return True, reason
        
        # 2. æ£€æŸ¥æšä¸¾å…³é”®è¯
        enumeration_count = sum(1 for kw in self.ENUMERATION_KEYWORDS if kw in query)
        logger.info(f"ğŸ”§ [DEBUG] æ£€æŸ¥2 - æšä¸¾å…³é”®è¯æ•°é‡: {enumeration_count} (éœ€è¦>=2)")
        if enumeration_count >= 2:
            reason = f"åŒ…å«{enumeration_count}ä¸ªæšä¸¾å…³é”®è¯"
            logger.info(f"âœ… [DEBUG] è§¦å‘åˆ†è§£: {reason}")
            return True, reason
        
        # 3. æ£€æŸ¥å¤šä¸ªé—®å¥
        question_count = sum(1 for marker in self.QUESTION_MARKERS if marker in query)
        logger.info(f"ğŸ”§ [DEBUG] æ£€æŸ¥3 - ç–‘é—®è¯æ•°é‡: {question_count} (éœ€è¦>=2)")
        if question_count >= 2:
            reason = f"åŒ…å«{question_count}ä¸ªä¸åŒçš„ç–‘é—®è¯"
            logger.info(f"âœ… [DEBUG] è§¦å‘åˆ†è§£: {reason}")
            return True, reason
        
        # 4. æ£€æŸ¥æ˜¯å¦åŒ…å«æ˜ç¡®çš„åˆ—ä¸¾ç»“æ„
        has_list_structure = bool(re.search(r'[ï¼Œã€ï¼›].{2,}[ï¼Œã€ï¼›].{2,}', query))
        logger.info(f"ğŸ”§ [DEBUG] æ£€æŸ¥4 - åˆ—ä¸¾ç»“æ„: {has_list_structure}")
        if has_list_structure:
            reason = "åŒ…å«æ˜ç¡®çš„åˆ—ä¸¾ç»“æ„"
            logger.info(f"âœ… [DEBUG] è§¦å‘åˆ†è§£: {reason}")
            return True, reason
        
        reason = "æŸ¥è¯¢ç›¸å¯¹ç®€å•"
        logger.info(f"âŒ [DEBUG] ä¸è§¦å‘åˆ†è§£: {reason}")
        return False, reason
    
    def decompose_query(
        self,
        query: str,
        query_id: Optional[int] = None
    ) -> List[str]:
        """
        ä½¿ç”¨LLMåˆ†è§£æŸ¥è¯¢ä¸ºå¤šä¸ªå­æŸ¥è¯¢
        
        Args:
            query: åŸå§‹æŸ¥è¯¢
            query_id: æŸ¥è¯¢IDï¼ˆç”¨äºæ—¥å¿—ï¼‰
            
        Returns:
            List[str]: å­æŸ¥è¯¢åˆ—è¡¨ï¼ˆå¦‚æœä¸éœ€è¦åˆ†è§£æˆ–å¤±è´¥ï¼Œè¿”å›ç©ºåˆ—è¡¨ï¼‰
        """
        # å…ˆåˆ¤æ–­æ˜¯å¦éœ€è¦åˆ†è§£
        should_decompose, reason = self.should_decompose(query)
        
        if not should_decompose:
            logger.info(f"ğŸ” æŸ¥è¯¢æ— éœ€åˆ†è§£: {reason}")
            if query_id:
                trace_logger.trace_step(
                    query_id=query_id,
                    step_name="æŸ¥è¯¢åˆ†è§£",
                    emoji="ğŸ”",
                    input_data=query,
                    output_data=f"æ— éœ€åˆ†è§£: {reason}",
                    status="skipped"
                )
            return []
        
        logger.info(f"ğŸ”¨ æŸ¥è¯¢éœ€è¦åˆ†è§£: {reason}")
        
        # æ„å»ºLLM Prompt
        prompt = self._build_decomposition_prompt(query)
        
        try:
            # è°ƒç”¨LLM
            response = self.zhipu_client.chat_completion(
                messages=[{"role": "user", "content": prompt}],
                model=self.model,
                temperature=0.3,
                max_tokens=500
            )
            
            content = response["content"].strip()
            logger.debug(f"LLMåˆ†è§£ç»“æœ: {content}")
            
            # è§£æJSONç»“æœ
            sub_queries = self._parse_subqueries(content)
            
            # éªŒè¯å’Œæ¸…ç†
            sub_queries = self._validate_subqueries(sub_queries, query)
            
            if len(sub_queries) > 1:
                logger.info(f"âœ… æŸ¥è¯¢åˆ†è§£æˆåŠŸ: {len(sub_queries)}ä¸ªå­æŸ¥è¯¢")
                logger.info(f"   å­æŸ¥è¯¢: {sub_queries}")
                
                # è®°å½•è¯¦ç»†æ—¥å¿—
                if query_id:
                    trace_logger.trace_step(
                        query_id=query_id,
                        step_name="æŸ¥è¯¢åˆ†è§£",
                        emoji="ğŸ”¨",
                        input_data={
                            "åŸå§‹æŸ¥è¯¢": query,
                            "åˆ¤æ–­ä¾æ®": reason,
                            "ä½¿ç”¨æ¨¡å‹": self.model
                        },
                        output_data={
                            "å­æŸ¥è¯¢æ•°é‡": len(sub_queries),
                            "å­æŸ¥è¯¢åˆ—è¡¨": sub_queries
                        },
                        status="success"
                    )
                
                return sub_queries
            else:
                logger.info("â„¹ï¸ LLMåˆ¤æ–­æŸ¥è¯¢æ— éœ€åˆ†è§£")
                if query_id:
                    trace_logger.trace_step(
                        query_id=query_id,
                        step_name="æŸ¥è¯¢åˆ†è§£",
                        emoji="â„¹ï¸",
                        input_data=query,
                        output_data="LLMåˆ¤æ–­æ— éœ€åˆ†è§£",
                        status="skipped"
                    )
                return []
        
        except Exception as e:
            logger.error(f"âŒ æŸ¥è¯¢åˆ†è§£å¤±è´¥: {e}")
            if query_id:
                trace_logger.trace_step(
                    query_id=query_id,
                    step_name="æŸ¥è¯¢åˆ†è§£",
                    emoji="âŒ",
                    input_data=query,
                    output_data=f"åˆ†è§£å¤±è´¥: {str(e)}",
                    status="failed"
                )
            return []
    
    def _build_decomposition_prompt(self, query: str) -> str:
        """æ„å»ºæŸ¥è¯¢åˆ†è§£çš„Prompt"""
        prompt = f"""ä½ æ˜¯æŸ¥è¯¢åˆ†è§£ä¸“å®¶ã€‚è¯·å°†å¤æ‚æŸ¥è¯¢æ‹†åˆ†ä¸ºå¤šä¸ªç‹¬ç«‹çš„å­æŸ¥è¯¢ã€‚

åŸå§‹æŸ¥è¯¢ï¼š{query}

è¦æ±‚ï¼š
1. æ¯ä¸ªå­æŸ¥è¯¢åº”è¯¥ç‹¬ç«‹ã€æ˜ç¡®ã€å¯å•ç‹¬å›ç­”
2. ä¿ç•™åŸæŸ¥è¯¢çš„æ ¸å¿ƒå®ä½“å’Œä¸Šä¸‹æ–‡
3. æœ€å¤šæ‹†åˆ†ä¸º{self.max_subqueries}ä¸ªå­æŸ¥è¯¢
4. å¦‚æœæŸ¥è¯¢æœ¬èº«å·²ç»è¶³å¤Ÿç®€å•ï¼ˆåªåŒ…å«å•ä¸€ä¿¡æ¯ç»´åº¦ï¼‰ï¼Œè¿”å›ç©ºåˆ—è¡¨
5. å­æŸ¥è¯¢åº”è¯¥è¦†ç›–åŸæŸ¥è¯¢çš„æ‰€æœ‰ä¿¡æ¯ç»´åº¦
6. è¾“å‡ºçº¯JSONæ ¼å¼çš„å­—ç¬¦ä¸²æ•°ç»„ï¼Œä¸è¦æœ‰å…¶ä»–æ–‡å­—

è¾“å‡ºæ ¼å¼ï¼ˆçº¯JSONï¼‰ï¼š
["å­æŸ¥è¯¢1", "å­æŸ¥è¯¢2", "å­æŸ¥è¯¢3"]

ç¤ºä¾‹1ï¼š
åŸå§‹æŸ¥è¯¢ï¼š"ä»‹ç»æå‡¡çš„èº«ä¸–ï¼ŒåŒ…å«ä»–çš„çˆ¶æ¯ã€å®¶æ—ã€å¸ˆå‚…ã€å¸ˆé—¨ä»¥åŠç°åœ¨çš„çŠ¶å†µ"
è¾“å‡ºï¼š["æå‡¡çš„çˆ¶æ¯æ˜¯è°", "æå‡¡çš„å®¶æ—èƒŒæ™¯", "æå‡¡çš„å¸ˆå‚…å’Œå¸ˆé—¨", "æå‡¡ç°åœ¨çš„çŠ¶å†µ"]

ç¤ºä¾‹2ï¼š
åŸå§‹æŸ¥è¯¢ï¼š"æå‡¡çš„æ¯äº²æ˜¯è°"
è¾“å‡ºï¼š[]

ç¤ºä¾‹3ï¼š
åŸå§‹æŸ¥è¯¢ï¼š"æè¿°å¼ ä¸‰ä¸°çš„æ­¦åŠŸå’Œä»–ä¸å¼ æ— å¿Œçš„å…³ç³»"
è¾“å‡ºï¼š["å¼ ä¸‰ä¸°çš„æ­¦åŠŸå®åŠ›", "å¼ ä¸‰ä¸°å’Œå¼ æ— å¿Œçš„å…³ç³»"]

è¯·ç›´æ¥è¾“å‡ºJSONæ ¼å¼çš„å­æŸ¥è¯¢æ•°ç»„ï¼š"""
        
        return prompt
    
    def _parse_subqueries(self, content: str) -> List[str]:
        """
        è§£æLLMè¿”å›çš„å­æŸ¥è¯¢åˆ—è¡¨
        
        æ”¯æŒå¤šç§æ ¼å¼ï¼š
        - çº¯JSONæ•°ç»„ï¼š["æŸ¥è¯¢1", "æŸ¥è¯¢2"]
        - å¸¦ä»£ç å—çš„JSONï¼š```json\n["æŸ¥è¯¢1", "æŸ¥è¯¢2"]\n```
        - å¸¦åºå·çš„åˆ—è¡¨ï¼š1. æŸ¥è¯¢1\n2. æŸ¥è¯¢2
        """
        # å°è¯•æå–JSONå†…å®¹
        json_match = re.search(r'\[.*?\]', content, re.DOTALL)
        if json_match:
            try:
                sub_queries = json.loads(json_match.group())
                if isinstance(sub_queries, list):
                    return [str(q).strip() for q in sub_queries if q]
            except json.JSONDecodeError:
                pass
        
        # å°è¯•è§£æå¸¦åºå·çš„åˆ—è¡¨æ ¼å¼
        lines = content.split('\n')
        sub_queries = []
        for line in lines:
            line = line.strip()
            # åŒ¹é… "1. æŸ¥è¯¢" æˆ– "- æŸ¥è¯¢" æ ¼å¼
            match = re.match(r'^[\d\-\*]+[\.\)]\s*(.+)$', line)
            if match:
                sub_queries.append(match.group(1).strip())
        
        if sub_queries:
            return sub_queries
        
        # å¦‚æœéƒ½å¤±è´¥äº†ï¼Œå°è¯•æŒ‰é€—å·åˆ†å‰²ï¼ˆæœ€åçš„fallbackï¼‰
        if ',' in content or 'ï¼Œ' in content:
            parts = re.split(r'[,ï¼Œ]', content)
            return [p.strip().strip('"\'') for p in parts if p.strip()]
        
        logger.warning(f"æ— æ³•è§£æå­æŸ¥è¯¢: {content}")
        return []
    
    def _validate_subqueries(self, sub_queries: List[str], original_query: str) -> List[str]:
        """
        éªŒè¯å’Œæ¸…ç†å­æŸ¥è¯¢
        
        1. å»é™¤ç©ºå­—ç¬¦ä¸²
        2. å»é‡
        3. é™åˆ¶æœ€å¤§æ•°é‡
        4. è¿‡æ»¤è¿‡çŸ­æˆ–æ— æ„ä¹‰çš„æŸ¥è¯¢
        """
        # å»é™¤ç©ºå­—ç¬¦ä¸²å’Œè¿‡çŸ­çš„æŸ¥è¯¢
        valid_queries = []
        for q in sub_queries:
            q = q.strip().strip('"\'')
            if len(q) >= 3 and q not in valid_queries:  # è‡³å°‘3ä¸ªå­—ç¬¦ä¸”ä¸é‡å¤
                valid_queries.append(q)
        
        # é™åˆ¶æœ€å¤§æ•°é‡
        if len(valid_queries) > self.max_subqueries:
            logger.warning(f"å­æŸ¥è¯¢æ•°é‡è¶…è¿‡é™åˆ¶ï¼Œæˆªæ–­ä¸ºå‰{self.max_subqueries}ä¸ª")
            valid_queries = valid_queries[:self.max_subqueries]
        
        # å¦‚æœæ‰€æœ‰å­æŸ¥è¯¢éƒ½ä¸åŸæŸ¥è¯¢ç›¸åŒï¼Œè¿”å›ç©ºï¼ˆè¯´æ˜æ— éœ€åˆ†è§£ï¼‰
        if len(valid_queries) == 1 and valid_queries[0] == original_query:
            return []
        
        return valid_queries

