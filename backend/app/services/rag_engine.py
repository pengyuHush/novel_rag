"""
RAGå¼•æ“ - æ£€ç´¢å¢å¼ºç”Ÿæˆ
å®ç°åŸºç¡€RAGæµç¨‹ï¼Œæ”¯æŒæ™ºèƒ½æŸ¥è¯¢è·¯ç”±ã€æŸ¥è¯¢æ”¹å†™ã€è‡ªé€‚åº”Prompt
"""

import logging
import math
import re
from typing import List, Dict, Optional, Tuple
from sqlalchemy.orm import Session

from app.services.embedding_service import get_embedding_service
from app.services.zhipu_client import get_zhipu_client
from app.services.query_router import query_router, QueryType
from app.services.query_rewriter import get_query_rewriter
from app.services.adaptive_prompt_builder import get_adaptive_prompt_builder
from app.services.nlp import get_hanlp_client
from app.models.database import Novel, Chapter
from app.models.schemas import Citation, Confidence
from app.core.trace_logger import get_trace_logger
from app.core.config import settings

logger = logging.getLogger(__name__)
trace_logger = get_trace_logger()


class RAGEngine:
    """RAGå¼•æ“"""
    
    def __init__(self):
        """åˆå§‹åŒ–RAGå¼•æ“"""
        self.embedding_service = get_embedding_service()
        self.zhipu_client = get_zhipu_client()
        self.top_k_retrieval = 30  # æ£€ç´¢Top-30
        self.top_k_rerank = 10     # RerankåTop-10
        self.min_similarity_threshold = settings.min_similarity_threshold  # ç›¸ä¼¼åº¦é˜ˆå€¼
        
        # æŸ¥è¯¢ä¼˜åŒ–ç»„ä»¶
        self.query_rewriter = get_query_rewriter()
        self.prompt_builder = get_adaptive_prompt_builder()
        
        # NLPç»„ä»¶ï¼ˆå¤ç”¨ç°æœ‰çš„HanLPå®¢æˆ·ç«¯ï¼‰
        self.hanlp_client = get_hanlp_client()
        
        # GraphRAGç»„ä»¶
        from app.services.graph.graph_query import GraphQuery
        from app.services.graph.graph_analyzer import GraphAnalyzer
        from app.services.graph.graph_builder import GraphBuilder
        
        self.graph_query = GraphQuery()
        self.graph_analyzer = GraphAnalyzer()
        self.graph_builder = GraphBuilder()
        
        logger.info("âœ… RAGå¼•æ“åˆå§‹åŒ–å®Œæˆï¼ˆå«æŸ¥è¯¢ä¼˜åŒ–ã€GraphRAGæ”¯æŒï¼‰")
    
    def query_embedding(self, query: str, query_id: Optional[int] = None) -> List[float]:
        """
        æŸ¥è¯¢å‘é‡åŒ–
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            query_id: æŸ¥è¯¢IDï¼ˆç”¨äºæ—¥å¿—è®°å½•ï¼‰
        
        Returns:
            List[float]: æŸ¥è¯¢å‘é‡
        """
        embedding = self.zhipu_client.embed_text(query)
        
        # è¯¦ç»†æ—¥å¿—
        if query_id:
            trace_logger.trace_embedding(
                query_id=query_id,
                query_text=query,
                embedding_vector=embedding
            )
        
        return embedding
    
    def vector_search(
        self,
        novel_id: int,
        query_embedding: List[float],
        top_k: int = None,
        query_id: Optional[int] = None
    ) -> Dict:
        """
        è¯­ä¹‰æ£€ç´¢
        
        Args:
            novel_id: å°è¯´ID
            query_embedding: æŸ¥è¯¢å‘é‡
            top_k: è¿”å›Top-Kç»“æœ
            query_id: æŸ¥è¯¢IDï¼ˆç”¨äºæ—¥å¿—è®°å½•ï¼‰
        
        Returns:
            Dict: æ£€ç´¢ç»“æœ
        """
        top_k = top_k or self.top_k_retrieval
        
        from app.core.chromadb_client import get_chroma_client
        chroma_client = get_chroma_client()
        
        collection_name = f"novel_{novel_id}"
        
        try:
            results = chroma_client.query_documents(
                collection_name=collection_name,
                query_embeddings=[query_embedding],
                n_results=top_k
            )
            
            original_count = len(results.get('ids', [[]])[0])
            
            # ğŸ¯ ç›¸ä¼¼åº¦é˜ˆå€¼è¿‡æ»¤
            ids = results.get('ids', [[]])[0]
            documents = results.get('documents', [[]])[0]
            metadatas = results.get('metadatas', [[]])[0]
            distances = results.get('distances', [[]])[0]
            
            # è¿‡æ»¤ä½ç›¸ä¼¼åº¦ç»“æœ
            filtered_ids = []
            filtered_documents = []
            filtered_metadatas = []
            filtered_distances = []
            
            for doc_id, content, metadata, distance in zip(ids, documents, metadatas, distances):
                # L2è·ç¦»ï¼šè·ç¦»è¶Šå°è¶Šç›¸ä¼¼ï¼Œè¿‡æ»¤æ‰è·ç¦»å¤§äºé˜ˆå€¼çš„ç»“æœ
                if distance <= self.min_similarity_threshold:
                    filtered_ids.append(doc_id)
                    filtered_documents.append(content)
                    filtered_metadatas.append(metadata)
                    filtered_distances.append(distance)
            
            # æ›´æ–°ç»“æœ
            results['ids'] = [filtered_ids]
            results['documents'] = [filtered_documents]
            results['metadatas'] = [filtered_metadatas]
            results['distances'] = [filtered_distances]
            
            filtered_count = len(filtered_ids)
            logger.info(f"âœ… è¯­ä¹‰æ£€ç´¢å®Œæˆ: {original_count} ä¸ªç»“æœ â†’ è¿‡æ»¤å {filtered_count} ä¸ª (é˜ˆå€¼: {self.min_similarity_threshold:.2f})")
            
            # è¯¦ç»†æ—¥å¿—
            if query_id:
                # æ ¼å¼åŒ–æ£€ç´¢ç»“æœ
                formatted_results = []
                for i, (doc_id, content, metadata, distance) in enumerate(zip(filtered_ids, filtered_documents, filtered_metadatas, filtered_distances), 1):
                    # L2è·ç¦»ï¼šdistanceæœ¬èº«å°±æ˜¯è·ç¦»å€¼ï¼ˆè¶Šå°è¶Šç›¸ä¼¼ï¼‰
                    formatted_results.append({
                        'id': doc_id,
                        'content': content,
                        'metadata': metadata,
                        'distance': distance,
                        'l2_distance': f"{distance:.4f}"
                    })
                
                trace_logger.trace_retrieval(
                    query_id=query_id,
                    top_k=top_k,
                    results=formatted_results
                )
                
                # å¦‚æœè¿‡æ»¤æ‰äº†ç»“æœï¼Œè®°å½•è¯¦æƒ…
                if original_count > filtered_count:
                    trace_logger.trace_step(
                        query_id=query_id,
                        step_name="L2è·ç¦»è¿‡æ»¤",
                        emoji="ğŸ”",
                        input_data=f"åŸå§‹ç»“æœ: {original_count} ä¸ª",
                        output_data={
                            "è¿‡æ»¤åç»“æœ": filtered_count,
                            "è¿‡æ»¤æ‰": original_count - filtered_count,
                            "L2è·ç¦»é˜ˆå€¼": self.min_similarity_threshold,
                            "æœ€å°L2è·ç¦»": f"{min(filtered_distances):.4f}" if filtered_distances else "N/A",
                            "æœ€å¤§L2è·ç¦»": f"{max(filtered_distances):.4f}" if filtered_distances else "N/A"
                        },
                        status="success"
                    )
            
            return results
            
        except Exception as e:
            logger.error(f"âŒ è¯­ä¹‰æ£€ç´¢å¤±è´¥: {e}")
            return {'ids': [[]], 'documents': [[]], 'metadatas': [[]], 'distances': [[]]}
    
    def keyword_search(
        self,
        db: Session,
        novel_id: int,
        query: str,
        top_k: int = 10
    ) -> List[Dict]:
        """
        å…³é”®è¯æ£€ç´¢ï¼ˆç®€å•å®ç°ï¼‰
        
        Args:
            db: æ•°æ®åº“ä¼šè¯
            novel_id: å°è¯´ID
            query: æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›Top-Kç»“æœ
        
        Returns:
            List[Dict]: æ£€ç´¢ç»“æœ
        """
        # ç®€å•çš„å…³é”®è¯åŒ¹é…ï¼ˆå®é™…åº”è¯¥ç”¨å…¨æ–‡ç´¢å¼•ï¼‰
        # è¿™é‡Œåªæ˜¯æ¼”ç¤ºï¼Œç”Ÿäº§ç¯å¢ƒåº”è¯¥ä½¿ç”¨Elasticsearchç­‰
        logger.info(f"ğŸ” å…³é”®è¯æ£€ç´¢: {query}")
        
        # TODO: å®ç°åŸºäºæ•°æ®åº“çš„å…³é”®è¯æ£€ç´¢
        # æš‚æ—¶è¿”å›ç©ºç»“æœ
        return []
    
    def _extract_entities(self, query: str) -> List[str]:
        """
        ä»æŸ¥è¯¢ä¸­æå–å…³é”®å®ä½“ï¼ˆäººåã€åœ°åç­‰ï¼‰
        
        å¤ç”¨ç°æœ‰çš„HanLPå®¢æˆ·ç«¯ï¼ˆä¸å°è¯´å¯¼å…¥æµç¨‹å…±äº«ï¼‰ï¼Œfallbackåˆ°ç®€å•æ­£åˆ™
        """
        try:
            # ä½¿ç”¨ç°æœ‰çš„HanLPå®¢æˆ·ç«¯ï¼ˆä¸å°è¯´å¯¼å…¥å…±äº«åŒä¸€ä¸ªå®ä¾‹ï¼‰
            if not self.hanlp_client.is_available():
                logger.warning("âš ï¸ HanLPä¸å¯ç”¨ï¼Œä½¿ç”¨ç®€å•æ­£åˆ™æå–å®ä½“")
                return self._extract_entities_fallback(query)
            
            # è°ƒç”¨HanLPæå–å®ä½“ - ä½¿ç”¨å®½æ¾æ¨¡å¼ï¼ˆæŸ¥è¯¢æ—¶ï¼‰
            entities_dict = self.hanlp_client.extract_entities(
                query, 
                max_length=512,
                strict=False  # ğŸ”‘ æŸ¥è¯¢æ—¶ä½¿ç”¨å®½æ¾æ¨¡å¼
            )
            
            # åˆå¹¶ä¸‰ç±»å®ä½“ï¼šäººåã€åœ°åã€ç»„ç»‡å
            entities = []
            entities.extend(entities_dict.get('characters', []))
            entities.extend(entities_dict.get('locations', []))
            entities.extend(entities_dict.get('organizations', []))
            
            # å¦‚æœHanLPæ²¡æå–åˆ°å®ä½“ï¼Œä½¿ç”¨fallback
            if not entities:
                logger.debug("HanLPæœªæå–åˆ°å®ä½“ï¼Œä½¿ç”¨fallbackæ–¹æ³•")
                return self._extract_entities_fallback(query)
            
            # å»é‡å¹¶ä¿æŒé¡ºåº
            seen = set()
            unique_entities = []
            for e in entities:
                if e not in seen:
                    seen.add(e)
                    unique_entities.append(e)
            
            logger.info(f"ğŸ¯ HanLPæå–å®ä½“: {unique_entities} (äººå:{len(entities_dict.get('characters', []))}, åœ°å:{len(entities_dict.get('locations', []))}, ç»„ç»‡:{len(entities_dict.get('organizations', []))})")
            return unique_entities
            
        except Exception as e:
            logger.warning(f"âš ï¸ HanLPå®ä½“æå–å¤±è´¥ï¼ˆ{type(e).__name__}: {e}ï¼‰ï¼Œä½¿ç”¨fallback")
            return self._extract_entities_fallback(query)
    
    def _extract_entities_fallback(self, query: str) -> List[str]:
        """
        Fallbackå®ä½“æå–æ–¹æ³•ï¼ˆç®€å•æ­£åˆ™ï¼‰
        """
        import re
        
        # æå–è¿ç»­ä¸­æ–‡å­—ç¬¦ï¼ˆ2-4å­—ï¼‰
        entities = re.findall(r'[\u4e00-\u9fa5]{2,4}', query)
        
        # è¿‡æ»¤å¸¸è§åœç”¨è¯
        stopwords = {'ä»€ä¹ˆ', 'æ€ä¹ˆ', 'ä¸ºä»€ä¹ˆ', 'å“ªé‡Œ', 'å¦‚ä½•', 'æ˜¯å¦', 'æœ‰æ²¡æœ‰', 
                     'å…³äº', 'æè¿°', 'ä»‹ç»', 'è¯´æ˜', 'è§£é‡Š', 'åˆ†æ', 'è¯„ä»·', 'æ—¶å€™',
                     'è¿™ä¸ª', 'é‚£ä¸ª', 'ä¸€ä¸ª', 'è¿™äº›', 'é‚£äº›', 'å‘ç”Ÿ', 'äº‹æƒ…'}
        entities = [e for e in entities if e not in stopwords]
        
        return entities
    
    def _resolve_entity_aliases(
        self, 
        entities: List[str], 
        novel_id: Optional[int],
        db: Optional[Session]
    ) -> List[str]:
        """
        å°†å®ä½“åˆ«åè§£æä¸ºè§„èŒƒåç§°
        
        Args:
            entities: æå–çš„å®ä½“åˆ—è¡¨ï¼ˆå¯èƒ½åŒ…å«åˆ«åï¼‰
            novel_id: å°è¯´ID
            db: æ•°æ®åº“ä¼šè¯
        
        Returns:
            è§£æåçš„è§„èŒƒåç§°åˆ—è¡¨
        """
        if not entities or not novel_id or not db:
            return entities
        
        try:
            from app.services.entity_service import get_entity_service
            entity_service = get_entity_service()
            
            canonical_entities = []
            for entity in entities:
                canonical = entity_service.get_canonical_name(db, novel_id, entity)
                if canonical != entity:
                    logger.info(f"ğŸ”„ å®ä½“åˆ«åè§£æ: '{entity}' â†’ '{canonical}'")
                canonical_entities.append(canonical)
            
            return canonical_entities
        except Exception as e:
            logger.warning(f"âš ï¸ åˆ«åè§£æå¤±è´¥: {e}")
            return entities
    
    def _calculate_entity_match_score(self, text: str, entities: List[str]) -> float:
        """
        è®¡ç®—æ–‡æœ¬ä¸­å®ä½“åŒ¹é…å¾—åˆ†ï¼ˆæ”¹è¿›ç‰ˆï¼Œé¿å…è¯¯åŒ¹é…ï¼‰
        
        Args:
            text: æ–‡æ¡£å†…å®¹
            entities: æŸ¥è¯¢ä¸­çš„å®ä½“åˆ—è¡¨
        
        Returns:
            float: åŒ¹é…å¾—åˆ† (0-1.5)
        """
        if not entities:
            return 1.0  # æ²¡æœ‰æ˜ç¡®å®ä½“ï¼Œä¸æƒ©ç½š
        
        import re
        
        matched_count = 0
        partial_match_count = 0  # éƒ¨åˆ†åŒ¹é…è®¡æ•°
        
        for entity in entities:
            # ç­–ç•¥1ï¼šé•¿å®ä½“ï¼ˆâ‰¥3å­—ç¬¦ï¼‰ä½¿ç”¨ç®€å•åŒ…å«å³å¯
            if len(entity) >= 3:
                if entity in text:
                    matched_count += 1
                continue
            
            # ç­–ç•¥2ï¼šçŸ­å®ä½“ï¼ˆ2å­—ç¬¦ï¼‰éœ€è¦ç²¾ç¡®åŒ¹é…
            # ä½¿ç”¨è¯è¾¹ç•Œï¼šå®ä½“å‰åä¸èƒ½æ˜¯ä¸­æ–‡å­—ç¬¦
            pattern = f'(?<![\\u4e00-\\u9fa5]){re.escape(entity)}(?![\\u4e00-\\u9fa5])'
            
            if re.search(pattern, text):
                # ç²¾ç¡®åŒ¹é…ï¼ˆç‹¬ç«‹è¯ï¼‰
                matched_count += 1
            elif entity in text:
                # éƒ¨åˆ†åŒ¹é…ï¼ˆåŒ…å«åœ¨å…¶ä»–è¯ä¸­ï¼‰
                partial_match_count += 1
        
        # è®¡ç®—å¾—åˆ†
        total_entities = len(entities)
        match_ratio = matched_count / total_entities
        partial_ratio = partial_match_count / total_entities
        
        # ç²¾ç¡®åŒ¹é… + éƒ¨åˆ†åŒ¹é…ï¼ˆæƒé‡å‡åŠï¼‰
        effective_ratio = match_ratio + (partial_ratio * 0.5)
        
        # å¾—åˆ†è®¡ç®—
        if effective_ratio >= 0.5:
            return 1.0 + (effective_ratio - 0.5)  # 1.0 - 1.5
        else:
            # ä¸¥é‡æƒ©ç½šï¼šåŒ¹é…ä¸è¶³50%
            return 0.3 + (effective_ratio * 0.7)  # 0.3 - 0.65
    
    def _calculate_recency_bias(
        self, 
        chapter_num: int, 
        total_chapters: int, 
        bias_weight: float
    ) -> float:
        """
        è®¡ç®—æ—¶é—´è¡°å‡åå‘å¾—åˆ†
        
        Args:
            chapter_num: å½“å‰ç« èŠ‚å·
            total_chapters: æ€»ç« èŠ‚æ•°
            bias_weight: è¡°å‡æƒé‡ (0.0-0.5)
        
        Returns:
            float: æ—¶é—´è¡°å‡å¾—åˆ† (0.7-1.3)
        """
        if bias_weight == 0.0 or total_chapters == 0:
            return 1.0  # æ— åå‘
        
        # ç« èŠ‚ä½ç½®å½’ä¸€åŒ– (0.0 = ç¬¬1ç« , 1.0 = æœ€åä¸€ç« )
        position = chapter_num / total_chapters
        
        # æŒ‡æ•°å¢é•¿æƒé‡
        recency_score = math.exp(bias_weight * position)
        
        # å½’ä¸€åŒ–åˆ° [0.7, 1.3] èŒƒå›´
        # bias_weight=0.5, chapter_num=total_chaptersæ—¶: recency_score â‰ˆ 1.65
        # å½’ä¸€åŒ–å â‰ˆ 1.3
        normalized = 0.7 + (recency_score - 1.0) * 0.6
        
        return max(0.7, min(1.3, normalized))
    
    def rerank(
        self,
        query: str,
        vector_results: Dict,
        keyword_results: List[Dict] = None,
        top_k: int = None,
        query_type: QueryType = None,
        novel_id: int = None,
        db: Session = None,
        query_id: Optional[int] = None,
        recency_bias_weight: float = 0.15
    ) -> List[Dict]:
        """
        æ··åˆRerankï¼Œæ”¯æŒæŸ¥è¯¢ç±»å‹ç‰¹å®šç­–ç•¥ + GraphRAGå¢å¼º + å®ä½“åŒ¹é…
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            vector_results: å‘é‡æ£€ç´¢ç»“æœ
            keyword_results: å…³é”®è¯æ£€ç´¢ç»“æœ
            top_k: è¿”å›Top-Kç»“æœ
            query_type: æŸ¥è¯¢ç±»å‹ï¼ˆè‡ªåŠ¨æ£€æµ‹æˆ–æ‰‹åŠ¨æŒ‡å®šï¼‰
            novel_id: å°è¯´IDï¼ˆç”¨äºGraphRAGï¼‰
            db: æ•°æ®åº“ä¼šè¯ï¼ˆç”¨äºGraphRAGï¼‰
        
        Returns:
            List[Dict]: Rerankåçš„ç»“æœ
        """
        top_k = top_k or self.top_k_rerank
        
        # æå–æŸ¥è¯¢ä¸­çš„å…³é”®å®ä½“
        query_entities = self._extract_entities(query)
        logger.info(f"ğŸ¯ æå–æŸ¥è¯¢å®ä½“: {query_entities}")
        
        # è§£æå®ä½“åˆ«åä¸ºè§„èŒƒåç§°
        query_entities = self._resolve_entity_aliases(query_entities, novel_id, db)
        if query_entities:
            logger.info(f"âœ… åˆ«åè§£æåå®ä½“: {query_entities}")
        
        # è‡ªåŠ¨æ£€æµ‹æŸ¥è¯¢ç±»å‹
        if query_type is None:
            query_type = query_router.classify_query(query)
        
        logger.info(f"ğŸ” æŸ¥è¯¢ç±»å‹: {query_type.value}")
        
        # è·å–æ€»ç« èŠ‚æ•°ï¼ˆç”¨äºæ—¶é—´è¡°å‡è®¡ç®—ï¼‰
        total_chapters = 0
        if novel_id and db:
            try:
                novel = db.query(Novel).filter(Novel.id == novel_id).first()
                if novel:
                    total_chapters = novel.total_chapters
                    logger.debug(f"ğŸ“š å°è¯´æ€»ç« èŠ‚æ•°: {total_chapters}")
            except Exception as e:
                logger.warning(f"âš ï¸ è·å–ç« èŠ‚æ•°å¤±è´¥: {e}")
        
        # GraphRAG: åŠ è½½çŸ¥è¯†å›¾è°±ï¼ˆå¦‚æœæä¾›äº†novel_idï¼‰
        graph = None
        chapter_importance_map = {}
        
        if novel_id is not None:
            try:
                graph = self.graph_builder.load_graph(novel_id)
                
                # è®¡ç®—æ‰€æœ‰ç« èŠ‚çš„é‡è¦æ€§è¯„åˆ†ï¼ˆç¼“å­˜ï¼‰
                if graph:
                    # è·å–æ‰€æœ‰ç« èŠ‚å·
                    chapters = set()
                    for node in graph.nodes():
                        first_chapter = graph.nodes[node].get('first_chapter')
                        if first_chapter:
                            chapters.add(first_chapter)
                    
                    # è®¡ç®—æ¯ä¸ªç« èŠ‚çš„é‡è¦æ€§
                    for chapter in chapters:
                        importance = self.graph_analyzer.compute_chapter_importance(graph, chapter)
                        chapter_importance_map[chapter] = importance
                    
                    logger.info(f"âœ… GraphRAG: åŠ è½½å›¾è°±æˆåŠŸï¼Œè®¡ç®—äº†{len(chapter_importance_map)}ä¸ªç« èŠ‚çš„é‡è¦æ€§")
            except Exception as e:
                logger.warning(f"âš ï¸ GraphRAGåŠ è½½å¤±è´¥ï¼ˆç»§ç»­ä½¿ç”¨çº¯å‘é‡æ£€ç´¢ï¼‰: {e}")
        
        # æå–å‘é‡æ£€ç´¢ç»“æœ
        documents = vector_results.get('documents', [[]])[0]
        metadatas = vector_results.get('metadatas', [[]])[0]
        distances = vector_results.get('distances', [[]])[0]
        
        # æ„å»ºå€™é€‰æ–‡æ¡£
        candidates = []
        for i, (doc, metadata, distance) in enumerate(zip(documents, metadatas, distances)):
            # L2è·ç¦»è½¬æ¢ä¸ºç›¸ä¼¼åº¦åˆ†æ•°ï¼šä½¿ç”¨é«˜æ–¯æ ¸å‡½æ•°
            # distance=0 -> score=1.0, distance=2 -> scoreâ‰ˆ0.135
            base_score = math.exp(-distance**2 / 2)
            
            # ğŸ¯ å®ä½“åŒ¹é…å¾—åˆ†
            entity_match_score = self._calculate_entity_match_score(doc, query_entities)
            
            # GraphRAG: è·å–ç« èŠ‚é‡è¦æ€§ï¼ˆæ—¶åºæƒé‡ï¼‰
            chapter_num = metadata.get('chapter_num')
            chapter_importance = 0.5  # é»˜è®¤ä¸­ç­‰é‡è¦æ€§
            
            if chapter_num and chapter_num in chapter_importance_map:
                chapter_importance = chapter_importance_map[chapter_num]
            
            # åº”ç”¨æŸ¥è¯¢ç±»å‹ç‰¹å®šçš„æƒé‡
            if query_type == QueryType.DIALOGUE:
                # å¯¹è¯ç±»æŸ¥è¯¢ï¼šæå‡åŒ…å«å¼•å·çš„å†…å®¹æƒé‡ + å®ä½“åŒ¹é…
                quote_boost = self._calculate_quote_boost(doc)
                
                # åŠ¨æ€è°ƒæ•´ï¼šé«˜ç›¸ä¼¼åº¦æ—¶é™ä½quote_boostå½±å“
                if base_score > 0.85:
                    quote_boost = 1.0 + (quote_boost - 1.0) * 0.5  # å‡å¼±quoteå½±å“
                    logger.debug(f"ğŸ”¥ é«˜ç›¸ä¼¼åº¦({base_score:.3f})ï¼šé™ä½å¯¹è¯æ ‡è®°æƒé‡å½±å“")
                
                # åº”ç”¨æ—¶é—´è¡°å‡
                recency_bias = self._calculate_recency_bias(
                    chapter_num, total_chapters, recency_bias_weight
                )
                
                final_score = base_score * quote_boost * entity_match_score * recency_bias
            elif query_type == QueryType.ANALYSIS:
                # åˆ†æç±»æŸ¥è¯¢ï¼šæå‡é‡è¦ç« èŠ‚æƒé‡ï¼ˆä½¿ç”¨å›¾è°±ç« èŠ‚é‡è¦æ€§ï¼‰+ å®ä½“åŒ¹é…
                importance_boost = chapter_importance + 0.5
                
                # åŠ¨æ€è°ƒæ•´ï¼šä½ç›¸ä¼¼åº¦æ—¶å¢å¼ºç« èŠ‚é‡è¦æ€§å½±å“
                if base_score < 0.60:
                    importance_boost = importance_boost * 1.3  # å¢å¼º30%
                    logger.debug(f"âš ï¸ ä½ç›¸ä¼¼åº¦({base_score:.3f})ï¼šå¢å¼ºç« èŠ‚é‡è¦æ€§æƒé‡")
                
                # åº”ç”¨æ—¶é—´è¡°å‡
                recency_bias = self._calculate_recency_bias(
                    chapter_num, total_chapters, recency_bias_weight
                )
                
                final_score = base_score * importance_boost * entity_match_score * recency_bias
            else:
                # äº‹å®ç±»æŸ¥è¯¢ - åŠ¨æ€æƒé‡è°ƒæ•´
                # åŸºç¡€æƒé‡é…æ¯”
                w_semantic = 0.50
                w_temporal = 0.10
                w_entity = 0.40
                
                # ğŸš€ åŠ¨æ€è°ƒæ•´ç­–ç•¥
                if base_score > 0.85:
                    # é«˜ç›¸ä¼¼åº¦ï¼ˆ>0.85ï¼‰ï¼šæ˜¾è‘—å¢å¼ºè¯­ä¹‰æƒé‡
                    w_semantic = 0.60  # +0.10
                    w_entity = 0.30    # -0.10
                    logger.debug(f"ğŸ”¥ é«˜ç›¸ä¼¼åº¦æ£€æµ‹({base_score:.3f})ï¼šå¢å¼ºè¯­ä¹‰æƒé‡")
                
                elif base_score < 0.50:
                    # ä½ç›¸ä¼¼åº¦ï¼ˆ<0.50ï¼‰ï¼šå¤§å¹…å¢å¼ºå®ä½“åŒ¹é…æƒé‡
                    w_semantic = 0.30  # -0.20
                    w_entity = 0.60    # +0.20
                    logger.debug(f"âš ï¸ ä½ç›¸ä¼¼åº¦æ£€æµ‹({base_score:.3f})ï¼šå¢å¼ºå®ä½“æƒé‡")
                
                # å®ä½“åŒ¹é…æƒ…å†µåŠ¨æ€è°ƒæ•´
                if entity_match_score > 1.3:
                    # å®ä½“åŒ¹é…ä¼˜ç§€ï¼šè¿›ä¸€æ­¥æå‡å®ä½“æƒé‡
                    w_entity = min(w_entity + 0.10, 0.70)  # æœ€é«˜ä¸è¶…è¿‡70%
                    w_semantic = max(w_semantic - 0.10, 0.20)
                    logger.debug(f"âœ¨ å®ä½“åŒ¹é…ä¼˜ç§€({entity_match_score:.2f})ï¼šè¿›ä¸€æ­¥æå‡å®ä½“æƒé‡")
                
                elif entity_match_score < 0.5:
                    # å®ä½“åŒ¹é…å·®ï¼šé™ä½å®ä½“æƒé‡ï¼Œæå‡è¯­ä¹‰æƒé‡
                    w_entity = max(w_entity - 0.15, 0.15)
                    w_semantic = min(w_semantic + 0.15, 0.75)
                    logger.debug(f"ğŸ”» å®ä½“åŒ¹é…ä¸ä½³({entity_match_score:.2f})ï¼šé™ä½å®ä½“æƒé‡")
                
                # è®¡ç®—æœ€ç»ˆå¾—åˆ†ï¼ˆç¡®ä¿æƒé‡å’Œä¸º1.0ï¼‰
                total_weight = w_semantic + w_temporal + w_entity
                semantic_weight = (base_score * w_semantic) / total_weight
                temporal_weight = (chapter_importance * w_temporal) / total_weight
                entity_weight = (entity_match_score * w_entity) / total_weight
                
                final_score = semantic_weight + temporal_weight + entity_weight
                
                # åº”ç”¨æ—¶é—´è¡°å‡
                recency_bias = self._calculate_recency_bias(
                    chapter_num, total_chapters, recency_bias_weight
                )
                final_score = final_score * recency_bias
            
            candidates.append({
                'content': doc,
                'metadata': metadata,
                'score': final_score,
                'base_score': base_score,
                'entity_match_score': entity_match_score,  # æ–°å¢ï¼šå®ä½“åŒ¹é…åˆ†æ•°
                'rank': i + 1,
                'query_type': query_type.value
            })
        
        # æ¼”å˜èŠ‚ç‚¹ä¼˜å…ˆrerankï¼šæå‡æ¼”å˜ç« èŠ‚çš„æƒé‡
        if graph and query_entities and len(query_entities) >= 2:
            for candidate in candidates:
                chapter_num = candidate['metadata'].get('chapter_num')
                if chapter_num and self._is_relation_evolution_chapter(graph, chapter_num, query_entities):
                    candidate['score'] *= 1.5  # æ¼”å˜èŠ‚ç‚¹æƒé‡æå‡50%
                    logger.info(f"ğŸ”„ æ£€æµ‹åˆ°å…³ç³»æ¼”å˜ç« èŠ‚{chapter_num}ï¼Œæå‡æƒé‡")
        
        # æ’åº
        candidates.sort(key=lambda x: -x['score'])
        
        # åˆ†æç±»æŸ¥è¯¢ï¼šåˆå¹¶ç›¸é‚»å—
        if query_type == QueryType.ANALYSIS:
            candidates = self._merge_adjacent_chunks(candidates)
        
        # è¿”å›Top-K
        reranked = candidates[:top_k]
        logger.info(f"âœ… Rerankå®Œæˆ ({query_type.value}): è¿”å› {len(reranked)} ä¸ªç»“æœ")
        
        # ğŸ“Š è®°å½•æƒé‡ä½¿ç”¨æƒ…å†µï¼ˆä»…è®°å½•å‰5ä¸ªå€™é€‰ï¼‰
        if len(candidates) > 0:
            logger.info(f"ğŸ“Š Top-5å€™é€‰æƒé‡åˆ†å¸ƒ:")
            for idx, cand in enumerate(candidates[:5]):
                recency_info = ""
                if recency_bias_weight > 0:
                    ch_num = cand['metadata'].get('chapter_num', 0)
                    if ch_num and total_chapters:
                        bias = self._calculate_recency_bias(ch_num, total_chapters, recency_bias_weight)
                        recency_info = f" | æ—¶é—´:{bias:.2f}"
                
                logger.info(
                    f"  [{idx+1}] æœ€ç»ˆå¾—åˆ†:{cand['score']:.3f} | "
                    f"è¯­ä¹‰:{cand['base_score']:.3f} | "
                    f"å®ä½“:{cand.get('entity_match_score', 1.0):.2f} | "
                    f"ç« èŠ‚:{cand['metadata'].get('chapter_num', '?')}{recency_info}"
                )
        
        # è¯¦ç»†æ—¥å¿—
        if query_id:
            # ä¸ºæ—¥å¿—ç»“æœæ·»åŠ å®ä½“åŒ¹é…ä¿¡æ¯
            reranked_with_entity_info = []
            for result in reranked:
                result_copy = result.copy()
                result_copy['entity_match'] = f"{result.get('entity_match_score', 1.0):.2f}"
                reranked_with_entity_info.append(result_copy)
            
            trace_logger.trace_rerank(
                query_id=query_id,
                query=query,
                candidates_count=len(candidates),
                reranked_results=reranked_with_entity_info,
                top_k=top_k
            )
            
            # é¢å¤–è®°å½•å®ä½“åŒ¹é…è¯¦æƒ…
            if query_entities:
                trace_logger.trace_step(
                    query_id=query_id,
                    step_name="å®ä½“åŒ¹é…åˆ†æ",
                    emoji="ğŸ¯",
                    input_data={
                        "æŸ¥è¯¢å®ä½“": query_entities,
                        "å€™é€‰æ–‡æ¡£æ•°": len(candidates)
                    },
                    output_data={
                        "Top-10å®ä½“åŒ¹é…æƒ…å†µ": [
                            {
                                "æ’å": i+1,
                                "ç« èŠ‚": f"ç¬¬{r.get('metadata', {}).get('chapter_num', '?')}ç« ",
                                "å®ä½“åŒ¹é…åˆ†": f"{r.get('entity_match_score', 1.0):.2f}",
                                "è¯­ä¹‰ç›¸ä¼¼åº¦": f"{r.get('base_score', 0):.2f}",
                                "æœ€ç»ˆå¾—åˆ†": f"{r.get('score', 0):.2f}",
                                "åŒ¹é…çš„å®ä½“": [e for e in query_entities if e in r.get('content', '')]
                            }
                            for i, r in enumerate(reranked[:10])
                        ]
                    },
                    status="success"
                )
        
        return reranked
    
    def _calculate_quote_boost(self, text: str) -> float:
        """
        è®¡ç®—å¼•å·å†…å®¹çš„æƒé‡åŠ æˆ
        
        å¯¹è¯ç±»æŸ¥è¯¢ä¼˜å…ˆå±•ç¤ºåŒ…å«å¯¹è¯çš„å†…å®¹
        
        Args:
            text: æ–‡æœ¬å†…å®¹
        
        Returns:
            float: æƒé‡åŠ æˆç³»æ•° (1.0-1.5)
        """
        # ç»Ÿè®¡å¼•å·æ•°é‡ï¼ˆä¸­æ–‡å¼•å·å’Œè‹±æ–‡å¼•å·ï¼‰
        quote_count = (
            text.count('"') + text.count('"') + 
            text.count("'") + text.count("'") +
            text.count('"') // 2  # è‹±æ–‡åŒå¼•å·æˆå¯¹
        )
        
        # è®¡ç®—å¼•å·å æ¯”
        if len(text) > 0:
            quote_density = min(quote_count / (len(text) / 100), 1.0)  # æ ‡å‡†åŒ–
            boost = 1.0 + (quote_density * 0.5)  # æœ€å¤šå¢åŠ 50%æƒé‡
            return boost
        
        return 1.0
    
    def _merge_adjacent_chunks(self, candidates: List[Dict]) -> List[Dict]:
        """
        åˆå¹¶ç›¸é‚»çš„æ–‡æœ¬å—ï¼ˆåˆ†æç±»æŸ¥è¯¢ï¼‰
        
        å°†åŒä¸€ç« èŠ‚çš„ç›¸é‚»å—åˆå¹¶ï¼Œæä¾›æ›´å®Œæ•´çš„ä¸Šä¸‹æ–‡
        
        Args:
            candidates: å€™é€‰æ–‡æ¡£åˆ—è¡¨
        
        Returns:
            List[Dict]: åˆå¹¶åçš„å€™é€‰åˆ—è¡¨
        """
        if not candidates:
            return candidates
        
        merged = []
        skip_indices = set()
        
        for i, current in enumerate(candidates):
            if i in skip_indices:
                continue
            
            current_chapter = current['metadata'].get('chapter_num')
            current_block = current['metadata'].get('block_num')
            merged_content = current['content']
            merged_score = current['score']
            
            # æŸ¥æ‰¾åç»­ç›¸é‚»å—
            for j in range(i + 1, min(i + 3, len(candidates))):  # æœ€å¤šå‘åçœ‹2ä¸ªå—
                next_candidate = candidates[j]
                next_chapter = next_candidate['metadata'].get('chapter_num')
                next_block = next_candidate['metadata'].get('block_num')
                
                # åŒä¸€ç« èŠ‚ä¸”å—å·ç›¸é‚»
                if (current_chapter == next_chapter and 
                    current_block is not None and next_block is not None and
                    next_block == current_block + 1):
                    
                    merged_content += "\n" + next_candidate['content']
                    merged_score = (merged_score + next_candidate['score']) / 2  # å¹³å‡åˆ†æ•°
                    skip_indices.add(j)
                    current_block = next_block  # æ›´æ–°å½“å‰å—å·
            
            # æ·»åŠ åˆå¹¶åçš„å—
            merged.append({
                'content': merged_content,
                'metadata': current['metadata'],
                'score': merged_score,
                'base_score': current.get('base_score'),
                'rank': current.get('rank'),
                'query_type': current.get('query_type'),
                'is_merged': len(merged_content) > len(current['content'])
            })
        
        return merged
    
    def build_prompt(
        self,
        db: Session,
        novel_id: int,
        query: str,
        context_chunks: List[Dict],
        max_chunks: int = 10
    ) -> str:
        """
        æ„å»ºRAG Prompt
        
        Args:
            db: æ•°æ®åº“ä¼šè¯
            novel_id: å°è¯´ID
            query: æŸ¥è¯¢æ–‡æœ¬
            context_chunks: ä¸Šä¸‹æ–‡å—åˆ—è¡¨
            max_chunks: æœ€å¤§ä½¿ç”¨çš„ä¸Šä¸‹æ–‡å—æ•°é‡ï¼ˆé»˜è®¤10ï¼‰
        
        Returns:
            str: æ„å»ºå¥½çš„Prompt
        """
        # è·å–å°è¯´ä¿¡æ¯
        novel = db.query(Novel).filter(Novel.id == novel_id).first()
        novel_title = novel.title if novel else "æœªçŸ¥"
        novel_author = novel.author if novel and novel.author else "æœªçŸ¥"
        
        # é™åˆ¶ä¸Šä¸‹æ–‡å—æ•°é‡
        limited_chunks = context_chunks[:max_chunks]
        
        # æ„å»ºä¸Šä¸‹æ–‡
        context_parts = []
        for i, chunk in enumerate(limited_chunks, 1):
            metadata = chunk['metadata']
            chapter_num = metadata.get('chapter_num', '?')
            chapter_title = metadata.get('chapter_title', '')
            content = chunk['content']
            
            context_parts.append(
                f"[ç‰‡æ®µ{i} - ç¬¬{chapter_num}ç«  {chapter_title}]\n{content}"
            )
        
        context_text = "\n\n".join(context_parts)
        
        # æ„å»ºå®Œæ•´Prompt
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å°è¯´é˜…è¯»åŠ©æ‰‹ã€‚è¯·åŸºäºä»¥ä¸‹å°è¯´å†…å®¹å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

**å°è¯´ä¿¡æ¯**
- æ ‡é¢˜: {novel_title}
- ä½œè€…: {novel_author}

**ç›¸å…³å†…å®¹**
{context_text}

**ç”¨æˆ·é—®é¢˜**
{query}

**å›ç­”è¦æ±‚**
1. åŸºäºæä¾›çš„å°è¯´å†…å®¹å›ç­”ï¼Œä¸è¦ç¼–é€ 
2. å¦‚æœå†…å®¹ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·æ˜ç¡®è¯´æ˜
3. å¼•ç”¨æ—¶è¯·æ ‡æ³¨ç« èŠ‚å·
4. å›ç­”è¦å‡†ç¡®ã€å®Œæ•´ã€æœ‰æ¡ç†

**ä½ çš„å›ç­”**:"""
        
        return prompt
    
    def generate_answer(
        self,
        prompt: str,
        model: str = "glm-4",
        stream: bool = False
    ):
        """
        ç”Ÿæˆç­”æ¡ˆ
        
        Args:
            prompt: å®Œæ•´çš„Prompt
            model: ä½¿ç”¨çš„æ¨¡å‹
            stream: æ˜¯å¦æµå¼è¾“å‡º
        
        Returns:
            str | Generator: ç­”æ¡ˆæ–‡æœ¬æˆ–ç”Ÿæˆå™¨
        """
        try:
            messages = [{"role": "user", "content": prompt}]
            
            if stream:
                # æµå¼ç”Ÿæˆ
                for chunk in self.zhipu_client.chat_completion_stream(
                    messages=messages,
                    model=model
                ):
                    if chunk.get("content"):
                        yield chunk["content"]
            else:
                # éæµå¼ç”Ÿæˆ
                response = self.zhipu_client.chat_completion(
                    messages=messages,
                    model=model
                )
                return response
        except Exception as e:
            logger.error(f"âŒ ç”Ÿæˆç­”æ¡ˆå¤±è´¥: {e}")
            raise
    
    def generate_answer_with_stats(
        self,
        prompt: str,
        model: str = "glm-4",
        stream: bool = False
    ):
        """
        ç”Ÿæˆç­”æ¡ˆï¼ˆå¸¦Tokenç»Ÿè®¡ï¼‰
        
        æ”¯æŒthinkingæ¨¡å¼çš„æ¨¡å‹ä¼šè‡ªåŠ¨è¿”å›reasoning_contentï¼ˆæ€è€ƒè¿‡ç¨‹ï¼‰
        
        Args:
            prompt: å®Œæ•´çš„Prompt
            model: ä½¿ç”¨çš„æ¨¡å‹
            stream: æ˜¯å¦æµå¼è¾“å‡º
        
        Returns:
            Dict | Generator[Dict]: åŒ…å«contentã€reasoning_contentå’Œusageçš„å­—å…¸æˆ–ç”Ÿæˆå™¨
        """
        try:
            messages = [{"role": "user", "content": prompt}]
            
            if stream:
                # æµå¼ç”Ÿæˆï¼ˆè¿”å›å®Œæ•´çš„chunkæ•°æ®ï¼ŒåŒ…å«contentã€reasoning_contentå’Œusageï¼‰
                for chunk_data in self.zhipu_client.chat_completion_stream(
                    messages=messages,
                    model=model
                ):
                    # è¿”å›å®Œæ•´çš„chunk_dataï¼ŒæŸäº›æ¨¡å‹ä¼šåŒ…å«reasoning_content
                    yield chunk_data
            else:
                # éæµå¼ç”Ÿæˆ
                response = self.zhipu_client.chat_completion(
                    messages=messages,
                    model=model
                )
                
                logger.info(f"âœ… ç­”æ¡ˆç”Ÿæˆå®Œæˆ")
                return response.get("content", "")
                
        except Exception as e:
            logger.error(f"âŒ ç­”æ¡ˆç”Ÿæˆå¤±è´¥: {e}")
            if stream:
                yield "æŠ±æ­‰ï¼Œç”Ÿæˆç­”æ¡ˆæ—¶å‡ºç°é”™è¯¯ã€‚"
            else:
                return "æŠ±æ­‰ï¼Œç”Ÿæˆç­”æ¡ˆæ—¶å‡ºç°é”™è¯¯ã€‚"
    
    def query(
        self,
        db: Session,
        novel_id: int,
        query: str,
        model: str = "glm-4",
        enable_query_rewrite: bool = True,
        query_id: Optional[int] = None,
        recency_bias_weight: float = 0.15
    ) -> Tuple[str, List[Citation], Dict, Optional[str]]:
        """
        å®Œæ•´RAGæŸ¥è¯¢æµç¨‹ï¼ˆå«æŸ¥è¯¢ä¼˜åŒ–ï¼‰
        
        Args:
            db: æ•°æ®åº“ä¼šè¯
            novel_id: å°è¯´ID
            query: æŸ¥è¯¢æ–‡æœ¬
            model: ä½¿ç”¨çš„æ¨¡å‹
            enable_query_rewrite: æ˜¯å¦å¯ç”¨æŸ¥è¯¢æ”¹å†™
            query_id: æŸ¥è¯¢IDï¼ˆç”¨äºæ—¥å¿—è®°å½•ï¼‰
        
        Returns:
            Tuple[str, List[Citation], Dict, Optional[str]]: (ç­”æ¡ˆ, å¼•ç”¨åˆ—è¡¨, ç»Ÿè®¡ä¿¡æ¯, æ”¹å†™åçš„æŸ¥è¯¢)
        """
        logger.info(f"ğŸ“ å¼€å§‹RAGæŸ¥è¯¢: {query}")
        
        # 0. æŸ¥è¯¢æ”¹å†™ï¼ˆå¯é€‰ï¼‰
        rewrite_result = self.query_rewriter.rewrite_query(
            query, 
            enable=enable_query_rewrite,
            query_id=query_id
        )
        query_for_retrieval = rewrite_result["rewritten"]
        query_type = rewrite_result.get("query_type")
        rewritten_query = query_for_retrieval if rewrite_result["rewrite_applied"] else None
        
        # 1. æŸ¥è¯¢å‘é‡åŒ–ï¼ˆä½¿ç”¨æ”¹å†™åçš„æŸ¥è¯¢ï¼‰
        query_embedding = self.query_embedding(query_for_retrieval, query_id=query_id)
        
        # 2. è¯­ä¹‰æ£€ç´¢
        vector_results = self.vector_search(novel_id, query_embedding, query_id=query_id)
        
        # 3. å…³é”®è¯æ£€ç´¢ï¼ˆå¯é€‰ï¼‰
        keyword_results = self.keyword_search(db, novel_id, query_for_retrieval)
        
        # 4. æ··åˆRerank
        reranked_chunks = self.rerank(
            query_for_retrieval, 
            vector_results, 
            keyword_results,
            novel_id=novel_id,
            db=db,
            query_id=query_id,
            recency_bias_weight=recency_bias_weight
        )
        
        if not reranked_chunks:
            logger.warning("âš ï¸ æœªæ‰¾åˆ°ç›¸å…³å†…å®¹")
            return "æŠ±æ­‰ï¼Œåœ¨å°è¯´ä¸­æœªæ‰¾åˆ°ç›¸å…³å†…å®¹ã€‚", [], {}, rewritten_query
        
        # 5. æ„å»ºè‡ªé€‚åº”Promptï¼ˆä½¿ç”¨åŸå§‹æŸ¥è¯¢ï¼‰
        prompt = self.prompt_builder.build_prompt(
            db, novel_id, query, reranked_chunks,
            query_type=QueryType(query_type) if query_type else None,
            query_id=query_id
        )
        
        # 6. ç”Ÿæˆç­”æ¡ˆ
        answer = self.generate_answer(prompt, model, stream=False)
        
        # 7. æ„å»ºå¼•ç”¨åˆ—è¡¨
        citations = []
        
        # è¿”å›å‰10æ¡å¼•ç”¨ï¼ˆæˆ–æ‰€æœ‰chunkï¼Œå–è¾ƒå°å€¼ï¼‰
        # ä¸è¿›è¡Œç« èŠ‚å»é‡ï¼Œå› ä¸ºåŒä¸€ç« èŠ‚å¯èƒ½æœ‰å¤šä¸ªç›¸å…³ç‰‡æ®µ
        max_citations = min(10, len(reranked_chunks))
        
        for chunk in reranked_chunks[:max_citations]:
            metadata = chunk['metadata']
            chapter_num = metadata.get('chapter_num')
            
            citations.append(Citation(
                chapter_num=chapter_num,
                chapter_title=metadata.get('chapter_title'),
                text=chunk['content'][:200] + "...",  # æˆªæ–­æ˜¾ç¤º
                score=chunk.get('score')
            ))
        
        # ç»Ÿè®¡ä¿¡æ¯
        stats = {
            'retrieved_chunks': len(vector_results.get('ids', [[]])[0]),
            'reranked_chunks': len(reranked_chunks),
            'citations': len(citations),
            'query_rewrite_applied': rewrite_result["rewrite_applied"]
        }
        
        logger.info(f"âœ… RAGæŸ¥è¯¢å®Œæˆ: {len(citations)} æ¡å¼•ç”¨")
        
        return answer, citations, stats, rewritten_query
    
    def _is_relationship_query(self, query: str) -> bool:
        """
        åˆ¤æ–­æ˜¯å¦ä¸ºå…³ç³»æŸ¥è¯¢
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
        
        Returns:
            bool: æ˜¯å¦ä¸ºå…³ç³»æŸ¥è¯¢
        """
        relation_keywords = ['å…³ç³»', 'ä»€ä¹ˆæ ·', 'å¦‚ä½•', 'æ˜¯ä¸æ˜¯', 'å˜åŒ–', 'æ¼”å˜', 'å¯¹å¾…', 'çœ‹å¾…']
        return any(kw in query for kw in relation_keywords)
    
    def _is_relation_evolution_chapter(
        self,
        graph,
        chapter_num: int,
        query_entities: List[str]
    ) -> bool:
        """
        æ£€æŸ¥ç« èŠ‚æ˜¯å¦ä¸ºæ¼”å˜èŠ‚ç‚¹
        
        Args:
            graph: çŸ¥è¯†å›¾è°±
            chapter_num: ç« èŠ‚å·
            query_entities: æŸ¥è¯¢å®ä½“åˆ—è¡¨
        
        Returns:
            bool: æ˜¯å¦ä¸ºæ¼”å˜èŠ‚ç‚¹
        """
        if len(query_entities) < 2 or not graph:
            return False
        
        try:
            # è·å–ä¸¤å®ä½“é—´çš„å…³ç³»æ¼”å˜
            evolution = self.graph_query.get_relationship_evolution(
                graph, query_entities[0], query_entities[1]
            )
            
            # æ£€æŸ¥è¯¥ç« èŠ‚æ˜¯å¦åœ¨æ¼”å˜åˆ—è¡¨ä¸­
            evolution_chapters = [evt['chapter'] for evt in evolution]
            return chapter_num in evolution_chapters
        except Exception as e:
            logger.debug(f"æ£€æŸ¥æ¼”å˜èŠ‚ç‚¹å¤±è´¥: {e}")
            return False
    
    def _filter_by_entity_attributes(
        self,
        candidates: List[Dict],
        graph,
        query_constraints: Dict
    ) -> List[Dict]:
        """
        åŸºäºå®ä½“å±æ€§è¿‡æ»¤å€™é€‰æ–‡æ¡£
        
        Args:
            candidates: å€™é€‰æ–‡æ¡£åˆ—è¡¨
            graph: çŸ¥è¯†å›¾è°±
            query_constraints: å±æ€§çº¦æŸï¼Œå¦‚{"æ€§åˆ«": "ç”·", "é˜µè¥": "åæ´¾"}
        
        Returns:
            List[Dict]: è¿‡æ»¤åçš„å€™é€‰æ–‡æ¡£
        """
        if not graph or not query_constraints:
            return candidates
        
        filtered = []
        for candidate in candidates:
            entities_in_doc = candidate['metadata'].get('entities', [])
            
            # å¦‚æœæ²¡æœ‰å®ä½“ä¿¡æ¯ï¼Œä¿ç•™ï¼ˆé¿å…è¿‡åº¦è¿‡æ»¤ï¼‰
            if not entities_in_doc:
                filtered.append(candidate)
                continue
            
            # æ£€æŸ¥æ–‡æ¡£ä¸­çš„å®ä½“æ˜¯å¦æ»¡è¶³çº¦æŸ
            for entity in entities_in_doc:
                if entity in graph:
                    attributes = graph.nodes[entity].get('attributes', {})
                    
                    # æ£€æŸ¥æ˜¯å¦æ»¡è¶³æ‰€æœ‰çº¦æŸ
                    if all(attributes.get(k) == v for k, v in query_constraints.items()):
                        filtered.append(candidate)
                        break
        
        return filtered


# å…¨å±€RAGå¼•æ“å®ä¾‹
_rag_engine: Optional[RAGEngine] = None


def get_rag_engine() -> RAGEngine:
    """è·å–å…¨å±€RAGå¼•æ“å®ä¾‹ï¼ˆå•ä¾‹ï¼‰"""
    global _rag_engine
    if _rag_engine is None:
        _rag_engine = RAGEngine()
    return _rag_engine

