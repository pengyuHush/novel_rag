"""
RAGå¼•æ“ - æ£€ç´¢å¢å¼ºç”Ÿæˆ
å®ç°åŸºç¡€RAGæµç¨‹ï¼Œæ”¯æŒæ™ºèƒ½æŸ¥è¯¢è·¯ç”±
"""

import logging
import re
from typing import List, Dict, Optional, Tuple
from sqlalchemy.orm import Session

from app.services.embedding_service import get_embedding_service
from app.services.zhipu_client import get_zhipu_client
from app.services.query_router import query_router, QueryType
from app.models.database import Novel, Chapter
from app.models.schemas import Citation, Confidence

logger = logging.getLogger(__name__)


class RAGEngine:
    """RAGå¼•æ“"""
    
    def __init__(self):
        """åˆå§‹åŒ–RAGå¼•æ“"""
        self.embedding_service = get_embedding_service()
        self.zhipu_client = get_zhipu_client()
        self.top_k_retrieval = 30  # æ£€ç´¢Top-30
        self.top_k_rerank = 10     # RerankåTop-10
        
        # GraphRAGç»„ä»¶
        from app.services.graph.graph_query import GraphQuery
        from app.services.graph.graph_analyzer import GraphAnalyzer
        from app.services.graph.graph_builder import GraphBuilder
        
        self.graph_query = GraphQuery()
        self.graph_analyzer = GraphAnalyzer()
        self.graph_builder = GraphBuilder()
        
        logger.info("âœ… RAGå¼•æ“åˆå§‹åŒ–å®Œæˆï¼ˆå«GraphRAGæ”¯æŒï¼‰")
    
    def query_embedding(self, query: str) -> List[float]:
        """
        æŸ¥è¯¢å‘é‡åŒ–
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
        
        Returns:
            List[float]: æŸ¥è¯¢å‘é‡
        """
        return self.zhipu_client.embed_text(query)
    
    def vector_search(
        self,
        novel_id: int,
        query_embedding: List[float],
        top_k: int = None
    ) -> Dict:
        """
        è¯­ä¹‰æ£€ç´¢
        
        Args:
            novel_id: å°è¯´ID
            query_embedding: æŸ¥è¯¢å‘é‡
            top_k: è¿”å›Top-Kç»“æœ
        
        Returns:
            Dict: æ£€ç´¢ç»“æœ
        """
        top_k = top_k or self.top_k_retrieval
        
        from app.core.chromadb_client import get_chroma_client
        chroma_client = get_chroma_client()
        
        collection_name = f"novel_{novel_id}"
        
        try:
            results = chroma_client.query_documents(
                collection_name=collection_name,
                query_embeddings=[query_embedding],
                n_results=top_k
            )
            
            logger.info(f"âœ… è¯­ä¹‰æ£€ç´¢å®Œæˆ: {len(results.get('ids', [[]])[0])} ä¸ªç»“æœ")
            return results
            
        except Exception as e:
            logger.error(f"âŒ è¯­ä¹‰æ£€ç´¢å¤±è´¥: {e}")
            return {'ids': [[]], 'documents': [[]], 'metadatas': [[]], 'distances': [[]]}
    
    def keyword_search(
        self,
        db: Session,
        novel_id: int,
        query: str,
        top_k: int = 10
    ) -> List[Dict]:
        """
        å…³é”®è¯æ£€ç´¢ï¼ˆç®€å•å®ç°ï¼‰
        
        Args:
            db: æ•°æ®åº“ä¼šè¯
            novel_id: å°è¯´ID
            query: æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›Top-Kç»“æœ
        
        Returns:
            List[Dict]: æ£€ç´¢ç»“æœ
        """
        # ç®€å•çš„å…³é”®è¯åŒ¹é…ï¼ˆå®é™…åº”è¯¥ç”¨å…¨æ–‡ç´¢å¼•ï¼‰
        # è¿™é‡Œåªæ˜¯æ¼”ç¤ºï¼Œç”Ÿäº§ç¯å¢ƒåº”è¯¥ä½¿ç”¨Elasticsearchç­‰
        logger.info(f"ğŸ” å…³é”®è¯æ£€ç´¢: {query}")
        
        # TODO: å®ç°åŸºäºæ•°æ®åº“çš„å…³é”®è¯æ£€ç´¢
        # æš‚æ—¶è¿”å›ç©ºç»“æœ
        return []
    
    def rerank(
        self,
        query: str,
        vector_results: Dict,
        keyword_results: List[Dict] = None,
        top_k: int = None,
        query_type: QueryType = None,
        novel_id: int = None,
        db: Session = None
    ) -> List[Dict]:
        """
        æ··åˆRerankï¼Œæ”¯æŒæŸ¥è¯¢ç±»å‹ç‰¹å®šç­–ç•¥ + GraphRAGå¢å¼º
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            vector_results: å‘é‡æ£€ç´¢ç»“æœ
            keyword_results: å…³é”®è¯æ£€ç´¢ç»“æœ
            top_k: è¿”å›Top-Kç»“æœ
            query_type: æŸ¥è¯¢ç±»å‹ï¼ˆè‡ªåŠ¨æ£€æµ‹æˆ–æ‰‹åŠ¨æŒ‡å®šï¼‰
            novel_id: å°è¯´IDï¼ˆç”¨äºGraphRAGï¼‰
            db: æ•°æ®åº“ä¼šè¯ï¼ˆç”¨äºGraphRAGï¼‰
        
        Returns:
            List[Dict]: Rerankåçš„ç»“æœ
        """
        top_k = top_k or self.top_k_rerank
        
        # è‡ªåŠ¨æ£€æµ‹æŸ¥è¯¢ç±»å‹
        if query_type is None:
            query_type = query_router.classify_query(query)
        
        logger.info(f"ğŸ” æŸ¥è¯¢ç±»å‹: {query_type.value}")
        
        # GraphRAG: åŠ è½½çŸ¥è¯†å›¾è°±ï¼ˆå¦‚æœæä¾›äº†novel_idï¼‰
        graph = None
        chapter_importance_map = {}
        
        if novel_id is not None:
            try:
                graph = self.graph_builder.load_graph(novel_id)
                
                # è®¡ç®—æ‰€æœ‰ç« èŠ‚çš„é‡è¦æ€§è¯„åˆ†ï¼ˆç¼“å­˜ï¼‰
                if graph:
                    # è·å–æ‰€æœ‰ç« èŠ‚å·
                    chapters = set()
                    for node in graph.nodes():
                        first_chapter = graph.nodes[node].get('first_chapter')
                        if first_chapter:
                            chapters.add(first_chapter)
                    
                    # è®¡ç®—æ¯ä¸ªç« èŠ‚çš„é‡è¦æ€§
                    for chapter in chapters:
                        importance = self.graph_analyzer.compute_chapter_importance(graph, chapter)
                        chapter_importance_map[chapter] = importance
                    
                    logger.info(f"âœ… GraphRAG: åŠ è½½å›¾è°±æˆåŠŸï¼Œè®¡ç®—äº†{len(chapter_importance_map)}ä¸ªç« èŠ‚çš„é‡è¦æ€§")
            except Exception as e:
                logger.warning(f"âš ï¸ GraphRAGåŠ è½½å¤±è´¥ï¼ˆç»§ç»­ä½¿ç”¨çº¯å‘é‡æ£€ç´¢ï¼‰: {e}")
        
        # æå–å‘é‡æ£€ç´¢ç»“æœ
        documents = vector_results.get('documents', [[]])[0]
        metadatas = vector_results.get('metadatas', [[]])[0]
        distances = vector_results.get('distances', [[]])[0]
        
        # æ„å»ºå€™é€‰æ–‡æ¡£
        candidates = []
        for i, (doc, metadata, distance) in enumerate(zip(documents, metadatas, distances)):
            base_score = 1 - distance  # è½¬æ¢ä¸ºç›¸ä¼¼åº¦åˆ†æ•°
            
            # GraphRAG: è·å–ç« èŠ‚é‡è¦æ€§ï¼ˆæ—¶åºæƒé‡ï¼‰
            chapter_num = metadata.get('chapter_num')
            chapter_importance = 0.5  # é»˜è®¤ä¸­ç­‰é‡è¦æ€§
            
            if chapter_num and chapter_num in chapter_importance_map:
                chapter_importance = chapter_importance_map[chapter_num]
            
            # åº”ç”¨æŸ¥è¯¢ç±»å‹ç‰¹å®šçš„æƒé‡
            if query_type == QueryType.DIALOGUE:
                # å¯¹è¯ç±»æŸ¥è¯¢ï¼šæå‡åŒ…å«å¼•å·çš„å†…å®¹æƒé‡
                quote_boost = self._calculate_quote_boost(doc)
                final_score = base_score * quote_boost
            elif query_type == QueryType.ANALYSIS:
                # åˆ†æç±»æŸ¥è¯¢ï¼šæå‡é‡è¦ç« èŠ‚æƒé‡ï¼ˆä½¿ç”¨å›¾è°±ç« èŠ‚é‡è¦æ€§ï¼‰
                importance_boost = chapter_importance + 0.5
                final_score = base_score * importance_boost
            else:
                # äº‹å®ç±»æŸ¥è¯¢ï¼šæ··åˆæƒé‡
                # è¯­ä¹‰ç›¸ä¼¼åº¦60% + ç« èŠ‚é‡è¦æ€§15% (GraphRAGå¢å¼º) + å®ä½“åŒ¹é…25%
                semantic_weight = base_score * 0.60
                temporal_weight = chapter_importance * 0.15
                entity_weight = 0.25 if metadata.get('entities') else 0.0
                
                final_score = semantic_weight + temporal_weight + entity_weight
            
            candidates.append({
                'content': doc,
                'metadata': metadata,
                'score': final_score,
                'base_score': base_score,
                'rank': i + 1,
                'query_type': query_type.value
            })
        
        # æ’åº
        candidates.sort(key=lambda x: -x['score'])
        
        # åˆ†æç±»æŸ¥è¯¢ï¼šåˆå¹¶ç›¸é‚»å—
        if query_type == QueryType.ANALYSIS:
            candidates = self._merge_adjacent_chunks(candidates)
        
        # è¿”å›Top-K
        reranked = candidates[:top_k]
        logger.info(f"âœ… Rerankå®Œæˆ ({query_type.value}): è¿”å› {len(reranked)} ä¸ªç»“æœ")
        
        return reranked
    
    def _calculate_quote_boost(self, text: str) -> float:
        """
        è®¡ç®—å¼•å·å†…å®¹çš„æƒé‡åŠ æˆ
        
        å¯¹è¯ç±»æŸ¥è¯¢ä¼˜å…ˆå±•ç¤ºåŒ…å«å¯¹è¯çš„å†…å®¹
        
        Args:
            text: æ–‡æœ¬å†…å®¹
        
        Returns:
            float: æƒé‡åŠ æˆç³»æ•° (1.0-1.5)
        """
        # ç»Ÿè®¡å¼•å·æ•°é‡ï¼ˆä¸­æ–‡å¼•å·å’Œè‹±æ–‡å¼•å·ï¼‰
        quote_count = (
            text.count('"') + text.count('"') + 
            text.count("'") + text.count("'") +
            text.count('"') // 2  # è‹±æ–‡åŒå¼•å·æˆå¯¹
        )
        
        # è®¡ç®—å¼•å·å æ¯”
        if len(text) > 0:
            quote_density = min(quote_count / (len(text) / 100), 1.0)  # æ ‡å‡†åŒ–
            boost = 1.0 + (quote_density * 0.5)  # æœ€å¤šå¢åŠ 50%æƒé‡
            return boost
        
        return 1.0
    
    def _merge_adjacent_chunks(self, candidates: List[Dict]) -> List[Dict]:
        """
        åˆå¹¶ç›¸é‚»çš„æ–‡æœ¬å—ï¼ˆåˆ†æç±»æŸ¥è¯¢ï¼‰
        
        å°†åŒä¸€ç« èŠ‚çš„ç›¸é‚»å—åˆå¹¶ï¼Œæä¾›æ›´å®Œæ•´çš„ä¸Šä¸‹æ–‡
        
        Args:
            candidates: å€™é€‰æ–‡æ¡£åˆ—è¡¨
        
        Returns:
            List[Dict]: åˆå¹¶åçš„å€™é€‰åˆ—è¡¨
        """
        if not candidates:
            return candidates
        
        merged = []
        skip_indices = set()
        
        for i, current in enumerate(candidates):
            if i in skip_indices:
                continue
            
            current_chapter = current['metadata'].get('chapter_num')
            current_block = current['metadata'].get('block_num')
            merged_content = current['content']
            merged_score = current['score']
            
            # æŸ¥æ‰¾åç»­ç›¸é‚»å—
            for j in range(i + 1, min(i + 3, len(candidates))):  # æœ€å¤šå‘åçœ‹2ä¸ªå—
                next_candidate = candidates[j]
                next_chapter = next_candidate['metadata'].get('chapter_num')
                next_block = next_candidate['metadata'].get('block_num')
                
                # åŒä¸€ç« èŠ‚ä¸”å—å·ç›¸é‚»
                if (current_chapter == next_chapter and 
                    current_block is not None and next_block is not None and
                    next_block == current_block + 1):
                    
                    merged_content += "\n" + next_candidate['content']
                    merged_score = (merged_score + next_candidate['score']) / 2  # å¹³å‡åˆ†æ•°
                    skip_indices.add(j)
                    current_block = next_block  # æ›´æ–°å½“å‰å—å·
            
            # æ·»åŠ åˆå¹¶åçš„å—
            merged.append({
                'content': merged_content,
                'metadata': current['metadata'],
                'score': merged_score,
                'base_score': current.get('base_score'),
                'rank': current.get('rank'),
                'query_type': current.get('query_type'),
                'is_merged': len(merged_content) > len(current['content'])
            })
        
        return merged
    
    def build_prompt(
        self,
        db: Session,
        novel_id: int,
        query: str,
        context_chunks: List[Dict],
        max_chunks: int = 10
    ) -> str:
        """
        æ„å»ºRAG Prompt
        
        Args:
            db: æ•°æ®åº“ä¼šè¯
            novel_id: å°è¯´ID
            query: æŸ¥è¯¢æ–‡æœ¬
            context_chunks: ä¸Šä¸‹æ–‡å—åˆ—è¡¨
            max_chunks: æœ€å¤§ä½¿ç”¨çš„ä¸Šä¸‹æ–‡å—æ•°é‡ï¼ˆé»˜è®¤10ï¼‰
        
        Returns:
            str: æ„å»ºå¥½çš„Prompt
        """
        # è·å–å°è¯´ä¿¡æ¯
        novel = db.query(Novel).filter(Novel.id == novel_id).first()
        novel_title = novel.title if novel else "æœªçŸ¥"
        novel_author = novel.author if novel and novel.author else "æœªçŸ¥"
        
        # é™åˆ¶ä¸Šä¸‹æ–‡å—æ•°é‡
        limited_chunks = context_chunks[:max_chunks]
        
        # æ„å»ºä¸Šä¸‹æ–‡
        context_parts = []
        for i, chunk in enumerate(limited_chunks, 1):
            metadata = chunk['metadata']
            chapter_num = metadata.get('chapter_num', '?')
            chapter_title = metadata.get('chapter_title', '')
            content = chunk['content']
            
            context_parts.append(
                f"[ç‰‡æ®µ{i} - ç¬¬{chapter_num}ç«  {chapter_title}]\n{content}"
            )
        
        context_text = "\n\n".join(context_parts)
        
        # æ„å»ºå®Œæ•´Prompt
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å°è¯´é˜…è¯»åŠ©æ‰‹ã€‚è¯·åŸºäºä»¥ä¸‹å°è¯´å†…å®¹å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

**å°è¯´ä¿¡æ¯**
- æ ‡é¢˜: {novel_title}
- ä½œè€…: {novel_author}

**ç›¸å…³å†…å®¹**
{context_text}

**ç”¨æˆ·é—®é¢˜**
{query}

**å›ç­”è¦æ±‚**
1. åŸºäºæä¾›çš„å°è¯´å†…å®¹å›ç­”ï¼Œä¸è¦ç¼–é€ 
2. å¦‚æœå†…å®¹ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·æ˜ç¡®è¯´æ˜
3. å¼•ç”¨æ—¶è¯·æ ‡æ³¨ç« èŠ‚å·
4. å›ç­”è¦å‡†ç¡®ã€å®Œæ•´ã€æœ‰æ¡ç†

**ä½ çš„å›ç­”**:"""
        
        return prompt
    
    def generate_answer(
        self,
        prompt: str,
        model: str = "glm-4",
        stream: bool = False
    ):
        """
        ç”Ÿæˆç­”æ¡ˆ
        
        Args:
            prompt: å®Œæ•´çš„Prompt
            model: ä½¿ç”¨çš„æ¨¡å‹
            stream: æ˜¯å¦æµå¼è¾“å‡º
        
        Returns:
            str | Generator: ç­”æ¡ˆæ–‡æœ¬æˆ–ç”Ÿæˆå™¨
        """
        try:
            messages = [{"role": "user", "content": prompt}]
            
            if stream:
                # æµå¼ç”Ÿæˆ
                for chunk in self.zhipu_client.chat_completion_stream(
                    messages=messages,
                    model=model
                ):
                    if chunk.get("content"):
                        yield chunk["content"]
            else:
                # éæµå¼ç”Ÿæˆ
                response = self.zhipu_client.chat_completion(
                    messages=messages,
                    model=model
                )
                return response
        except Exception as e:
            logger.error(f"âŒ ç”Ÿæˆç­”æ¡ˆå¤±è´¥: {e}")
            raise
    
    def generate_answer_with_stats(
        self,
        prompt: str,
        model: str = "glm-4",
        stream: bool = False
    ):
        """
        ç”Ÿæˆç­”æ¡ˆï¼ˆå¸¦Tokenç»Ÿè®¡ï¼‰
        
        æ”¯æŒthinkingæ¨¡å¼çš„æ¨¡å‹ä¼šè‡ªåŠ¨è¿”å›reasoning_contentï¼ˆæ€è€ƒè¿‡ç¨‹ï¼‰
        
        Args:
            prompt: å®Œæ•´çš„Prompt
            model: ä½¿ç”¨çš„æ¨¡å‹
            stream: æ˜¯å¦æµå¼è¾“å‡º
        
        Returns:
            Dict | Generator[Dict]: åŒ…å«contentã€reasoning_contentå’Œusageçš„å­—å…¸æˆ–ç”Ÿæˆå™¨
        """
        try:
            messages = [{"role": "user", "content": prompt}]
            
            if stream:
                # æµå¼ç”Ÿæˆï¼ˆè¿”å›å®Œæ•´çš„chunkæ•°æ®ï¼ŒåŒ…å«contentã€reasoning_contentå’Œusageï¼‰
                for chunk_data in self.zhipu_client.chat_completion_stream(
                    messages=messages,
                    model=model
                ):
                    # è¿”å›å®Œæ•´çš„chunk_dataï¼ŒæŸäº›æ¨¡å‹ä¼šåŒ…å«reasoning_content
                    yield chunk_data
            else:
                # éæµå¼ç”Ÿæˆ
                response = self.zhipu_client.chat_completion(
                    messages=messages,
                    model=model
                )
                
                logger.info(f"âœ… ç­”æ¡ˆç”Ÿæˆå®Œæˆ")
                return response.get("content", "")
                
        except Exception as e:
            logger.error(f"âŒ ç­”æ¡ˆç”Ÿæˆå¤±è´¥: {e}")
            if stream:
                yield "æŠ±æ­‰ï¼Œç”Ÿæˆç­”æ¡ˆæ—¶å‡ºç°é”™è¯¯ã€‚"
            else:
                return "æŠ±æ­‰ï¼Œç”Ÿæˆç­”æ¡ˆæ—¶å‡ºç°é”™è¯¯ã€‚"
    
    def query(
        self,
        db: Session,
        novel_id: int,
        query: str,
        model: str = "glm-4"
    ) -> Tuple[str, List[Citation], Dict]:
        """
        å®Œæ•´RAGæŸ¥è¯¢æµç¨‹
        
        Args:
            db: æ•°æ®åº“ä¼šè¯
            novel_id: å°è¯´ID
            query: æŸ¥è¯¢æ–‡æœ¬
            model: ä½¿ç”¨çš„æ¨¡å‹
        
        Returns:
            Tuple[str, List[Citation], Dict]: (ç­”æ¡ˆ, å¼•ç”¨åˆ—è¡¨, ç»Ÿè®¡ä¿¡æ¯)
        """
        logger.info(f"ğŸ“ å¼€å§‹RAGæŸ¥è¯¢: {query}")
        
        # 1. æŸ¥è¯¢å‘é‡åŒ–
        query_embedding = self.query_embedding(query)
        
        # 2. è¯­ä¹‰æ£€ç´¢
        vector_results = self.vector_search(novel_id, query_embedding)
        
        # 3. å…³é”®è¯æ£€ç´¢ï¼ˆå¯é€‰ï¼‰
        keyword_results = self.keyword_search(db, novel_id, query)
        
        # 4. æ··åˆRerank
        reranked_chunks = self.rerank(query, vector_results, keyword_results)
        
        if not reranked_chunks:
            logger.warning("âš ï¸ æœªæ‰¾åˆ°ç›¸å…³å†…å®¹")
            return "æŠ±æ­‰ï¼Œåœ¨å°è¯´ä¸­æœªæ‰¾åˆ°ç›¸å…³å†…å®¹ã€‚", [], {}
        
        # 5. æ„å»ºPrompt
        prompt = self.build_prompt(db, novel_id, query, reranked_chunks)
        
        # 6. ç”Ÿæˆç­”æ¡ˆ
        answer = self.generate_answer(prompt, model, stream=False)
        
        # 7. æ„å»ºå¼•ç”¨åˆ—è¡¨
        citations = []
        seen_chapters = set()
        
        for chunk in reranked_chunks:
            metadata = chunk['metadata']
            chapter_num = metadata.get('chapter_num')
            
            # å»é‡ï¼ˆæ¯ç« æœ€å¤šä¸€æ¡å¼•ç”¨ï¼‰
            if chapter_num in seen_chapters:
                continue
            seen_chapters.add(chapter_num)
            
            citations.append(Citation(
                chapter_num=chapter_num,
                chapter_title=metadata.get('chapter_title'),
                text=chunk['content'][:200] + "...",  # æˆªæ–­æ˜¾ç¤º
                score=chunk.get('score')
            ))
        
        # ç»Ÿè®¡ä¿¡æ¯
        stats = {
            'retrieved_chunks': len(vector_results.get('ids', [[]])[0]),
            'reranked_chunks': len(reranked_chunks),
            'citations': len(citations)
        }
        
        logger.info(f"âœ… RAGæŸ¥è¯¢å®Œæˆ: {len(citations)} æ¡å¼•ç”¨")
        
        return answer, citations, stats


# å…¨å±€RAGå¼•æ“å®ä¾‹
_rag_engine: Optional[RAGEngine] = None


def get_rag_engine() -> RAGEngine:
    """è·å–å…¨å±€RAGå¼•æ“å®ä¾‹ï¼ˆå•ä¾‹ï¼‰"""
    global _rag_engine
    if _rag_engine is None:
        _rag_engine = RAGEngine()
    return _rag_engine

