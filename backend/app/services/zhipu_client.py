"""
æ™ºè°±AIå®¢æˆ·ç«¯å°è£…
æ”¯æŒGLM-4ç³»åˆ—æ¨¡å‹å’ŒEmbedding-3å‘é‡åŒ–
"""

import time
import logging
from typing import List, Dict, Optional, Iterator, Any
from zhipuai import ZhipuAI
import asyncio
from functools import wraps

from app.core.config import settings
from app.core.error_handlers import ZhipuAPIError

logger = logging.getLogger(__name__)


def retry_on_failure(max_retries: int = 3, delay: float = 1.0):
    """
    APIè°ƒç”¨å¤±è´¥é‡è¯•è£…é¥°å™¨ï¼ˆæŒ‡æ•°é€€é¿ï¼‰
    
    Args:
        max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
        delay: åˆå§‹å»¶è¿Ÿæ—¶é—´ï¼ˆç§’ï¼‰
    """
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            retries = 0
            current_delay = delay
            
            while retries < max_retries:
                try:
                    return func(*args, **kwargs)
                except Exception as e:
                    retries += 1
                    if retries >= max_retries:
                        logger.error(f"âŒ APIè°ƒç”¨å¤±è´¥ï¼Œå·²è¾¾æœ€å¤§é‡è¯•æ¬¡æ•°: {e}")
                        raise
                    
                    logger.warning(f"âš ï¸ APIè°ƒç”¨å¤±è´¥ï¼Œ{current_delay}ç§’åé‡è¯• ({retries}/{max_retries}): {e}")
                    time.sleep(current_delay)
                    current_delay *= 2  # æŒ‡æ•°é€€é¿
            
            return None
        return wrapper
    return decorator


class ZhipuAIClient:
    """æ™ºè°±AIå®¢æˆ·ç«¯å°è£…ç±»"""
    
    def __init__(self, api_key: Optional[str] = None):
        """
        åˆå§‹åŒ–æ™ºè°±AIå®¢æˆ·ç«¯
        
        Args:
            api_key: APIå¯†é’¥ï¼Œå¦‚ä¸æä¾›åˆ™ä»é…ç½®è¯»å–
        """
        self.api_key = api_key or settings.zhipu_api_key
        
        if not self.api_key or self.api_key == "your_zhipuai_api_key_here":
            raise ValueError("æ™ºè°±AI API Keyæœªé…ç½®ï¼Œè¯·åœ¨.envæ–‡ä»¶ä¸­è®¾ç½®ZHIPU_API_KEY")
        
        self.client = ZhipuAI(api_key=self.api_key)
        self.default_model = settings.zhipu_default_model
        
        logger.info(f"âœ… æ™ºè°±AIå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ (é»˜è®¤æ¨¡å‹: {self.default_model})")
    
    @retry_on_failure(max_retries=3, delay=1.0)
    def embed_texts(
        self,
        texts: List[str],
        model: str = "embedding-3"
    ) -> List[List[float]]:
        """
        æ–‡æœ¬å‘é‡åŒ–ï¼ˆEmbedding-3ï¼‰
        
        Args:
            texts: æ–‡æœ¬åˆ—è¡¨
            model: Embeddingæ¨¡å‹åç§°
        
        Returns:
            List[List[float]]: å‘é‡åˆ—è¡¨
        """
        try:
            logger.debug(f"ğŸ”„ æ­£åœ¨å‘é‡åŒ– {len(texts)} ä¸ªæ–‡æœ¬...")
            
            response = self.client.embeddings.create(
                model=model,
                input=texts
            )
            
            embeddings = [item.embedding for item in response.data]
            
            logger.info(f"âœ… å‘é‡åŒ–å®Œæˆ: {len(embeddings)} ä¸ªå‘é‡")
            return embeddings
            
        except Exception as e:
            logger.error(f"âŒ å‘é‡åŒ–å¤±è´¥: {e}")
            raise ZhipuAPIError(str(e))
    
    def embed_text(self, text: str, model: str = "embedding-3") -> List[float]:
        """
        å•ä¸ªæ–‡æœ¬å‘é‡åŒ–
        
        Args:
            text: æ–‡æœ¬
            model: Embeddingæ¨¡å‹åç§°
        
        Returns:
            List[float]: å‘é‡
        """
        embeddings = self.embed_texts([text], model=model)
        return embeddings[0]
    
    @retry_on_failure(max_retries=3, delay=1.0)
    def chat_completion(
        self,
        messages: List[Dict[str, str]],
        model: Optional[str] = None,
        temperature: float = 0.7,
        top_p: float = 0.9,
        max_tokens: Optional[int] = None,
        **kwargs
    ) -> Dict[str, Any]:
        """
        èŠå¤©è¡¥å…¨ï¼ˆåŒæ­¥ï¼‰
        
        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨ [{"role": "user", "content": "..."}]
            model: æ¨¡å‹åç§°ï¼ˆé»˜è®¤ä½¿ç”¨é…ç½®çš„é»˜è®¤æ¨¡å‹ï¼‰
            temperature: æ¸©åº¦å‚æ•° (0.0-1.0)
            top_p: Top-pé‡‡æ ·å‚æ•°
            max_tokens: æœ€å¤§tokenæ•°
            **kwargs: å…¶ä»–å‚æ•°
        
        Returns:
            Dict: å“åº”æ•°æ®
        """
        model = model or self.default_model
        
        try:
            logger.debug(f"ğŸ”„ è°ƒç”¨ {model}...")
            
            response = self.client.chat.completions.create(
                model=model,
                messages=messages,
                temperature=temperature,
                top_p=top_p,
                max_tokens=max_tokens,
                **kwargs
            )
            
            # æå–å“åº”æ•°æ®
            result = {
                "content": response.choices[0].message.content,
                "model": response.model,
                "usage": {
                    "prompt_tokens": response.usage.prompt_tokens,
                    "completion_tokens": response.usage.completion_tokens,
                    "total_tokens": response.usage.total_tokens
                },
                "finish_reason": response.choices[0].finish_reason
            }
            
            logger.info(f"âœ… {model} è°ƒç”¨æˆåŠŸ (tokens: {result['usage']['total_tokens']})")
            return result
            
        except Exception as e:
            logger.error(f"âŒ {model} è°ƒç”¨å¤±è´¥: {e}")
            raise ZhipuAPIError(str(e))
    
    @retry_on_failure(max_retries=2, delay=1.0)
    def chat_completion_stream(
        self,
        messages: List[Dict[str, str]],
        model: Optional[str] = None,
        temperature: float = 0.7,
        top_p: float = 0.9,
        max_tokens: Optional[int] = None,
        **kwargs
    ) -> Iterator[Dict[str, Any]]:
        """
        èŠå¤©è¡¥å…¨ï¼ˆæµå¼ï¼‰
        
        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            model: æ¨¡å‹åç§°
            temperature: æ¸©åº¦å‚æ•°
            top_p: Top-pé‡‡æ ·å‚æ•°
            max_tokens: æœ€å¤§tokenæ•°
            **kwargs: å…¶ä»–å‚æ•°
        
        Yields:
            Dict: æµå¼å“åº”æ•°æ®
        """
        model = model or self.default_model
        
        try:
            logger.debug(f"ğŸ”„ è°ƒç”¨ {model} (æµå¼)...")
            
            response = self.client.chat.completions.create(
                model=model,
                messages=messages,
                temperature=temperature,
                top_p=top_p,
                max_tokens=max_tokens,
                stream=True,
                **kwargs
            )
            
            total_tokens = 0
            for chunk in response:
                if chunk.choices:
                    delta = chunk.choices[0].delta
                    
                    # æå–å†…å®¹
                    content = delta.content if hasattr(delta, 'content') else ""
                    
                    # æå–Tokenä½¿ç”¨æƒ…å†µï¼ˆå¦‚æœæœ‰ï¼‰
                    usage = None
                    if hasattr(chunk, 'usage') and chunk.usage:
                        usage = {
                            "prompt_tokens": chunk.usage.prompt_tokens,
                            "completion_tokens": chunk.usage.completion_tokens,
                            "total_tokens": chunk.usage.total_tokens
                        }
                        total_tokens = chunk.usage.total_tokens
                    
                    yield {
                        "content": content,
                        "model": model,
                        "usage": usage,
                        "finish_reason": chunk.choices[0].finish_reason if hasattr(chunk.choices[0], 'finish_reason') else None
                    }
            
            logger.info(f"âœ… {model} æµå¼è°ƒç”¨å®Œæˆ (tokens: {total_tokens})")
            
        except Exception as e:
            logger.error(f"âŒ {model} æµå¼è°ƒç”¨å¤±è´¥: {e}")
            raise ZhipuAPIError(str(e))
    
    def get_model_info(self, model: str) -> Dict[str, Any]:
        """
        è·å–æ¨¡å‹ä¿¡æ¯
        
        Args:
            model: æ¨¡å‹åç§°
        
        Returns:
            Dict: æ¨¡å‹ä¿¡æ¯
        """
        # æ™ºè°±AIæ¨¡å‹ä¿¡æ¯ï¼ˆ2024å¹´æ•°æ®ï¼‰
        model_info = {
            "glm-4-flash": {
                "name": "GLM-4-Flash",
                "description": "æœ€å¿«é€Ÿã€æœ€ç»æµçš„æ¨¡å‹",
                "max_tokens": 128000,
                "price_per_million_tokens": {
                    "input": 0.1,
                    "output": 0.1
                }
            },
            "glm-4": {
                "name": "GLM-4",
                "description": "å¹³è¡¡æ€§èƒ½å’Œæˆæœ¬",
                "max_tokens": 128000,
                "price_per_million_tokens": {
                    "input": 5.0,
                    "output": 5.0
                }
            },
            "glm-4-plus": {
                "name": "GLM-4-Plus",
                "description": "æœ€å¼ºæ¨ç†èƒ½åŠ›",
                "max_tokens": 128000,
                "price_per_million_tokens": {
                    "input": 50.0,
                    "output": 50.0
                }
            },
            "glm-4-5": {
                "name": "GLM-4.5",
                "description": "å¢å¼ºç‰ˆæœ¬",
                "max_tokens": 128000,
                "price_per_million_tokens": {
                    "input": 10.0,
                    "output": 10.0
                }
            },
            "glm-4-6": {
                "name": "GLM-4.6",
                "description": "æœ€æ–°ç‰ˆæœ¬",
                "max_tokens": 128000,
                "price_per_million_tokens": {
                    "input": 15.0,
                    "output": 15.0
                }
            },
            "embedding-3": {
                "name": "Embedding-3",
                "description": "æ–‡æœ¬å‘é‡åŒ–æ¨¡å‹",
                "dimensions": 1024,
                "price_per_million_tokens": {
                    "input": 0.5,
                    "output": 0
                }
            }
        }
        
        return model_info.get(model, {
            "name": model,
            "description": "æœªçŸ¥æ¨¡å‹",
            "max_tokens": 128000
        })
    
    def estimate_cost(
        self,
        model: str,
        prompt_tokens: int,
        completion_tokens: int = 0
    ) -> float:
        """
        ä¼°ç®—APIè°ƒç”¨æˆæœ¬ï¼ˆäººæ°‘å¸ï¼‰
        
        Args:
            model: æ¨¡å‹åç§°
            prompt_tokens: è¾“å…¥tokenæ•°
            completion_tokens: è¾“å‡ºtokenæ•°
        
        Returns:
            float: ä¼°ç®—æˆæœ¬ï¼ˆå…ƒï¼‰
        """
        model_info = self.get_model_info(model)
        pricing = model_info.get("price_per_million_tokens", {"input": 0, "output": 0})
        
        input_cost = (prompt_tokens / 1_000_000) * pricing["input"]
        output_cost = (completion_tokens / 1_000_000) * pricing["output"]
        
        return input_cost + output_cost


# å…¨å±€æ™ºè°±AIå®¢æˆ·ç«¯å®ä¾‹
_zhipu_client: Optional[ZhipuAIClient] = None


def get_zhipu_client() -> ZhipuAIClient:
    """è·å–å…¨å±€æ™ºè°±AIå®¢æˆ·ç«¯å®ä¾‹ï¼ˆå•ä¾‹ï¼‰"""
    global _zhipu_client
    if _zhipu_client is None:
        _zhipu_client = ZhipuAIClient()
    return _zhipu_client

