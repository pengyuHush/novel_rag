"""
æ™ºè°±AIå®¢æˆ·ç«¯å°è£…
æ”¯æŒGLM-4ç³»åˆ—æ¨¡å‹å’ŒEmbedding-3å‘é‡åŒ–
"""

import time
import logging
from typing import List, Dict, Optional, Iterator, Any
from zhipuai import ZhipuAI
import asyncio
from functools import wraps

from app.core.config import settings
from app.core.error_handlers import ZhipuAPIError

logger = logging.getLogger(__name__)


def retry_on_failure(max_retries: int = 3, delay: float = 2.0):
    """
    APIè°ƒç”¨å¤±è´¥é‡è¯•è£…é¥°å™¨ï¼ˆæŒ‡æ•°é€€é¿ï¼‰
    
    Args:
        max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
        delay: åˆå§‹å»¶è¿Ÿæ—¶é—´ï¼ˆç§’ï¼‰
    """
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            retries = 0
            current_delay = delay
            
            while retries < max_retries:
                try:
                    return func(*args, **kwargs)
                except Exception as e:
                    retries += 1
                    if retries >= max_retries:
                        logger.error(f"âŒ APIè°ƒç”¨å¤±è´¥ï¼Œå·²è¾¾æœ€å¤§é‡è¯•æ¬¡æ•°: {e}")
                        raise
                    
                    # 429é”™è¯¯ï¼ˆå¹¶å‘é™åˆ¶ï¼‰æ—¶å¢åŠ ç­‰å¾…æ—¶é—´
                    if "429" in str(e) or "1302" in str(e):
                        current_delay = max(current_delay, 5.0)  # è‡³å°‘ç­‰5ç§’
                        logger.warning(f"âš ï¸ å¹¶å‘é™åˆ¶é”™è¯¯ï¼Œ{current_delay}ç§’åé‡è¯• ({retries}/{max_retries}): {e}")
                    else:
                        logger.warning(f"âš ï¸ APIè°ƒç”¨å¤±è´¥ï¼Œ{current_delay}ç§’åé‡è¯• ({retries}/{max_retries}): {e}")
                    
                    time.sleep(current_delay)
                    current_delay *= 2  # æŒ‡æ•°é€€é¿
            
            return None
        return wrapper
    return decorator


class ZhipuAIClient:
    """æ™ºè°±AIå®¢æˆ·ç«¯å°è£…ç±»"""
    
    def __init__(self, api_key: Optional[str] = None):
        """
        åˆå§‹åŒ–æ™ºè°±AIå®¢æˆ·ç«¯
        
        Args:
            api_key: APIå¯†é’¥ï¼Œå¦‚ä¸æä¾›åˆ™ä»é…ç½®è¯»å–
        """
        self.api_key = api_key or settings.zhipu_api_key
        
        if not self.api_key or self.api_key == "your_zhipuai_api_key_here":
            raise ValueError("æ™ºè°±AI API Keyæœªé…ç½®ï¼Œè¯·åœ¨.envæ–‡ä»¶ä¸­è®¾ç½®ZHIPU_API_KEY")
        
        self.client = ZhipuAI(api_key=self.api_key)
        self.default_model = settings.zhipu_default_model
        
        logger.info(f"âœ… æ™ºè°±AIå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ (é»˜è®¤æ¨¡å‹: {self.default_model})")
    
    @retry_on_failure(max_retries=3, delay=1.0)
    def embed_texts(
        self,
        texts: List[str],
        model: str = "embedding-3"
    ) -> List[List[float]]:
        """
        æ–‡æœ¬å‘é‡åŒ–ï¼ˆEmbedding-3ï¼‰
        
        Args:
            texts: æ–‡æœ¬åˆ—è¡¨
            model: Embeddingæ¨¡å‹åç§°
        
        Returns:
            List[List[float]]: å‘é‡åˆ—è¡¨
        """
        try:
            logger.debug(f"ğŸ”„ æ­£åœ¨å‘é‡åŒ– {len(texts)} ä¸ªæ–‡æœ¬...")
            
            response = self.client.embeddings.create(
                model=model,
                input=texts
            )
            
            embeddings = [item.embedding for item in response.data]
            
            logger.info(f"âœ… å‘é‡åŒ–å®Œæˆ: {len(embeddings)} ä¸ªå‘é‡")
            return embeddings
            
        except Exception as e:
            logger.error(f"âŒ å‘é‡åŒ–å¤±è´¥: {e}")
            raise ZhipuAPIError(str(e))
    
    def embed_text(self, text: str, model: str = "embedding-3") -> List[float]:
        """
        å•ä¸ªæ–‡æœ¬å‘é‡åŒ–
        
        Args:
            text: æ–‡æœ¬
            model: Embeddingæ¨¡å‹åç§°
        
        Returns:
            List[float]: å‘é‡
        """
        embeddings = self.embed_texts([text], model=model)
        return embeddings[0]
    
    @retry_on_failure(max_retries=3, delay=1.0)
    def chat_completion(
        self,
        messages: List[Dict[str, str]],
        model: Optional[str] = None,
        temperature: float = 0.7,
        top_p: float = 0.9,
        max_tokens: Optional[int] = None,
        **kwargs
    ) -> Dict[str, Any]:
        """
        èŠå¤©è¡¥å…¨ï¼ˆåŒæ­¥ï¼‰
        
        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨ [{"role": "user", "content": "..."}]
            model: æ¨¡å‹åç§°ï¼ˆé»˜è®¤ä½¿ç”¨é…ç½®çš„é»˜è®¤æ¨¡å‹ï¼‰
            temperature: æ¸©åº¦å‚æ•° (0.0-1.0)
            top_p: Top-pé‡‡æ ·å‚æ•°
            max_tokens: æœ€å¤§tokenæ•°
            **kwargs: å…¶ä»–å‚æ•°
        
        Returns:
            Dict: å“åº”æ•°æ®
        """
        model = model or self.default_model
        
        try:
            logger.debug(f"ğŸ”„ è°ƒç”¨ {model}...")
            
            response = self.client.chat.completions.create(
                model=model,
                messages=messages,
                temperature=temperature,
                top_p=top_p,
                max_tokens=max_tokens,
                **kwargs
            )
            
            # æå–å“åº”æ•°æ®
            message = response.choices[0].message
            
            # GLM-4.5-Flashæ€è€ƒæ¨¡å¼ï¼šå†…å®¹å¯èƒ½åœ¨reasoning_contentä¸­
            content = message.content or ""
            if hasattr(message, 'reasoning_content') and message.reasoning_content:
                # å¦‚æœæœ‰æ¨ç†å†…å®¹ä½†æ²¡æœ‰æ™®é€šå†…å®¹ï¼Œä½¿ç”¨æ¨ç†å†…å®¹
                if not content:
                    content = message.reasoning_content
                    logger.debug(f"ä½¿ç”¨reasoning_contentä½œä¸ºå“åº”å†…å®¹ï¼ˆé•¿åº¦: {len(content)}ï¼‰")
            
            result = {
                "content": content,
                "model": response.model,
                "usage": {
                    "prompt_tokens": response.usage.prompt_tokens,
                    "completion_tokens": response.usage.completion_tokens,
                    "total_tokens": response.usage.total_tokens
                },
                "finish_reason": response.choices[0].finish_reason
            }
            
            logger.info(f"âœ… {model} è°ƒç”¨æˆåŠŸ (tokens: {result['usage']['total_tokens']}, å†…å®¹é•¿åº¦: {len(content)})")
            return result
            
        except Exception as e:
            logger.error(f"âŒ {model} è°ƒç”¨å¤±è´¥: {e}")
            raise ZhipuAPIError(str(e))
    
    @retry_on_failure(max_retries=2, delay=1.0)
    def chat_completion_stream(
        self,
        messages: List[Dict[str, str]],
        model: Optional[str] = None,
        temperature: float = 0.7,
        top_p: float = 0.9,
        max_tokens: Optional[int] = None,
        **kwargs
    ) -> Iterator[Dict[str, Any]]:
        """
        èŠå¤©è¡¥å…¨ï¼ˆæµå¼ï¼‰
        
        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            model: æ¨¡å‹åç§°
            temperature: æ¸©åº¦å‚æ•°
            top_p: Top-pé‡‡æ ·å‚æ•°
            max_tokens: æœ€å¤§tokenæ•°
            **kwargs: å…¶ä»–å‚æ•°
        
        Yields:
            Dict: æµå¼å“åº”æ•°æ®
        """
        model = model or self.default_model
        
        try:
            logger.debug(f"ğŸ”„ è°ƒç”¨ {model} (æµå¼)...")
            
            response = self.client.chat.completions.create(
                model=model,
                messages=messages,
                temperature=temperature,
                top_p=top_p,
                max_tokens=max_tokens,
                stream=True,
                **kwargs
            )
            
            total_tokens = 0
            chunk_count = 0
            first_chunk_logged = False
            
            for chunk in response:
                chunk_count += 1
                
                # ğŸ” åªæ‰“å°ç¬¬1ä¸ªchunkçš„è¯¦ç»†ç»“æ„
                if chunk_count == 1:
                    logger.info(f"ğŸ“¦ ç¬¬1ä¸ªChunkç»“æ„: {chunk}")
                    if hasattr(chunk, 'choices') and chunk.choices:
                        logger.info(f"   - delta: {chunk.choices[0].delta}")
                        delta_attrs = [attr for attr in dir(chunk.choices[0].delta) if not attr.startswith('_')]
                        logger.info(f"   - deltaå…¬å…±å±æ€§: {delta_attrs}")
                    first_chunk_logged = True
                
                if chunk.choices:
                    delta = chunk.choices[0].delta
                    choice = chunk.choices[0]
                    
                    # æå–å†…å®¹ï¼ˆå¯èƒ½åœ¨contentå­—æ®µï¼‰
                    content = delta.content if hasattr(delta, 'content') and delta.content else ""
                    
                    # ğŸ¤” æå–thinkingæ¨¡å¼çš„æ¨ç†å†…å®¹
                    reasoning_content = None
                    if hasattr(delta, 'reasoning_content') and delta.reasoning_content:
                        reasoning_content = delta.reasoning_content
                        if chunk_count <= 3:
                            logger.info(f"ğŸ¤” Chunk #{chunk_count} æœ‰reasoning_content (å‰20å­—ç¬¦): {reasoning_content[:20]}...")
                    
                    # æå–Tokenä½¿ç”¨æƒ…å†µï¼ˆå¦‚æœæœ‰ï¼‰
                    usage = None
                    if hasattr(chunk, 'usage') and chunk.usage:
                        usage = {
                            "prompt_tokens": chunk.usage.prompt_tokens,
                            "completion_tokens": chunk.usage.completion_tokens,
                            "total_tokens": chunk.usage.total_tokens
                        }
                        total_tokens = chunk.usage.total_tokens
                        logger.info(f"ğŸ“Š Chunk #{chunk_count} æ”¶åˆ°usage: {usage}")
                    
                    # è·å–finish_reason
                    finish_reason = choice.finish_reason if hasattr(choice, 'finish_reason') else None
                    if finish_reason:
                        logger.info(f"ğŸ Chunk #{chunk_count} finish_reason: {finish_reason}")
                    
                    # åªyieldæœ‰å†…å®¹ã€thinkingå†…å®¹ã€usageæˆ–finish_reasonçš„chunk
                    if content or reasoning_content or usage or finish_reason:
                        yield {
                            "content": content,
                            "reasoning_content": reasoning_content,
                            "model": model,
                            "usage": usage,
                            "finish_reason": finish_reason
                        }
            
            logger.info(f"âœ… {model} æµå¼è°ƒç”¨å®Œæˆ (tokens: {total_tokens})")
            
        except Exception as e:
            logger.error(f"âŒ {model} æµå¼è°ƒç”¨å¤±è´¥: {e}")
            raise ZhipuAPIError(str(e))
    
    def get_model_info(self, model: str) -> Dict[str, Any]:
        """
        è·å–æ¨¡å‹ä¿¡æ¯
        
        Args:
            model: æ¨¡å‹åç§°
        
        Returns:
            Dict: æ¨¡å‹ä¿¡æ¯
        """
        # æ™ºè°±AIæ¨¡å‹ä¿¡æ¯ - åŸºäºå®˜æ–¹æ–‡æ¡£
        # å‚è€ƒ: https://docs.bigmodel.cn/cn/guide/start/model-overview
        model_info = {
            # å…è´¹æ¨¡å‹
            "GLM-4.5-Flash": {
                "name": "GLM-4.5-Flash",
                "description": "å…è´¹æ¨¡å‹ - æœ€æ–°åŸºåº§æ¨¡å‹çš„æ™®æƒ ç‰ˆæœ¬",
                "max_tokens": 128000,
                "max_output": 96000,
                "price_per_million_tokens": {"input": 0.0, "output": 0.0}
            },
            "GLM-4-Flash-250414": {
                "name": "GLM-4-Flash",
                "description": "å…è´¹æ¨¡å‹ - è¶…é•¿ä¸Šä¸‹æ–‡å¤„ç†èƒ½åŠ›ã€å¤šè¯­è¨€æ”¯æŒ",
                "max_tokens": 128000,
                "max_output": 16000,
                "price_per_million_tokens": {"input": 0.0, "output": 0.0}
            },
            # é«˜æ€§ä»·æ¯”æ¨¡å‹
            "GLM-4.5-Air": {
                "name": "GLM-4.5-Air",
                "description": "é«˜æ€§ä»·æ¯” - åœ¨æ¨ç†ã€ç¼–ç å’Œæ™ºèƒ½ä½“ä»»åŠ¡ä¸Šè¡¨ç°å¼ºåŠ²",
                "max_tokens": 128000,
                "max_output": 96000,
                "price_per_million_tokens": {"input": 1.0, "output": 1.0}
            },
            "GLM-4.5-AirX": {
                "name": "GLM-4.5-AirX",
                "description": "é«˜æ€§ä»·æ¯”-æé€Ÿç‰ˆ - æ¨ç†é€Ÿåº¦å¿«ï¼Œä¸”ä»·æ ¼é€‚ä¸­",
                "max_tokens": 128000,
                "max_output": 96000,
                "price_per_million_tokens": {"input": 1.0, "output": 1.0}
            },
            "GLM-4-Air-250414": {
                "name": "GLM-4-Air",
                "description": "é«˜æ€§ä»·æ¯” - å¿«é€Ÿæ‰§è¡Œå¤æ‚ä»»åŠ¡ã€æ“…é•¿å·¥å…·è°ƒç”¨",
                "max_tokens": 128000,
                "max_output": 16000,
                "price_per_million_tokens": {"input": 1.0, "output": 1.0}
            },
            # æé€Ÿæ¨¡å‹
            "GLM-4.5-X": {
                "name": "GLM-4.5-X",
                "description": "è¶…å¼ºæ€§èƒ½-æé€Ÿç‰ˆ - æ¨ç†é€Ÿåº¦æ›´å¿«",
                "max_tokens": 128000,
                "max_output": 96000,
                "price_per_million_tokens": {"input": 5.0, "output": 5.0}
            },
            "GLM-4-AirX": {
                "name": "GLM-4-AirX",
                "description": "æé€Ÿæ¨ç† - è¶…å¿«çš„æ¨ç†é€Ÿåº¦",
                "max_tokens": 8000,
                "max_output": 4000,
                "price_per_million_tokens": {"input": 1.0, "output": 1.0}
            },
            "GLM-4-FlashX-250414": {
                "name": "GLM-4-FlashX",
                "description": "é«˜é€Ÿä½ä»· - Flashå¢å¼ºç‰ˆæœ¬ã€è¶…å¿«æ¨ç†é€Ÿåº¦",
                "max_tokens": 128000,
                "max_output": 16000,
                "price_per_million_tokens": {"input": 0.1, "output": 0.1}
            },
            # é«˜æ€§èƒ½æ¨¡å‹
            "GLM-4.5": {
                "name": "GLM-4.5",
                "description": "è¶…å¼ºæ€§èƒ½ - å¼ºå¤§çš„æ¨ç†èƒ½åŠ›ã€ä»£ç ç”Ÿæˆèƒ½åŠ›",
                "max_tokens": 128000,
                "max_output": 96000,
                "price_per_million_tokens": {"input": 5.0, "output": 5.0}
            },
            "GLM-4-Plus": {
                "name": "GLM-4-Plus",
                "description": "æ€§èƒ½ä¼˜ç§€ - è¯­è¨€ç†è§£ã€é€»è¾‘æ¨ç†ã€æŒ‡ä»¤éµå¾ªæ•ˆæœé¢†å…ˆ",
                "max_tokens": 128000,
                "max_output": 4000,
                "price_per_million_tokens": {"input": 50.0, "output": 50.0}
            },
            "GLM-4.6": {
                "name": "GLM-4.6",
                "description": "é«˜æ™ºèƒ½æ——èˆ° - æ™ºè°±æœ€å¼ºæ€§èƒ½ã€é«˜çº§ç¼–ç èƒ½åŠ›",
                "max_tokens": 200000,
                "max_output": 128000,
                "price_per_million_tokens": {"input": 10.0, "output": 10.0}
            },
            # è¶…é•¿ä¸Šä¸‹æ–‡
            "GLM-4-Long": {
                "name": "GLM-4-Long",
                "description": "è¶…é•¿è¾“å…¥ - æ”¯æŒé«˜è¾¾1Mçš„ä¸Šä¸‹æ–‡é•¿åº¦",
                "max_tokens": 1000000,
                "max_output": 4000,
                "price_per_million_tokens": {"input": 100.0, "output": 100.0}
            },
            # è§†è§‰æ¨¡å‹
            "GLM-4.5V": {
                "name": "GLM-4.5V",
                "description": "æ——èˆ°è§†è§‰æ¨ç†æ¨¡å‹ - è§†é¢‘ã€å›¾ç‰‡ã€å›¾è¡¨è§£æ",
                "max_tokens": 128000,
                "max_output": 96000,
                "price_per_million_tokens": {"input": 10.0, "output": 10.0}
            },
            "GLM-4V": {
                "name": "GLM-4V",
                "description": "è§†è§‰ç†è§£æ¨¡å‹ - å›¾æ–‡æ··åˆç†è§£",
                "max_tokens": 128000,
                "max_output": 4000,
                "price_per_million_tokens": {"input": 10.0, "output": 10.0}
            },
            "embedding-3": {
                "name": "Embedding-3",
                "description": "æ–‡æœ¬å‘é‡åŒ–æ¨¡å‹",
                "dimensions": 1024,
                "price_per_million_tokens": {
                    "input": 0.5,
                    "output": 0
                }
            }
        }
        
        return model_info.get(model, {
            "name": model,
            "description": "æœªçŸ¥æ¨¡å‹",
            "max_tokens": 128000
        })
    
    def estimate_cost(
        self,
        model: str,
        prompt_tokens: int,
        completion_tokens: int = 0
    ) -> float:
        """
        ä¼°ç®—APIè°ƒç”¨æˆæœ¬ï¼ˆäººæ°‘å¸ï¼‰
        
        Args:
            model: æ¨¡å‹åç§°
            prompt_tokens: è¾“å…¥tokenæ•°
            completion_tokens: è¾“å‡ºtokenæ•°
        
        Returns:
            float: ä¼°ç®—æˆæœ¬ï¼ˆå…ƒï¼‰
        """
        model_info = self.get_model_info(model)
        pricing = model_info.get("price_per_million_tokens", {"input": 0, "output": 0})
        
        input_cost = (prompt_tokens / 1_000_000) * pricing["input"]
        output_cost = (completion_tokens / 1_000_000) * pricing["output"]
        
        return input_cost + output_cost


# å…¨å±€æ™ºè°±AIå®¢æˆ·ç«¯å®ä¾‹
_zhipu_client: Optional[ZhipuAIClient] = None


def get_zhipu_client() -> ZhipuAIClient:
    """è·å–å…¨å±€æ™ºè°±AIå®¢æˆ·ç«¯å®ä¾‹ï¼ˆå•ä¾‹ï¼‰"""
    global _zhipu_client
    if _zhipu_client is None:
        _zhipu_client = ZhipuAIClient()
    return _zhipu_client

