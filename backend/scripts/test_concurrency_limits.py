"""
æµ‹è¯•æ™ºè°±AI APIçš„å®é™…å¹¶å‘é™åˆ¶
ç”¨äºç¡®å®šä¸åŒæ¨¡å‹çš„çœŸå®å¹¶å‘ä¸Šé™
"""

import asyncio
import time
import sys
from pathlib import Path
from typing import List, Dict, Tuple

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from zhipuai import ZhipuAI
from app.core.config import settings

# åˆå§‹åŒ–å®¢æˆ·ç«¯
client = ZhipuAI(api_key=settings.zhipu_api_key)


class ConcurrencyTester:
    """å¹¶å‘æµ‹è¯•å™¨"""
    
    def __init__(self, model: str, test_type: str = "chat"):
        self.model = model
        self.test_type = test_type  # "chat" or "embedding"
        self.results = []
    
    async def single_request(self, request_id: int) -> Dict:
        """å‘é€å•ä¸ªè¯·æ±‚"""
        start_time = time.time()
        
        try:
            if self.test_type == "chat":
                response = client.chat.completions.create(
                    model=self.model,
                    messages=[
                        {"role": "user", "content": f"æµ‹è¯•è¯·æ±‚#{request_id}ï¼šè¯·ç”¨ä¸€å¥è¯ä»‹ç»Pythonã€‚"}
                    ],
                    temperature=0.1
                )
                content = response.choices[0].message.content
            else:  # embedding
                response = client.embeddings.create(
                    model=self.model,
                    input=[f"æµ‹è¯•æ–‡æœ¬#{request_id}"]
                )
                content = f"embeddingç»´åº¦: {len(response.data[0].embedding)}"
            
            duration = time.time() - start_time
            
            return {
                'request_id': request_id,
                'status': 'success',
                'duration': duration,
                'content_preview': content[:50] if content else None
            }
        
        except Exception as e:
            duration = time.time() - start_time
            error_msg = str(e)
            
            # åˆ¤æ–­æ˜¯å¦æ˜¯å¹¶å‘é™åˆ¶é”™è¯¯
            is_rate_limit = "429" in error_msg or "1302" in error_msg
            
            return {
                'request_id': request_id,
                'status': 'rate_limit' if is_rate_limit else 'error',
                'duration': duration,
                'error': error_msg[:200]
            }
    
    async def test_concurrency_level(
        self, 
        concurrency: int, 
        num_requests: int = 10
    ) -> Dict:
        """æµ‹è¯•ç‰¹å®šå¹¶å‘çº§åˆ«"""
        print(f"\n{'='*60}")
        print(f"ğŸ§ª æµ‹è¯•å¹¶å‘æ•°: {concurrency} | æ€»è¯·æ±‚æ•°: {num_requests}")
        print(f"{'='*60}")
        
        semaphore = asyncio.Semaphore(concurrency)
        
        async def request_with_limit(req_id: int):
            async with semaphore:
                return await self.single_request(req_id)
        
        start_time = time.time()
        
        # å¹¶å‘å‘é€æ‰€æœ‰è¯·æ±‚
        results = await asyncio.gather(
            *[request_with_limit(i) for i in range(num_requests)],
            return_exceptions=True
        )
        
        total_duration = time.time() - start_time
        
        # ç»Ÿè®¡ç»“æœ
        success_count = sum(1 for r in results if isinstance(r, dict) and r['status'] == 'success')
        rate_limit_count = sum(1 for r in results if isinstance(r, dict) and r['status'] == 'rate_limit')
        error_count = sum(1 for r in results if isinstance(r, dict) and r['status'] == 'error')
        exception_count = sum(1 for r in results if not isinstance(r, dict))
        
        avg_duration = sum(r['duration'] for r in results if isinstance(r, dict)) / len(results)
        
        result = {
            'concurrency': concurrency,
            'num_requests': num_requests,
            'success_count': success_count,
            'rate_limit_count': rate_limit_count,
            'error_count': error_count,
            'exception_count': exception_count,
            'total_duration': total_duration,
            'avg_request_duration': avg_duration,
            'passed': rate_limit_count == 0 and exception_count == 0
        }
        
        # æ‰“å°ç»“æœ
        status_emoji = "âœ…" if result['passed'] else "âŒ"
        print(f"\n{status_emoji} æµ‹è¯•ç»“æœ:")
        print(f"  - æˆåŠŸ: {success_count}/{num_requests}")
        print(f"  - å¹¶å‘é™åˆ¶é”™è¯¯: {rate_limit_count}")
        print(f"  - å…¶ä»–é”™è¯¯: {error_count + exception_count}")
        print(f"  - æ€»è€—æ—¶: {total_duration:.2f}ç§’")
        print(f"  - å¹³å‡è¯·æ±‚è€—æ—¶: {avg_duration:.2f}ç§’")
        
        if rate_limit_count > 0:
            print(f"\nâš ï¸ è§¦å‘å¹¶å‘é™åˆ¶ï¼å¹¶å‘{concurrency}è¿‡é«˜")
            # æ˜¾ç¤ºç¬¬ä¸€ä¸ªé”™è¯¯è¯¦æƒ…
            for r in results:
                if isinstance(r, dict) and r['status'] == 'rate_limit':
                    print(f"  é”™è¯¯è¯¦æƒ…: {r['error'][:150]}...")
                    break
        
        self.results.append(result)
        return result
    
    async def find_max_concurrency(
        self, 
        start: int = 1, 
        end: int = 10, 
        num_requests: int = 15
    ) -> int:
        """äºŒåˆ†æŸ¥æ‰¾æœ€å¤§å®‰å…¨å¹¶å‘æ•°"""
        print(f"\n{'#'*60}")
        print(f"ğŸš€ å¼€å§‹æµ‹è¯•æ¨¡å‹: {self.model} ({self.test_type})")
        print(f"ğŸ“Š æµ‹è¯•èŒƒå›´: {start} - {end}")
        print(f"{'#'*60}")
        
        max_safe_concurrency = 0
        
        # å…ˆæµ‹è¯•ä¸€äº›å…³é”®ç‚¹
        test_levels = [1, 2, 3, 5, 8, 10]
        test_levels = [c for c in test_levels if start <= c <= end]
        
        for concurrency in test_levels:
            result = await self.test_concurrency_level(concurrency, num_requests)
            
            if result['passed']:
                max_safe_concurrency = concurrency
                print(f"âœ… å¹¶å‘{concurrency}é€šè¿‡æµ‹è¯•")
            else:
                print(f"âŒ å¹¶å‘{concurrency}å¤±è´¥ï¼Œåœæ­¢æµ‹è¯•")
                break
            
            # çŸ­æš‚ç­‰å¾…ï¼Œé¿å…å½±å“ä¸‹ä¸€æ¬¡æµ‹è¯•
            await asyncio.sleep(2)
        
        return max_safe_concurrency
    
    def print_summary(self, max_concurrency: int):
        """æ‰“å°æµ‹è¯•æ€»ç»“"""
        print(f"\n{'='*60}")
        print(f"ğŸ“‹ æµ‹è¯•æ€»ç»“ - {self.model}")
        print(f"{'='*60}")
        print(f"ğŸ¯ å»ºè®®çš„æœ€å¤§å¹¶å‘æ•°: {max_concurrency}")
        print(f"âš ï¸ ä¿å®ˆå»ºè®®ï¼ˆç•™20%ä½™é‡ï¼‰: {max(1, int(max_concurrency * 0.8))}")
        print(f"\nè¯¦ç»†ç»“æœ:")
        
        for r in self.results:
            status = "âœ… é€šè¿‡" if r['passed'] else "âŒ å¤±è´¥"
            print(f"  å¹¶å‘{r['concurrency']:2d}: {status} | "
                  f"æˆåŠŸ {r['success_count']}/{r['num_requests']} | "
                  f"é™æµ {r['rate_limit_count']} | "
                  f"è€—æ—¶ {r['total_duration']:.1f}s")
        
        print(f"\n{'='*60}\n")


async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘         æ™ºè°±AI API å¹¶å‘é™åˆ¶æµ‹è¯•å·¥å…·                        â•‘
â•‘                                                            â•‘
â•‘  ç”¨é€”: æµ‹è¯•ä¸åŒæ¨¡å‹çš„å®é™…æœ€å¤§å¹¶å‘æ•°                         â•‘
â•‘  å‚è€ƒ: https://bigmodel.cn/usercenter/proj-mgmt/rate-limits â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)
    
    # æµ‹è¯•é…ç½®
    tests = [
        {
            'model': 'glm-4-flash',
            'test_type': 'chat',
            'description': 'GLM-4-Flash (èŠå¤©æ¨¡å‹)',
            'start': 1,
            'end': 10,
            'requests': 15
        },
        {
            'model': 'embedding-3',
            'test_type': 'embedding',
            'description': 'Embedding-3 (å‘é‡åŒ–æ¨¡å‹)',
            'start': 1,
            'end': 10,
            'requests': 15
        }
    ]
    
    all_results = {}
    
    for test_config in tests:
        print(f"\n\n{'*'*60}")
        print(f"å¼€å§‹æµ‹è¯•: {test_config['description']}")
        print(f"{'*'*60}\n")
        
        tester = ConcurrencyTester(
            model=test_config['model'],
            test_type=test_config['test_type']
        )
        
        max_concurrency = await tester.find_max_concurrency(
            start=test_config['start'],
            end=test_config['end'],
            num_requests=test_config['requests']
        )
        
        tester.print_summary(max_concurrency)
        
        all_results[test_config['model']] = {
            'max_concurrency': max_concurrency,
            'safe_concurrency': max(1, int(max_concurrency * 0.8)),
            'results': tester.results
        }
        
        # ç­‰å¾…ä¸€æ®µæ—¶é—´å†æµ‹è¯•ä¸‹ä¸€ä¸ªæ¨¡å‹
        if test_config != tests[-1]:
            print("â³ ç­‰å¾…5ç§’åæµ‹è¯•ä¸‹ä¸€ä¸ªæ¨¡å‹...\n")
            await asyncio.sleep(5)
    
    # æœ€ç»ˆå»ºè®®
    print(f"\n{'#'*60}")
    print(f"ğŸ‰ æ‰€æœ‰æµ‹è¯•å®Œæˆï¼")
    print(f"{'#'*60}")
    print(f"\nğŸ“ å»ºè®®çš„é…ç½® (backend/app/core/config.py):\n")
    
    for model, result in all_results.items():
        if 'embedding' in model:
            print(f"# {model}")
            print(f"embedding_batch_size = {result['safe_concurrency']}  "
                  f"# å®æµ‹æœ€å¤§{result['max_concurrency']}ï¼Œå»ºè®®{result['safe_concurrency']}")
        elif 'glm-4-flash' in model or 'flash' in model:
            print(f"# {model}")
            print(f"graph_attribute_concurrency = {result['safe_concurrency']}  "
                  f"# å®æµ‹æœ€å¤§{result['max_concurrency']}ï¼Œå»ºè®®{result['safe_concurrency']}")
            print(f"graph_relation_concurrency = {result['safe_concurrency']}  "
                  f"# å®æµ‹æœ€å¤§{result['max_concurrency']}ï¼Œå»ºè®®{result['safe_concurrency']}")
    
    print(f"\n{'#'*60}\n")


if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\n\nâš ï¸ æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\n\nâŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

