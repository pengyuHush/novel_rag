"""
æµ‹è¯•æŒç»­é«˜é¢‘è¯·æ±‚åœºæ™¯ä¸‹çš„å¹¶å‘é™åˆ¶
æ¨¡æ‹Ÿå®é™…é¡¹ç›®ä¸­çš„è¿ç»­æ‰¹æ¬¡å¤„ç†
"""

import asyncio
import time
import sys
from pathlib import Path
from typing import List, Dict

sys.path.insert(0, str(Path(__file__).parent.parent))

from zhipuai import ZhipuAI
from app.core.config import settings

client = ZhipuAI(api_key=settings.zhipu_api_key)


async def test_sustained_load(
    model: str,
    test_type: str,
    concurrency: int,
    num_batches: int = 10,
    requests_per_batch: int = 20
):
    """
    æµ‹è¯•æŒç»­è´Ÿè½½ä¸‹çš„å¹¶å‘æ§åˆ¶
    
    Args:
        model: æ¨¡å‹åç§°
        test_type: "chat" æˆ– "embedding"
        concurrency: å¹¶å‘æ•°
        num_batches: æ‰¹æ¬¡æ•°
        requests_per_batch: æ¯æ‰¹è¯·æ±‚æ•°
    """
    print(f"\n{'='*70}")
    print(f"ğŸ”¥ æŒç»­è´Ÿè½½æµ‹è¯•")
    print(f"{'='*70}")
    print(f"æ¨¡å‹: {model}")
    print(f"å¹¶å‘æ•°: {concurrency}")
    print(f"æ‰¹æ¬¡æ•°: {num_batches}")
    print(f"æ¯æ‰¹è¯·æ±‚æ•°: {requests_per_batch}")
    print(f"æ€»è¯·æ±‚æ•°: {num_batches * requests_per_batch}")
    print(f"{'='*70}\n")
    
    total_success = 0
    total_rate_limit = 0
    total_errors = 0
    
    start_time = time.time()
    
    for batch_idx in range(num_batches):
        print(f"ğŸ“¦ å¤„ç†æ‰¹æ¬¡ {batch_idx + 1}/{num_batches}...")
        
        semaphore = asyncio.Semaphore(concurrency)
        
        async def single_request(req_id: int):
            async with semaphore:
                try:
                    if test_type == "chat":
                        response = await asyncio.to_thread(
                            client.chat.completions.create,
                            model=model,
                            messages=[{"role": "user", "content": f"è¯·è¯´ä¸€ä¸ªå­—ï¼š{req_id}"}],
                            temperature=0.1
                        )
                        return {'status': 'success'}
                    else:  # embedding
                        response = await asyncio.to_thread(
                            client.embeddings.create,
                            model=model,
                            input=[f"æµ‹è¯•æ–‡æœ¬{req_id}"]
                        )
                        return {'status': 'success'}
                
                except Exception as e:
                    error_msg = str(e)
                    if "429" in error_msg or "1302" in error_msg:
                        return {'status': 'rate_limit', 'error': error_msg}
                    else:
                        return {'status': 'error', 'error': error_msg}
        
        # æ‰§è¡Œæœ¬æ‰¹æ¬¡
        batch_start = time.time()
        results = await asyncio.gather(
            *[single_request(batch_idx * requests_per_batch + i) 
              for i in range(requests_per_batch)],
            return_exceptions=True
        )
        batch_duration = time.time() - batch_start
        
        # ç»Ÿè®¡ç»“æœ
        batch_success = sum(1 for r in results if isinstance(r, dict) and r['status'] == 'success')
        batch_rate_limit = sum(1 for r in results if isinstance(r, dict) and r['status'] == 'rate_limit')
        batch_errors = sum(1 for r in results if not isinstance(r, dict) or 
                          (isinstance(r, dict) and r['status'] == 'error'))
        
        total_success += batch_success
        total_rate_limit += batch_rate_limit
        total_errors += batch_errors
        
        status_emoji = "âœ…" if batch_rate_limit == 0 else "âŒ"
        print(f"  {status_emoji} æ‰¹æ¬¡ {batch_idx + 1}: "
              f"æˆåŠŸ {batch_success}/{requests_per_batch} | "
              f"é™æµ {batch_rate_limit} | "
              f"è€—æ—¶ {batch_duration:.2f}s")
        
        if batch_rate_limit > 0:
            print(f"  âš ï¸ è§¦å‘é™æµï¼è¯¦æƒ…: {[r.get('error', '')[:80] for r in results if isinstance(r, dict) and r['status'] == 'rate_limit'][0]}")
        
        # æ‰¹æ¬¡é—´çŸ­æš‚å»¶è¿Ÿï¼ˆæ¨¡æ‹Ÿå®é™…é¡¹ç›®ï¼‰
        if batch_idx < num_batches - 1:
            await asyncio.sleep(0.5)
    
    total_duration = time.time() - start_time
    
    # æœ€ç»ˆç»Ÿè®¡
    print(f"\n{'='*70}")
    print(f"ğŸ“Š æµ‹è¯•å®Œæˆ")
    print(f"{'='*70}")
    print(f"æ€»è¯·æ±‚æ•°: {num_batches * requests_per_batch}")
    print(f"æˆåŠŸ: {total_success} ({total_success/(num_batches*requests_per_batch)*100:.1f}%)")
    print(f"é™æµé”™è¯¯: {total_rate_limit} ({total_rate_limit/(num_batches*requests_per_batch)*100:.1f}%)")
    print(f"å…¶ä»–é”™è¯¯: {total_errors} ({total_errors/(num_batches*requests_per_batch)*100:.1f}%)")
    print(f"æ€»è€—æ—¶: {total_duration:.2f}ç§’")
    print(f"å¹³å‡åå: {(num_batches * requests_per_batch) / total_duration:.2f} è¯·æ±‚/ç§’")
    
    if total_rate_limit == 0:
        print(f"\nâœ… æµ‹è¯•é€šè¿‡ï¼å¹¶å‘{concurrency}åœ¨æŒç»­è´Ÿè½½ä¸‹å®‰å…¨")
    else:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥ï¼å¹¶å‘{concurrency}åœ¨æŒç»­è´Ÿè½½ä¸‹ä¼šè§¦å‘é™æµ")
    
    print(f"{'='*70}\n")
    
    return total_rate_limit == 0


async def find_safe_concurrency(model: str, test_type: str):
    """é€æ­¥æµ‹è¯•æ‰¾å‡ºå®‰å…¨çš„å¹¶å‘æ•°"""
    print(f"\n{'#'*70}")
    print(f"ğŸ¯ å¯»æ‰¾å®‰å…¨å¹¶å‘æ•°: {model} ({test_type})")
    print(f"{'#'*70}\n")
    
    test_levels = [1, 2, 3, 5, 8]
    safe_concurrency = 1
    
    for concurrency in test_levels:
        passed = await test_sustained_load(
            model=model,
            test_type=test_type,
            concurrency=concurrency,
            num_batches=5,  # 5æ‰¹æ¬¡
            requests_per_batch=15  # æ¯æ‰¹15ä¸ª
        )
        
        if passed:
            safe_concurrency = concurrency
            print(f"âœ… å¹¶å‘{concurrency}é€šè¿‡æŒç»­è´Ÿè½½æµ‹è¯•\n")
        else:
            print(f"âŒ å¹¶å‘{concurrency}åœ¨æŒç»­è´Ÿè½½ä¸‹å¤±è´¥\n")
            break
        
        # ç­‰å¾…APIå†·å´
        if concurrency < test_levels[-1]:
            print("â³ ç­‰å¾…5ç§’åç»§ç»­æµ‹è¯•...\n")
            await asyncio.sleep(5)
    
    return safe_concurrency


async def main():
    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘          æ™ºè°±AI æŒç»­è´Ÿè½½å¹¶å‘æµ‹è¯•å·¥å…·                              â•‘
â•‘                                                                    â•‘
â•‘  ç”¨é€”: æµ‹è¯•æŒç»­é«˜é¢‘è¯·æ±‚åœºæ™¯ä¸‹çš„çœŸå®å¹¶å‘é™åˆ¶                       â•‘
â•‘  æ¨¡æ‹Ÿ: å®é™…é¡¹ç›®ä¸­çš„è¿ç»­æ‰¹æ¬¡å¤„ç†                                  â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)
    
    # æµ‹è¯•é…ç½®
    tests = [
        {
            'model': 'glm-4-flash',
            'test_type': 'chat',
            'description': 'GLM-4-Flash (èŠå¤©æ¨¡å‹)'
        },
        {
            'model': 'embedding-3',
            'test_type': 'embedding',
            'description': 'Embedding-3 (å‘é‡åŒ–æ¨¡å‹)'
        }
    ]
    
    results = {}
    
    for test_config in tests:
        safe_concurrency = await find_safe_concurrency(
            model=test_config['model'],
            test_type=test_config['test_type']
        )
        
        results[test_config['model']] = safe_concurrency
        
        print(f"âœ… {test_config['description']}: å®‰å…¨å¹¶å‘æ•° = {safe_concurrency}\n")
        
        # ç­‰å¾…APIå†·å´
        await asyncio.sleep(5)
    
    # æœ€ç»ˆå»ºè®®
    print(f"\n{'#'*70}")
    print(f"ğŸ‰ æ‰€æœ‰æµ‹è¯•å®Œæˆï¼")
    print(f"{'#'*70}\n")
    print(f"ğŸ“ å»ºè®®çš„é…ç½® (æŒç»­è´Ÿè½½åœºæ™¯):\n")
    
    for model, safe_val in results.items():
        if 'embedding' in model:
            print(f"embedding_batch_size = {safe_val}  # {model} æŒç»­è´Ÿè½½å®‰å…¨å€¼")
        elif 'glm-4-flash' in model or 'flash' in model:
            print(f"graph_attribute_concurrency = {safe_val}  # {model} æŒç»­è´Ÿè½½å®‰å…¨å€¼")
            print(f"graph_relation_concurrency = {safe_val}  # {model} æŒç»­è´Ÿè½½å®‰å…¨å€¼")
    
    print(f"\nğŸ’¡ æç¤º: å®é™…é¡¹ç›®å»ºè®®åœ¨å®‰å…¨å€¼åŸºç¡€ä¸Šå†é™ä½20-30%ä»¥ç¡®ä¿ç¨³å®šæ€§")
    print(f"{'#'*70}\n")


if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\n\nâš ï¸ æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\n\nâŒ æµ‹è¯•å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()

