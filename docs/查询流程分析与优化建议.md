# 查询流程分析与优化建议

## 📋 当前查询流程概览

系统采用**混合RAG + GraphRAG + Self-RAG**的多阶段查询流程，共分为5个核心阶段：

```
查询理解 → 检索上下文 → 生成答案 → Self-RAG验证 → 完成汇总
```

---

## 🔍 详细流程分析

### 阶段1: 查询理解（Understanding）

**当前实现：**
- 自动检测查询类型（`QueryRouter.classify_query`）
- 支持三种查询类型：
  - **事实类（FACTUAL）**：询问具体事实、情节细节
  - **对话类（DIALOGUE）**：询问角色对话、台词
  - **分析类（ANALYSIS）**：需要综合分析、推理的问题

**对查询的影响：**
- ✅ **正面**：为后续的检索和重排序提供类型指引
- ⚠️ **局限**：分类基于简单的关键词匹配，可能误判复杂查询

**可配置参数：**
- 无（目前为自动检测）

---

### 阶段2: 检索上下文（Retrieving）

#### 2.1 向量化查询
**实现：**
```python
query_embedding = rag_engine.query_embedding(query)
```

**作用：**
- 将用户查询转换为768维向量（ZhipuAI Embedding-3）
- 为语义相似度搜索做准备

**对查询的影响：**
- ✅ **正面**：捕捉查询的语义信息，支持模糊匹配
- ⚠️ **局限**：单一Embedding模型可能无法完美表达复杂查询

**可配置参数：**
- 无（Embedding模型固定）

---

#### 2.2 向量检索（Vector Search）
**实现：**
```python
vector_results = rag_engine.vector_search(
    novel_id, 
    query_embedding,
    top_k=top_k_retrieval  # 可配置：10-100，默认30
)
```

**作用：**
- 从ChromaDB向量数据库检索Top-K个最相似的文本块
- 基于余弦相似度计算距离

**对查询的影响：**
- ✅ **正面**：
  - 高召回率，不会遗漏语义相关内容
  - 支持跨章节检索
- ⚠️ **局限**：
  - 可能检索到语义相似但内容无关的块
  - 无法理解时间顺序和因果关系

**可配置参数：**
- **`top_k_retrieval`**（10-100）：
  - 数值越大：召回率越高，但噪音增加，计算量增大
  - 推荐：常规30，高精度100

---

#### 2.3 重排序与GraphRAG增强（Rerank）
**实现：**
```python
reranked_chunks = rag_engine.rerank(
    query=query,
    vector_results=vector_results,
    novel_id=novel_id,
    db=db,
    top_k=top_k_rerank  # 可配置：5-30，默认10
)
```

**作用：**
1. **查询类型自适应权重**：
   - **对话类**：提升包含引号内容的块权重（`quote_boost`）
   - **分析类**：提升重要章节的块权重（基于GraphRAG章节重要性）
   - **事实类**：混合语义相似度（60%）+ 章节重要性（15%）+ 实体匹配（25%）

2. **GraphRAG增强**：
   - 加载知识图谱
   - 计算每个章节的重要性（基于PageRank和实体密度）
   - 为检索块添加时序权重

3. **相邻块合并**（仅分析类）：
   - 合并同一章节连续的文本块
   - 提供更完整的上下文

**对查询的影响：**
- ✅ **正面**：
  - 显著提升结果质量，剔除噪音
  - 结合图谱信息，增强因果推理能力
  - 针对不同查询类型优化策略
- ⚠️ **局限**：
  - 章节重要性是静态计算的，无法动态适应查询
  - 实体匹配仅检查metadata，未深度NER匹配

**可配置参数：**
- **`top_k_rerank`**（5-30）：
  - 数值越大：上下文越丰富，但Token成本增加，可能引入冗余
  - 推荐：常规10，高精度30

---

### 阶段3: 生成答案（Generating）

#### 3.1 构建Prompt（Build Prompt）
**实现：**
```python
prompt = rag_engine.build_prompt(
    db, 
    novel_id, 
    query, 
    reranked_chunks,
    max_chunks=max_context_chunks  # 可配置：5-20，默认10
)
```

**作用：**
- 组织小说信息、上下文片段、用户问题
- 添加回答要求（基于内容、引用章节、准确完整）

**Prompt结构：**
```
小说信息（标题、作者）
↓
相关内容（max_chunks个文本块，每块标注章节）
↓
用户问题
↓
回答要求（4条）
```

**对查询的影响：**
- ✅ **正面**：
  - 清晰的结构化Prompt提升LLM理解能力
  - 明确的要求约束LLM行为
- ⚠️ **局限**：
  - 静态模板，未针对查询类型调整Prompt风格
  - 未提供示例（Few-shot）或思维链（CoT）引导

**可配置参数：**
- **`max_context_chunks`**（5-20）：
  - 数值越大：答案更全面，但Token成本显著增加，可能超出上下文窗口
  - 推荐：常规10，高精度20

---

#### 3.2 流式生成答案
**实现：**
```python
for chunk_data in rag_engine.generate_answer_with_stats(prompt, model, stream=True):
    # 流式输出thinking（推理过程）和answer（最终答案）
```

**作用：**
- 调用智谱AI GLM系列模型（glm-4、glm-4-plus等）
- 实时返回思考过程（thinking）和最终答案（answer）
- 支持Token统计

**对查询的影响：**
- ✅ **正面**：
  - 用户体验流畅，实时反馈
  - thinking过程透明化，增强可解释性
- ⚠️ **局限**：
  - LLM可能产生幻觉（hallucination）
  - 未对生成过程进行中间验证

---

### 阶段4: Self-RAG验证（Validating）

**实现：**
多轮验证流程，包含6个子步骤：

#### 4.1 提取断言（Assertion Extraction）
```python
assertions = assertion_extractor.extract_assertions(full_answer)
```
- 从LLM答案中提取可验证的事实陈述
- 例如："张九皋在第41章首次使用灵印" → 可验证的断言

#### 4.2 收集证据（Evidence Collection）
```python
evidence_list = evidence_collector.collect_evidence_for_assertion(
    db, novel_id, assertion, top_k=3
)
```
- 为每个断言在原文中查找支撑证据
- 使用向量检索 + 关键词匹配

#### 4.3 评分证据（Evidence Scoring）
```python
scored = evidence_scorer.score_evidence(
    db, novel_id, evidence, query_context={'assertion': assertion}
)
```
- 评估证据对断言的支持度
- 考虑时序关系、逻辑一致性

#### 4.4 一致性检查（Consistency Check）
```python
temporal_issues = consistency_checker.check_temporal_consistency(assertions, evidence_map)
character_issues = consistency_checker.check_character_consistency(db, novel_id, assertions, evidence_map)
```
- **时序一致性**：检查事件发生顺序
- **角色一致性**：检查角色行为与设定是否矛盾

#### 4.5 检测矛盾（Contradiction Detection）
```python
contradictions = contradiction_detector.detect_contradictions(
    db, novel_id, assertions, evidence_map, consistency_report
)
```
- 识别答案中与原文矛盾的部分
- 标注矛盾类型、章节范围、置信度

#### 4.6 修正答案（Answer Correction）
```python
correction_result = answer_corrector.correct_answer(
    full_answer, contradictions, "high"
)
corrected_answer = correction_result.get('corrected_answer', full_answer)
confidence_level = correction_result.get('final_confidence', 'high')
```
- 根据检测到的矛盾修正答案
- 重新评估置信度

**对查询的影响：**
- ✅ **正面**：
  - 显著降低幻觉问题
  - 提供矛盾检测结果，增强透明度
  - 动态修正错误答案
- ⚠️ **局限**：
  - 计算开销较大（每个断言需要二次检索）
  - 可能过度保守，修正正确答案
  - 未考虑原文可能本身存在矛盾

---

### 阶段5: 完成汇总（Finalizing）

**实现：**
```python
# 构建最终响应，包含：
QueryResponse(
    query_id=query_record.id,
    answer=corrected_answer,           # 修正后的答案
    citations=citations,                # 引用列表
    contradictions=contradictions_list, # 矛盾检测结果
    token_stats=token_stats_obj,        # Token统计
    confidence=confidence_level,        # 置信度
    response_time=response_time         # 响应时间
)
```

**作用：**
- 整合所有中间结果
- 记录查询历史到数据库
- 更新Token统计到novel表

---

## 📊 参数影响矩阵

| 参数 | 范围 | 默认 | 影响召回率 | 影响精度 | 影响成本 | 影响速度 |
|------|------|------|-----------|---------|---------|---------|
| `top_k_retrieval` | 10-100 | 30 | ⬆️ 高 | ⬇️ 低 | ➡️ 中 | ⬇️ 低 |
| `top_k_rerank` | 5-30 | 10 | ➡️ 中 | ⬆️ 高 | ⬆️ 高 | ➡️ 中 |
| `max_context_chunks` | 5-20 | 10 | ➡️ 低 | ⬆️ 高 | ⬆️⬆️ 极高 | ⬇️ 低 |

**解读：**
- **召回率**：能找到相关信息的能力
- **精度**：返回结果的准确性
- **成本**：Token消耗（主要是Prompt长度）
- **速度**：处理时间

---

## 🚀 优化建议

### 优化方向1：增强查询理解（阶段1）

#### 建议1.1：引入查询改写（Query Rewriting）
**目的**：将用户的模糊查询转换为更精确的检索查询

**实现方案**：
```python
def rewrite_query(original_query: str, query_type: QueryType) -> str:
    """
    基于LLM的查询改写
    
    示例：
    原始："张无忌和赵敏怎么在一起的？"
    改写："张无忌与赵敏初次相遇、感情发展、确立关系的过程"
    """
    prompt = f"""
你是一个查询优化专家。请将用户查询改写为更利于检索的形式。

查询类型：{query_type.value}

原始查询：{original_query}

改写要求：
1. 保留核心语义
2. 添加同义词或相关概念
3. 明确时间、因果关系
4. 对话类：添加"说""道""台词"等关键词
5. 分析类：添加"原因""过程""影响"等关键词

改写后的查询："""
    
    return llm_generate(prompt)
```

**优势**：
- ✅ 提升检索召回率
- ✅ 减少歧义
- ✅ 适配不同查询类型

**成本**：+1次LLM调用（小模型即可，约500 tokens）

---

#### 建议1.2：引入查询意图分类（多标签）
**目的**：识别复合查询的多个意图

**实现方案**：
```python
class QueryIntent(Enum):
    TIMELINE = "timeline"      # 时间线查询
    RELATION = "relation"      # 关系查询
    MOTIVATION = "motivation"  # 动机查询
    OUTCOME = "outcome"        # 结果查询
    COMPARISON = "comparison"  # 对比查询

def classify_intents(query: str) -> List[QueryIntent]:
    """
    多标签意图分类
    
    示例：
    查询："张无忌为何在光明顶之战后成为教主？"
    意图：[MOTIVATION, TIMELINE, OUTCOME]
    """
    # 使用轻量级分类模型或规则
    return detected_intents
```

**优势**：
- ✅ 更细粒度的查询理解
- ✅ 指导后续的检索策略和Prompt构建

---

### 优化方向2：增强检索能力（阶段2）

#### 建议2.1：混合检索（Hybrid Search）
**目的**：结合语义检索和关键词检索，提升召回率

**实现方案**：
```python
def hybrid_search(query: str, novel_id: int, top_k: int) -> List[Dict]:
    """
    混合检索：70%向量检索 + 30%BM25关键词检索
    """
    # 向量检索
    vector_results = vector_search(query, novel_id, top_k=top_k*2)
    
    # BM25关键词检索（使用Elasticsearch或Whoosh）
    keyword_results = bm25_search(query, novel_id, top_k=top_k)
    
    # 融合（Reciprocal Rank Fusion）
    fused_results = rrf_fusion(vector_results, keyword_results)
    
    return fused_results[:top_k]
```

**优势**：
- ✅ 弥补纯语义检索的不足（如专有名词、数字）
- ✅ 提升对精确匹配查询的召回率

**成本**：需要额外的索引存储（BM25索引）

---

#### 建议2.2：动态Top-K调整
**目的**：根据查询复杂度自动调整检索数量

**实现方案**：
```python
def adaptive_top_k(query: str, query_type: QueryType) -> int:
    """
    根据查询复杂度调整top_k
    """
    # 简单查询（短句、单一意图）
    if len(query) < 10 and query_type == QueryType.FACTUAL:
        return 20
    
    # 复杂查询（长句、分析类）
    elif len(query) > 30 or query_type == QueryType.ANALYSIS:
        return 50
    
    # 默认
    else:
        return 30
```

**优势**：
- ✅ 平衡性能和效果
- ✅ 自动化参数选择

---

#### 建议2.3：引入查询扩展（Query Expansion）
**目的**：自动添加同义词、相关实体，提升召回率

**实现方案**：
```python
def expand_query(query: str, graph: nx.MultiDiGraph) -> List[str]:
    """
    查询扩展：添加相关实体
    
    示例：
    原始："张无忌的师父"
    扩展：["张无忌的师父", "张三丰", "明教高层", "武当派掌门"]
    """
    # 1. 提取查询中的实体
    entities = extract_entities_from_query(query)
    
    # 2. 从知识图谱查找相关实体
    expanded_entities = []
    for entity in entities:
        if entity in graph.nodes():
            # 添加直接关联的实体
            neighbors = list(graph.neighbors(entity))[:5]
            expanded_entities.extend(neighbors)
    
    # 3. 生成扩展查询
    expanded_queries = [query]
    for entity in expanded_entities:
        expanded_queries.append(query + " " + entity)
    
    return expanded_queries
```

**优势**：
- ✅ 显著提升召回率
- ✅ 利用知识图谱的结构信息

**成本**：需要多次向量检索（每个扩展查询一次）

---

### 优化方向3：优化Rerank策略（阶段2.3）

#### 建议3.1：引入Cross-Encoder精排
**目的**：使用更强的模型对候选文档进行精细排序

**实现方案**：
```python
def cross_encoder_rerank(query: str, candidates: List[Dict], top_k: int) -> List[Dict]:
    """
    使用Cross-Encoder模型（如bge-reranker-v2-m3）重排序
    """
    from sentence_transformers import CrossEncoder
    
    model = CrossEncoder('BAAI/bge-reranker-v2-m3')
    
    # 构建输入对
    pairs = [(query, candidate['content']) for candidate in candidates]
    
    # 批量计算相关性分数
    scores = model.predict(pairs)
    
    # 更新分数并排序
    for i, candidate in enumerate(candidates):
        candidate['rerank_score'] = scores[i]
    
    candidates.sort(key=lambda x: -x['rerank_score'])
    
    return candidates[:top_k]
```

**优势**：
- ✅ 显著提升排序精度（相比双塔模型）
- ✅ 更好地理解查询-文档相关性

**成本**：推理时间增加（每个候选文档需要单独计算）

---

#### 建议3.2：基于图谱的上下文扩展
**目的**：利用知识图谱补充检索结果的上下文

**实现方案**：
```python
def graph_enhanced_context(
    query: str, 
    reranked_chunks: List[Dict], 
    graph: nx.MultiDiGraph,
    novel_id: int
) -> List[Dict]:
    """
    为检索结果添加图谱上下文
    """
    enhanced_chunks = []
    
    for chunk in reranked_chunks:
        # 提取块中的实体
        entities = extract_entities_from_text(chunk['content'])
        
        # 从图谱查询实体的关键关系
        graph_context = []
        for entity in entities:
            if entity in graph.nodes():
                # 获取高重要性的关系
                relations = get_entity_relations(graph, entity, top_k=3)
                graph_context.extend(relations)
        
        # 将图谱上下文附加到块
        chunk['graph_context'] = graph_context
        enhanced_chunks.append(chunk)
    
    return enhanced_chunks
```

**优势**：
- ✅ 补充显式的结构化知识
- ✅ 增强因果推理能力

---

### 优化方向4：优化Prompt构建（阶段3.1）

#### 建议4.1：查询类型自适应Prompt
**目的**：针对不同查询类型定制Prompt模板

**实现方案**：
```python
def build_adaptive_prompt(
    query: str,
    query_type: QueryType,
    context_chunks: List[Dict],
    novel_info: Dict
) -> str:
    """
    自适应Prompt构建
    """
    if query_type == QueryType.DIALOGUE:
        # 对话类：强调引用原文
        prompt = f"""
你是小说对话分析专家。请从以下片段中提取与问题相关的对话内容。

{format_context(context_chunks)}

用户问题：{query}

回答要求：
1. **直接引用原文对话**，使用引号标注
2. 标注说话者和章节
3. 如有必要，简要说明对话发生的背景

你的回答："""
    
    elif query_type == QueryType.ANALYSIS:
        # 分析类：引导逻辑推理
        prompt = f"""
你是小说情节分析专家。请基于以下片段进行深度分析。

{format_context(context_chunks)}

用户问题：{query}

回答要求：
1. 首先梳理关键情节和时间线
2. 分析因果关系和人物动机
3. 综合多个片段，形成连贯的解释
4. 标注引用的章节范围

思考步骤：
第1步：梳理时间线
第2步：分析因果关系
第3步：综合结论

你的回答："""
    
    else:  # FACTUAL
        # 事实类：强调准确性
        prompt = f"""
你是小说内容助手。请准确回答用户的事实性问题。

{format_context(context_chunks)}

用户问题：{query}

回答要求：
1. 回答必须基于提供的内容
2. 如内容不足以回答，明确说明
3. 标注信息来源章节

你的回答："""
    
    return prompt
```

**优势**：
- ✅ 提升各类查询的回答质量
- ✅ 引导LLM使用正确的推理方式

---

#### 建议4.2：引入Few-shot示例
**目的**：通过示例引导LLM生成更好的答案

**实现方案**：
```python
def add_few_shot_examples(prompt: str, query_type: QueryType) -> str:
    """
    添加Few-shot示例
    """
    examples = {
        QueryType.DIALOGUE: """
示例：
问题：萧峰在聚贤庄说了什么？
回答：萧峰在聚贤庄大声说道："我萧峰大好男儿，何惧于死？"（第42章）他表达了宁死不屈的决心。
""",
        QueryType.ANALYSIS: """
示例：
问题：令狐冲为何被逐出华山派？
回答：令狐冲被逐出华山派主要有三个原因：
1. 在思过崖学习了"吸星大法"等魔教武功（第13章）
2. 与魔教长老向问天结交，被怀疑投靠魔教（第18章）
3. 与小师妹岳灵珊的感情破裂，岳不群借机清理门户（第21章）
综合以上因素，岳不群以"行为不端"为由将令狐冲逐出师门。
"""
    }
    
    example = examples.get(query_type, "")
    return example + "\n\n" + prompt
```

**优势**：
- ✅ 显著提升答案格式的一致性
- ✅ 引导LLM学习正确的回答模式

---

#### 建议4.3：引入思维链（Chain of Thought）
**目的**：引导LLM进行逐步推理，提升复杂问题的回答质量

**实现方案**：
```python
def add_chain_of_thought(prompt: str, query_type: QueryType) -> str:
    """
    添加CoT提示
    """
    if query_type == QueryType.ANALYSIS:
        cot_instruction = """
在回答之前，请按以下步骤思考：
1. 识别问题中的关键要素（人物、事件、时间）
2. 从提供的片段中定位相关信息
3. 建立信息之间的因果关系
4. 形成连贯的解释

现在，请逐步思考并回答："""
        return prompt.replace("你的回答：", cot_instruction)
    
    return prompt
```

**优势**：
- ✅ 提升复杂推理问题的准确性
- ✅ 增加答案的可解释性

---

### 优化方向5：增强Self-RAG验证（阶段4）

#### 建议5.1：并行化验证流程
**目的**：减少Self-RAG的时间开销

**实现方案**：
```python
import asyncio

async def parallel_self_rag(
    answer: str,
    db: Session,
    novel_id: int
) -> Dict:
    """
    并行执行Self-RAG的多个步骤
    """
    # 1. 提取断言（同步）
    assertions = assertion_extractor.extract_assertions(answer)
    
    # 2. 并行收集证据
    evidence_tasks = [
        asyncio.create_task(
            evidence_collector.collect_evidence_async(db, novel_id, assertion)
        )
        for assertion in assertions
    ]
    evidence_lists = await asyncio.gather(*evidence_tasks)
    
    # 3. 并行一致性检查
    temporal_task = asyncio.create_task(
        consistency_checker.check_temporal_async(assertions, evidence_map)
    )
    character_task = asyncio.create_task(
        consistency_checker.check_character_async(db, novel_id, assertions, evidence_map)
    )
    
    temporal_issues, character_issues = await asyncio.gather(temporal_task, character_task)
    
    return {
        'assertions': assertions,
        'evidence': evidence_lists,
        'temporal_issues': temporal_issues,
        'character_issues': character_issues
    }
```

**优势**：
- ✅ 显著降低验证时间（约50%）
- ✅ 改善用户体验

---

#### 建议5.2：引入置信度阈值
**目的**：只验证低置信度的答案，节省计算资源

**实现方案**：
```python
def selective_self_rag(answer: str, initial_confidence: float) -> Dict:
    """
    选择性Self-RAG验证
    """
    if initial_confidence > 0.8:
        # 高置信度答案：跳过验证或仅轻量验证
        return {'verified': False, 'confidence': initial_confidence}
    
    else:
        # 低置信度答案：完整验证
        return full_self_rag_validation(answer)
```

**优势**：
- ✅ 平衡准确性和性能
- ✅ 降低平均查询时间

---

### 优化方向6：引入新步骤

#### 建议6.1：查询路由（Query Routing）
**目的**：根据查询难度选择不同的处理流程

**实现方案**：
```python
class QueryComplexity(Enum):
    SIMPLE = "simple"      # 简单查询：单次检索即可
    MODERATE = "moderate"  # 中等查询：标准RAG流程
    COMPLEX = "complex"    # 复杂查询：多轮检索+推理

def route_query(query: str) -> QueryComplexity:
    """
    查询复杂度路由
    """
    # 简单查询：短句、明确的事实问题
    if len(query) < 15 and is_factual_query(query):
        return QueryComplexity.SIMPLE
    
    # 复杂查询：需要推理、综合分析
    elif needs_reasoning(query) or is_multi_hop_query(query):
        return QueryComplexity.COMPLEX
    
    else:
        return QueryComplexity.MODERATE

def handle_query(query: str, novel_id: int):
    """
    根据复杂度选择流程
    """
    complexity = route_query(query)
    
    if complexity == QueryComplexity.SIMPLE:
        # 简化流程：跳过Self-RAG
        return simple_rag_pipeline(query, novel_id)
    
    elif complexity == QueryComplexity.COMPLEX:
        # 增强流程：多轮检索+推理
        return iterative_rag_pipeline(query, novel_id)
    
    else:
        return standard_rag_pipeline(query, novel_id)
```

**优势**：
- ✅ 优化资源分配
- ✅ 提升整体系统效率

---

#### 建议6.2：迭代式检索（Iterative Retrieval）
**目的**：对于复杂查询，多轮检索-推理循环

**实现方案**：
```python
def iterative_retrieval(query: str, novel_id: int, max_iterations: int = 3) -> str:
    """
    迭代式检索-推理
    
    流程：
    1. 初始检索 → 生成部分答案
    2. 识别答案中的信息缺口
    3. 生成后续查询，再次检索
    4. 补充答案
    5. 重复直到答案完整或达到最大迭代次数
    """
    context = []
    partial_answer = ""
    
    for i in range(max_iterations):
        # 第一轮使用原始查询，后续使用补充查询
        current_query = query if i == 0 else generate_followup_query(partial_answer, query)
        
        # 检索
        chunks = retrieve_and_rerank(current_query, novel_id)
        context.extend(chunks)
        
        # 生成部分答案
        prompt = build_prompt(current_query, context)
        partial_answer = generate_answer(prompt)
        
        # 检查是否完整
        if is_answer_complete(partial_answer, query):
            break
    
    return partial_answer

def generate_followup_query(partial_answer: str, original_query: str) -> str:
    """
    生成后续查询以补充信息
    """
    prompt = f"""
原始问题：{original_query}

当前答案：{partial_answer}

请识别当前答案中的信息缺口，生成一个后续查询以补充缺失的信息。

后续查询："""
    
    return llm_generate(prompt)
```

**优势**：
- ✅ 显著提升复杂问题的回答质量
- ✅ 适应多跳推理（multi-hop reasoning）

**成本**：+2-3倍的检索和生成次数

---

#### 建议6.3：答案融合（Answer Fusion）
**目的**：从多个候选答案中融合最优结果

**实现方案**：
```python
def multi_candidate_fusion(query: str, novel_id: int) -> str:
    """
    生成多个候选答案并融合
    """
    # 1. 使用不同的检索策略生成3个候选答案
    candidate1 = rag_pipeline(query, novel_id, strategy='semantic')
    candidate2 = rag_pipeline(query, novel_id, strategy='keyword')
    candidate3 = rag_pipeline(query, novel_id, strategy='graph_guided')
    
    # 2. 融合候选答案
    fusion_prompt = f"""
问题：{query}

候选答案1：{candidate1}
候选答案2：{candidate2}
候选答案3：{candidate3}

请融合以上3个候选答案，生成一个更准确、更完整的最终答案。要求：
1. 保留所有候选答案中的正确信息
2. 解决候选答案之间的矛盾
3. 去除冗余信息

最终答案："""
    
    final_answer = llm_generate(fusion_prompt)
    
    return final_answer
```

**优势**：
- ✅ 综合多种检索策略的优势
- ✅ 降低单次查询的失败率

**成本**：+3倍的检索和生成次数

---

## 📈 优化效果预估

| 优化方案 | 准确性提升 | 成本增加 | 实现难度 | 优先级 |
|---------|-----------|---------|---------|-------|
| 查询改写 | +5-10% | +5% | 低 | ⭐⭐⭐⭐ |
| 混合检索 | +10-15% | +20% | 中 | ⭐⭐⭐⭐⭐ |
| Cross-Encoder精排 | +15-20% | +30% | 中 | ⭐⭐⭐⭐⭐ |
| 自适应Prompt | +10-15% | 0% | 低 | ⭐⭐⭐⭐⭐ |
| 并行化Self-RAG | 0% | 0% (速度提升50%) | 中 | ⭐⭐⭐⭐ |
| 迭代式检索 | +20-30% | +200% | 高 | ⭐⭐⭐ |
| 答案融合 | +15-25% | +300% | 高 | ⭐⭐ |

---

## 🎯 推荐实施路线图

### 第一阶段（快速优化，1-2周）
1. ✅ **自适应Prompt模板**（零成本，高收益）
2. ✅ **查询改写**（低成本，中等收益）
3. ✅ **并行化Self-RAG**（提升速度）

### 第二阶段（核心增强，2-4周）
4. ✅ **混合检索（Hybrid Search）**（显著提升召回率）
5. ✅ **Cross-Encoder精排**（显著提升精度）
6. ✅ **查询意图多标签分类**（更精细的查询理解）

### 第三阶段（高级特性，4-8周）
7. ✅ **迭代式检索**（处理复杂查询）
8. ✅ **查询路由与自适应流程**（优化资源分配）
9. ✅ **答案融合**（极致准确性）

---

## 📚 总结

当前系统已实现了**完整的RAG + GraphRAG + Self-RAG流程**，具备以下优势：
- ✅ 结合语义检索和知识图谱
- ✅ 查询类型自适应重排序
- ✅ Self-RAG验证降低幻觉

主要瓶颈：
- ⚠️ 查询理解相对简单（基于关键词）
- ⚠️ 纯语义检索对精确匹配查询效果不佳
- ⚠️ Prompt模板静态，未针对查询类型优化
- ⚠️ Self-RAG串行执行，耗时较长

推荐优先实施：
1. **自适应Prompt模板**（最高性价比）
2. **混合检索**（显著提升召回率）
3. **Cross-Encoder精排**（显著提升精度）

长期规划：引入**迭代式检索**和**答案融合**，达到业界领先水平。

