# 查询阶段可配置参数说明

> 本文档详细说明在查询问题时可以配置的所有参数，帮助优化查询效果和成本。

---

## 📋 参数总览

查询阶段的参数可分为 **5大类**：

1. **检索参数** - 控制检索数量和范围
2. **LLM生成参数** - 控制答案生成行为
3. **Rerank权重参数** - 控制不同查询类型的权重策略
4. **Self-RAG参数** - 控制自我验证阈值
5. **GraphRAG参数** - 控制图谱增强策略

---

## 🎯 一、检索参数（Retrieval Parameters）

### 1.1 基础检索参数

| 参数名 | 默认值 | 取值范围 | 说明 | 配置位置 |
|--------|--------|----------|------|----------|
| `top_k_retrieval` | **30** | 10-100 | 初始向量检索返回的候选文档数 | `config.py:156` |
| `top_k_rerank` | **10** | 5-30 | Rerank后保留的文档数（发送给LLM） | `config.py:157` |
| `max_context_chunks` | **10** | 5-20 | 发送给LLM的最大文本块数（防止超出上下文） | `config.py:151` |
| `max_tokens_per_query` | **8000** | 4000-128000 | 单次查询的最大Token数（包括Prompt+Context） | `config.py:150` |

**影响**：
- `top_k_retrieval` ↑ → 召回率提高，但计算量增加
- `top_k_rerank` ↑ → 更多上下文，但Token成本增加
- `max_context_chunks` ↑ → 答案更全面，但可能引入噪音

**推荐配置**：
```bash
# 标准配置（平衡）
TOP_K_RETRIEVAL=30
TOP_K_RERANK=10
MAX_CONTEXT_CHUNKS=10

# 高精度配置（复杂问题）
TOP_K_RETRIEVAL=50
TOP_K_RERANK=15
MAX_CONTEXT_CHUNKS=15

# 快速配置（简单问题）
TOP_K_RETRIEVAL=20
TOP_K_RERANK=8
MAX_CONTEXT_CHUNKS=8
```

---

### 1.2 查询类型特定策略（自动）

系统会**自动识别**查询类型并应用不同策略：

| 查询类型 | Top-K | 合并块 | 引号权重 | 适用场景 |
|---------|-------|--------|----------|----------|
| **对话类** (dialogue) | 15 | ❌ 否 | 1.5x | "XX说了什么" |
| **分析类** (analysis) | 20 | ✅ 是 | 1.0x | "为什么"、"如何演变" |
| **事实类** (fact) | 10 | ❌ 否 | 1.0x | "XX是谁"、"在第几章" |

**配置位置**: `query_router.py:153-192`

**如何修改**：
```python
# backend/app/services/query_router.py
def get_retrieval_strategy(self, query_type: QueryType) -> dict:
    if query_type == QueryType.DIALOGUE:
        return {
            "top_k": 15,              # 可修改
            "quote_weight": 1.5,      # 可修改
            # ...
        }
```

---

## 🤖 二、LLM生成参数（LLM Generation Parameters）

### 2.1 模型选择

| 参数名 | 默认值 | 说明 | 配置位置 |
|--------|--------|------|----------|
| `zhipu_default_model` | **"GLM-4.5-Air"** | 默认使用的模型 | `config.py:16` |

**支持的模型**：
- 免费模型: `GLM-4.5-Flash`, `GLM-4-Flash-250414`
- 高性价比: `GLM-4.5-Air`, `GLM-4.5-AirX`
- 高性能: `GLM-4.5`, `GLM-4-Plus`, `GLM-4.6`
- 超长上下文: `GLM-4-Long` (1M tokens)

**用户可在前端查询时动态选择模型！**

---

### 2.2 生成控制参数

| 参数名 | 默认值 | 取值范围 | 说明 | 配置位置 |
|--------|--------|----------|------|----------|
| `temperature` | **0.7** | 0.0-1.0 | 生成随机性（越高越有创造性） | `zhipu_client.py:123` |
| `top_p` | **0.9** | 0.0-1.0 | 核采样参数（控制输出多样性） | `zhipu_client.py:124` |
| `max_tokens` | **None** | 1-128000 | 最大生成Token数（None=自动） | `zhipu_client.py:125` |

**影响**：
- `temperature = 0.0` → 确定性输出，适合事实查询
- `temperature = 0.7` → 平衡创造性和准确性（默认）
- `temperature = 1.0` → 高度创造性，适合分析类查询

**如何修改**：
```python
# backend/app/services/zhipu_client.py
def chat_completion(
    self,
    messages: List[Dict[str, str]],
    model: Optional[str] = None,
    temperature: float = 0.7,  # 可修改
    top_p: float = 0.9,        # 可修改
    max_tokens: Optional[int] = None,
    **kwargs
):
```

---

## ⚖️ 三、Rerank权重参数（Rerank Weights）

### 3.1 查询类型特定权重（自动应用）

#### 对话类查询 (Dialogue)
```python
# 代码位置: rag_engine.py:195-198
quote_boost = calculate_quote_boost(doc)  # 1.0-2.0x
final_score = base_score * quote_boost
```

**参数**：
- 引号权重加成: `1.5x`（包含引号的文本块优先）

---

#### 分析类查询 (Analysis)
```python
# 代码位置: rag_engine.py:199-202
importance_boost = chapter_importance + 0.5  # 使用GraphRAG
final_score = base_score * importance_boost
```

**参数**：
- 章节重要性权重: `chapter_importance` (0.0-1.0)
- 基础加成: `+0.5`

---

#### 事实类查询 (Fact)
```python
# 代码位置: rag_engine.py:203-210
semantic_weight = base_score * 0.60      # 语义相似度
temporal_weight = chapter_importance * 0.15  # 时序权重
entity_weight = 0.25 if has_entities else 0.0  # 实体匹配

final_score = semantic_weight + temporal_weight + entity_weight
```

**参数**（可在 `rag_engine.py:206-208` 修改）：
- 语义权重: `60%`
- 时序权重: `15%` (GraphRAG)
- 实体权重: `25%`

**如何优化**：
```python
# backend/app/services/rag_engine.py:206-208
semantic_weight = base_score * 0.70  # 提高语义权重到70%
temporal_weight = chapter_importance * 0.20  # 提高时序权重到20%
entity_weight = 0.10 if metadata.get('entities') else 0.0  # 降低实体权重到10%
```

---

## 🔍 四、Self-RAG参数（Self-Verification）

### 4.1 矛盾检测参数

| 参数 | 默认值 | 说明 | 配置位置 |
|------|--------|------|----------|
| 章节间隔阈值 | **10** | 相隔多少章才认为是潜在矛盾 | `contradiction_detector.py:166` |
| 证据支持阈值 | **0.7** | 高置信度断言的最低证据支持度 | `contradiction_detector.py:203` |
| 冲突词对 | **7组** | 用于检测矛盾的词对（如"是"vs"不是"） | `contradiction_detector.py:132-140` |

**如何修改**：
```python
# backend/app/services/self_rag/contradiction_detector.py

# 1. 章节间隔阈值
if abs(chapter2 - chapter1) > 10:  # 改为 5 或 20

# 2. 证据支持阈值
if assertion.get('confidence', 0) > 0.7:  # 改为 0.6 或 0.8

# 3. 添加冲突词对
conflict_pairs = [
    ('是', '不是'),
    ('强大', '弱小'),  # 新增
    # ...
]
```

---

### 4.2 置信度计算参数

**影响置信度的因素**：
1. 证据数量（越多越高）
2. 证据相似度（越高越高）
3. 矛盾数量（越多越低）
4. 章节跨度（跨度越大越需要验证）

**配置位置**: `contradiction_detector.py:245-251`

---

## 🕸️ 五、GraphRAG参数（Graph-Augmented）

### 5.1 图谱增强开关

| 参数 | 默认值 | 说明 |
|------|--------|------|
| 启用GraphRAG | **✅ 是** | 自动加载并使用知识图谱 |
| 章节重要性计算 | **✅ 是** | 基于PageRank计算章节重要性 |
| 实体关系增强 | **✅ 是** | 提取实体关系用于Rerank |

**代码位置**: `rag_engine.py:151-176`

**禁用GraphRAG**（如果需要）：
```python
# backend/app/services/rag_engine.py:155
if novel_id is not None and False:  # 强制禁用
    # GraphRAG代码
```

---

### 5.2 PageRank参数

知识图谱使用PageRank计算实体和章节重要性：

| 参数 | 默认值 | 说明 |
|------|--------|------|
| 阻尼系数 | **0.85** | PageRank的damping factor |
| 最大迭代次数 | **100** | 收敛迭代次数 |
| 容差 | **1e-6** | 收敛容差 |

**配置位置**: `graph_analyzer.py` (使用NetworkX默认值)

---

## 🎛️ 六、完整配置示例

### 6.1 环境变量配置（推荐）

创建或编辑 `backend/.env` 文件：

```bash
# ========== LLM配置 ==========
ZHIPU_API_KEY=your_api_key_here
ZHIPU_DEFAULT_MODEL=GLM-4.5-Air

# ========== 检索参数 ==========
TOP_K_RETRIEVAL=30
TOP_K_RERANK=10
MAX_CONTEXT_CHUNKS=10
MAX_TOKENS_PER_QUERY=8000

# ========== RAG配置（索引参数，不影响查询） ==========
CHUNK_SIZE=550
CHUNK_OVERLAP=125

# ========== 日志配置 ==========
LOG_LEVEL=INFO
DEBUG=True
```

---

### 6.2 代码级配置（高级）

如果需要更精细的控制，可以直接修改代码：

#### 修改默认检索参数
```python
# backend/app/services/rag_engine.py:27-28
self.top_k_retrieval = 50  # 改为50
self.top_k_rerank = 15     # 改为15
```

#### 修改LLM生成参数
```python
# backend/app/services/zhipu_client.py:123-125
temperature: float = 0.5,  # 降低随机性
top_p: float = 0.95,       # 提高采样阈值
max_tokens: Optional[int] = 4096,  # 限制输出长度
```

#### 修改Rerank权重
```python
# backend/app/services/rag_engine.py:206-208
semantic_weight = base_score * 0.70  # 语义权重70%
temporal_weight = chapter_importance * 0.20  # 时序权重20%
entity_weight = 0.10  # 实体权重10%
```

---

## 📊 七、参数调优建议

### 7.1 按查询难度调整

| 查询难度 | Top-K检索 | Rerank保留 | Temperature | 推荐模型 |
|---------|----------|-----------|-------------|----------|
| **简单** | 20 | 8 | 0.3 | GLM-4.5-Flash |
| **中等** | 30 | 10 | 0.7 | GLM-4.5-Air |
| **复杂** | 50 | 15 | 0.8 | GLM-4-Plus |
| **超复杂** | 70 | 20 | 0.9 | GLM-4.6 |

---

### 7.2 按小说长度调整

| 小说长度 | Top-K检索 | Chunk Size | 建议 |
|---------|----------|-----------|------|
| **短篇** (<10万字) | 15 | 400 | 降低检索数量 |
| **中篇** (10-100万字) | 30 | 550 | 标准配置 |
| **长篇** (100-500万字) | 40 | 600 | 增加检索深度 |
| **超长篇** (>500万字) | 50 | 800 | 使用GLM-4-Long |

---

### 7.3 按成本优化

| 目标 | 配置策略 | 预计成本/查询 |
|------|---------|--------------|
| **最低成本** | GLM-4.5-Flash + Top-K=10 | ¥0.00 |
| **高性价比** | GLM-4.5-Air + Top-K=15 | ¥0.01-0.02 |
| **高精度** | GLM-4-Plus + Top-K=20 | ¥0.20-0.30 |

---

## 🔧 八、动态参数传递（前端支持）

### 8.1 当前支持的动态参数

用户在查询时可以动态选择：

1. **模型选择** ✅ - 前端下拉框
2. **小说选择** ✅ - 小说列表

### 8.2 建议扩展（未来实现）

可以在前端添加"高级选项"：

```typescript
interface QueryOptions {
  model: string;           // ✅ 已支持
  novelId: number;         // ✅ 已支持
  topK?: number;           // ⭕ 建议添加
  temperature?: number;    // ⭕ 建议添加
  enableSelfRAG?: boolean; // ⭕ 建议添加
  enableGraphRAG?: boolean;// ⭕ 建议添加
}
```

---

## 📖 九、参数优先级

当多处配置冲突时，优先级为：

1. **API请求参数** (最高) - 如果前端传递了参数
2. **环境变量** (.env) - 运行时配置
3. **代码默认值** (最低) - 硬编码默认值

---

## ✅ 十、总结

### 核心可配置参数（最常用）

| 参数 | 默认值 | 影响 | 推荐调整场景 |
|------|--------|------|-------------|
| `TOP_K_RETRIEVAL` | 30 | 召回率 | 复杂问题增加到50 |
| `TOP_K_RERANK` | 10 | 精度/成本 | 高精度查询增加到15 |
| `ZHIPU_DEFAULT_MODEL` | GLM-4.5-Air | 能力/成本 | 复杂分析用GLM-4-Plus |
| `MAX_CONTEXT_CHUNKS` | 10 | 上下文量 | 演变类查询增加到15 |
| `temperature` | 0.7 | 创造性 | 事实查询降到0.3 |

### 修改步骤
1. 编辑 `backend/.env` 文件
2. 重启后端服务
3. 测试查询效果
4. 根据结果微调

---

**文档版本**: v1.0  
**最后更新**: 2025-11-17  
**维护者**: AI Assistant

